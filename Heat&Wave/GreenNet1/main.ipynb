{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU memory: 24210.31 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu_info.name}\")\n",
    "    print(f\"GPU memory: {gpu_info.total_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreenFun(nn.Module):\n",
    "    def __init__(self, N, N_quadrature):     \n",
    "        super(GreenFun, self).__init__()\n",
    "        self.N = N\n",
    "        self.N_quad = N_quadrature\n",
    "        self.tau_layer = nn.Sequential(nn.Linear(1, N_quadrature), nn.ReLU(), nn.Linear(N_quadrature, N_quadrature), nn.ReLU(), nn.Linear(N_quadrature, N_quadrature), nn.ReLU(), nn.Linear(N_quadrature, N_quadrature))\n",
    "        self.G_layer = nn.Sequential(nn.Linear(2, N_quadrature), nn.ReLU(), nn.Linear(N_quadrature, N_quadrature), nn.ReLU(), nn.Linear(N_quadrature, N_quadrature), nn.ReLU(), nn.Linear(N_quadrature, N_quadrature))\n",
    "\n",
    "    def forward(self, f, x, tau):    # f: (batch_size, N_quad), x: (N, 2), tau: (batch_size, 1)\n",
    "        T = self.tau_layer(torch.sqrt(tau))\n",
    "        G = self.G_layer(x)     # G is (N, N_quad) with G(i, j) = G((x_i, y_i); (x_quad_j, y_quad_j))\n",
    "        output = torch.matmul(f * T, G.t()) \n",
    "        output = output / self.N_quad \n",
    "        return output       # output: (batch_size, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Helmholtz equation\n",
    "$\\begin{cases} u - \\tau \\Delta u  = f, & x \\in \\Omega \\\\ u(x) = 0, & x \\in \\partial \\Omega \\end{cases}$\n",
    "with $\\Omega = [0, 1]^2$.\n",
    "\n",
    "$\\tau \\in [0.05, 0.1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import random\n",
    "\n",
    "def generate_gaussian_source_terms(num_samples, nx, ny, sigma):\n",
    "    source_terms = []\n",
    "    for _ in range(num_samples):\n",
    "        f = np.random.randn(nx, ny)\n",
    "        f = gaussian_filter(f, sigma=sigma)\n",
    "        source_terms.append(f)\n",
    "    return source_terms\n",
    "\n",
    "\n",
    "def generate_functional_source_terms(num_samples, nx, ny):\n",
    "    source_terms = []\n",
    "    x = np.linspace(0, 1, nx)\n",
    "    y = np.linspace(0, 1, ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        f = a * np.sin(b * np.pi * X) * np.sin(c * np.pi * Y)\n",
    "        source_terms.append(f)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        f = a * np.cos(b * np.pi * X) * np.cos(c * np.pi * Y)\n",
    "        source_terms.append(f)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        f = a * np.sin(b * np.pi * X) * np.cos(c * np.pi * Y)\n",
    "        source_terms.append(f)\n",
    "\n",
    "    for _ in range(num_samples - num_samples // 4 - num_samples // 4 - num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        f = a * np.cos(b * np.pi * X) * np.sin(c * np.pi * Y)\n",
    "        source_terms.append(f)\n",
    "\n",
    "    return source_terms\n",
    "\n",
    "from scipy.fft import dstn, idstn\n",
    "def solve_by_FFT2(tau, f, nx, ny, dx, dy):\n",
    "    u = np.zeros((nx,ny))\n",
    "    lam = np.zeros((nx-2,ny-2))\n",
    "    for i in range(nx-2):\n",
    "        for j in range(ny-2):\n",
    "            lam[i,j] = 2 * (np.cos((i+1)*np.pi*dx)+np.cos((j+1)*np.pi*dy)-2) / (dx*dy)\n",
    "    \n",
    "    u[1:-1, 1:-1] = idstn(dstn(f[1:-1,1:-1],type=1)/(1 - tau * lam),type=1)\n",
    "    \n",
    "    return 1, u\n",
    "\n",
    "\n",
    "def select(u, nx, ny, m):       # get low resolution data from high resolution data\n",
    "    result = np.zeros((len(u), nx, ny))\n",
    "    for l in range(len(u)):\n",
    "        for i in range(nx):\n",
    "            for j in range(ny):\n",
    "                result[l, i, j] = u[l, m * i, m * j]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_poisson_data1(tau, n, nx, ny):\n",
    "    m = 2       # for getting higher resolution data\n",
    "    dx, dy = 1 / (nx - 1), 1 / (ny - 1)\n",
    "    nnx = m * (nx - 1) + 1\n",
    "    nny = m * (ny - 1) + 1\n",
    "    ddx, ddy = 1 / (nnx - 1), 1 / (nny - 1)\n",
    "    \n",
    "    source_terms1 = generate_gaussian_source_terms(n // 10, nx, ny, 1)\n",
    "    source_terms2 = generate_gaussian_source_terms(n // 10, nx, ny, 2)\n",
    "    source_terms3 = generate_gaussian_source_terms(n // 10, nx, ny, 3)\n",
    "    source_terms4 = generate_gaussian_source_terms(n // 10, nx, ny, 4)\n",
    "    source_terms5 = generate_gaussian_source_terms(n // 10, nx, ny, 5)\n",
    "    source_terms6 = generate_functional_source_terms(n - 5 * (n // 10), nnx, nny)\n",
    "    \n",
    "    solutions1 = []\n",
    "    solutions2 = []\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for f in source_terms1:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms2:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms3:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms4:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms5:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms6:\n",
    "        success, u = solve_by_FFT2(tau, f, nnx, nny, ddx, ddy)\n",
    "        if success == 1:\n",
    "            solutions2.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    a = np.array(source_terms1 + source_terms2 + source_terms3 + source_terms4 + source_terms5)\n",
    "    b = select(np.array(source_terms6), nx, ny, m)\n",
    "    c = np.array(solutions1)\n",
    "    d = select(np.array(solutions2), nx, ny, m)\n",
    "    \n",
    "    return np.stack([a, b], axis=0), np.stack([c, d], axis=0)\n",
    "\n",
    "\n",
    "def generate_poisson_data2(tau, n, nx, ny):\n",
    "    f = []\n",
    "    u = []\n",
    "    ttau = []\n",
    "\n",
    "    count = 1\n",
    "    for t in tau:\n",
    "        temp_tau = np.zeros((n, 1)) + t\n",
    "        temp_f, temp_u = generate_poisson_data1(t, n, nx, ny)\n",
    "        print(\"Finish\", count, \"of\", len(tau), \"tau's\")\n",
    "        f.append(temp_f)\n",
    "        u.append(temp_u)\n",
    "        ttau.append(temp_tau)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return np.concatenate(f, axis=0), np.concatenate(u, axis=0), np.concatenate(ttau, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 41\n",
    "tau = np.array([0.05, 0.055, 0.06, 0.065, 0.07, 0.075, 0.08, 0.085, 0.09, 0.095, 0.1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 1 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 2 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 3 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 4 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 5 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 6 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 7 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 8 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 9 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 10 of 11 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 11 of 11 tau's\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"f_np.npy\") and os.path.exists(\"u_np.npy\") and os.path.exists(\"tau_np.npy\"):\n",
    "    f_np, u_np, tau_np = np.load('f_np.npy'), np.load('u_np.npy'), np.load('tau_np.npy')\n",
    "else:\n",
    "    f_np, u_np, tau_np = generate_poisson_data2(tau, 2000, N, N)\n",
    "    np.save('f_np.npy', f_np)\n",
    "    np.save('u_np.npy', u_np)\n",
    "    np.save('tau_np.npy', tau_np)\n",
    "\n",
    "f, u, tau = torch.tensor(f_np, dtype=torch.float32).view(-1, N * N), torch.tensor(u_np, dtype=torch.float32).view(-1, N * N), torch.tensor(tau_np, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22000, 1681]), torch.Size([22000, 1681]), torch.Size([22000, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape, u.shape, tau.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "dataset = TensorDataset(f, tau, u)\n",
    "\n",
    "# 定义训练集和测试集的大小\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# 将数据集按比例分成训练集和测试集\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 获取训练集的数据\n",
    "train_f = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "train_tau = torch.stack([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "train_u = torch.stack([train_dataset[i][2] for i in range(len(train_dataset))])\n",
    "\n",
    "# 获取测试集的数据\n",
    "test_f = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])\n",
    "test_tau = torch.stack([test_dataset[i][1] for i in range(len(test_dataset))])\n",
    "test_u = torch.stack([test_dataset[i][2] for i in range(len(test_dataset))])\n",
    "\n",
    "train_f = train_f.to(device)\n",
    "train_tau = train_tau.to(device)\n",
    "train_u = train_u.to(device)\n",
    "test_f = test_f.to(device)\n",
    "test_tau = test_tau.to(device)\n",
    "test_u = test_u.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(N*N, 2).to(device)\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        x[i*N+j, 0] = 1 * j / (N - 1)\n",
    "        x[i*N+j, 1] = 1 * i / (N - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_validation1 = np.zeros((9, N, N))\n",
    "U_validation1 = np.zeros((9, N, N))\n",
    "tau_validation1 = np.array([0.0633, 0.0633, 0.0633, 0.0766, 0.0766, 0.0766, 0.0999, 0.0999, 0.0999])\n",
    "\n",
    "for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                xx = 1 * j / (N - 1)\n",
    "                yy = 1 * i / (N - 1)\n",
    "                t = tau_validation1[k1 * 3 + k2]\n",
    "                \n",
    "                U_validation1[k1 * 3 + k2, i, j] = np.sin((k1 + 1) * np.pi * xx) * np.sin((k2 + 1) * np.pi * yy)\n",
    "                f_validation1[k1 * 3 + k2, i, j] = - np.pi ** 2 * ((k1 + 1) * (k1 + 1) + (k2 + 1) * (k2 + 1)) * np.sin((k1 + 1) * np.pi * xx) * np.sin((k2 + 1) * np.pi * yy)\n",
    "                f_validation1[k1 * 3 + k2, i, j] = U_validation1[k1 * 3 + k2, i, j] - t * f_validation1[k1 * 3 + k2, i, j]\n",
    "\n",
    "\n",
    "U_validation1 = torch.tensor(U_validation1, dtype=torch.float32).view(-1, N * N).to(device)\n",
    "f_validation1 = torch.tensor(f_validation1, dtype=torch.float32).view(-1, N * N).to(device)\n",
    "tau_validation1 = torch.tensor(tau_validation1, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose, delta, path):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.8f} --> {val_loss:.8f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/50000] Training Loss: 0.00296524 Testing Loss: 0.00296443 Validation loss 1: 0.07770678\n",
      "Epoch [200/50000] Training Loss: 0.00021247 Testing Loss: 0.00020658 Validation loss 1: 0.00191839\n",
      "Epoch [300/50000] Training Loss: 0.00008121 Testing Loss: 0.00007502 Validation loss 1: 0.00076212\n",
      "Epoch [400/50000] Training Loss: 0.00010844 Testing Loss: 0.00009145 Validation loss 1: 0.00214136\n",
      "Epoch [500/50000] Training Loss: 0.00003746 Testing Loss: 0.00004072 Validation loss 1: 0.00068116\n",
      "Epoch [600/50000] Training Loss: 0.00002714 Testing Loss: 0.00002749 Validation loss 1: 0.00030972\n",
      "Epoch [700/50000] Training Loss: 0.00002832 Testing Loss: 0.00002332 Validation loss 1: 0.00032201\n",
      "Epoch [800/50000] Training Loss: 0.00002085 Testing Loss: 0.00002012 Validation loss 1: 0.00038077\n",
      "Epoch [900/50000] Training Loss: 0.00001711 Testing Loss: 0.00001693 Validation loss 1: 0.00028790\n",
      "Epoch [1000/50000] Training Loss: 0.00001560 Testing Loss: 0.00001545 Validation loss 1: 0.00027222\n",
      "Epoch [1100/50000] Training Loss: 0.00002020 Testing Loss: 0.00002122 Validation loss 1: 0.00045002\n",
      "Epoch [1200/50000] Training Loss: 0.00002433 Testing Loss: 0.00002406 Validation loss 1: 0.00033505\n",
      "Epoch [1300/50000] Training Loss: 0.00001207 Testing Loss: 0.00001184 Validation loss 1: 0.00017827\n",
      "Epoch [1400/50000] Training Loss: 0.00001094 Testing Loss: 0.00001201 Validation loss 1: 0.00016571\n",
      "Epoch [1500/50000] Training Loss: 0.00002495 Testing Loss: 0.00003271 Validation loss 1: 0.00045736\n",
      "Epoch [1600/50000] Training Loss: 0.00004041 Testing Loss: 0.00003197 Validation loss 1: 0.00059616\n",
      "Epoch [1700/50000] Training Loss: 0.00001647 Testing Loss: 0.00001916 Validation loss 1: 0.00029904\n",
      "Epoch [1800/50000] Training Loss: 0.00002112 Testing Loss: 0.00002521 Validation loss 1: 0.00033038\n",
      "Epoch [1900/50000] Training Loss: 0.00001717 Testing Loss: 0.00001578 Validation loss 1: 0.00032380\n",
      "Epoch [2000/50000] Training Loss: 0.00003393 Testing Loss: 0.00004385 Validation loss 1: 0.00065339\n",
      "Epoch [2100/50000] Training Loss: 0.00002348 Testing Loss: 0.00002783 Validation loss 1: 0.00062273\n",
      "Epoch [2200/50000] Training Loss: 0.00000642 Testing Loss: 0.00000661 Validation loss 1: 0.00008676\n",
      "Epoch [2300/50000] Training Loss: 0.00001101 Testing Loss: 0.00000718 Validation loss 1: 0.00011017\n",
      "Epoch [2400/50000] Training Loss: 0.00000735 Testing Loss: 0.00000729 Validation loss 1: 0.00009997\n",
      "Epoch [2500/50000] Training Loss: 0.00001780 Testing Loss: 0.00002050 Validation loss 1: 0.00036063\n",
      "Epoch [2600/50000] Training Loss: 0.00000647 Testing Loss: 0.00000674 Validation loss 1: 0.00010989\n",
      "Epoch [2700/50000] Training Loss: 0.00000669 Testing Loss: 0.00000646 Validation loss 1: 0.00006137\n",
      "Epoch [2800/50000] Training Loss: 0.00000656 Testing Loss: 0.00000663 Validation loss 1: 0.00009936\n",
      "Epoch [2900/50000] Training Loss: 0.00001283 Testing Loss: 0.00000978 Validation loss 1: 0.00018009\n",
      "Epoch [3000/50000] Training Loss: 0.00000583 Testing Loss: 0.00000611 Validation loss 1: 0.00008426\n",
      "Epoch [3100/50000] Training Loss: 0.00001206 Testing Loss: 0.00001371 Validation loss 1: 0.00024210\n",
      "Epoch [3200/50000] Training Loss: 0.00000858 Testing Loss: 0.00000844 Validation loss 1: 0.00009529\n",
      "Epoch [3300/50000] Training Loss: 0.00003434 Testing Loss: 0.00003910 Validation loss 1: 0.00074933\n",
      "Epoch [3400/50000] Training Loss: 0.00000944 Testing Loss: 0.00000733 Validation loss 1: 0.00009141\n",
      "Epoch [3500/50000] Training Loss: 0.00000940 Testing Loss: 0.00001127 Validation loss 1: 0.00015041\n",
      "Epoch [3600/50000] Training Loss: 0.00000759 Testing Loss: 0.00000762 Validation loss 1: 0.00011582\n",
      "Epoch [3700/50000] Training Loss: 0.00000421 Testing Loss: 0.00000408 Validation loss 1: 0.00004181\n",
      "Epoch [3800/50000] Training Loss: 0.00002137 Testing Loss: 0.00002122 Validation loss 1: 0.00032491\n",
      "Epoch [3900/50000] Training Loss: 0.00000451 Testing Loss: 0.00000451 Validation loss 1: 0.00004945\n",
      "Epoch [4000/50000] Training Loss: 0.00002479 Testing Loss: 0.00003211 Validation loss 1: 0.00060309\n",
      "Epoch [4100/50000] Training Loss: 0.00001382 Testing Loss: 0.00001316 Validation loss 1: 0.00024694\n",
      "Epoch [4200/50000] Training Loss: 0.00000423 Testing Loss: 0.00000416 Validation loss 1: 0.00003889\n",
      "Epoch [4300/50000] Training Loss: 0.00002539 Testing Loss: 0.00002804 Validation loss 1: 0.00057260\n",
      "Epoch [4400/50000] Training Loss: 0.00000610 Testing Loss: 0.00000562 Validation loss 1: 0.00007119\n",
      "Epoch [4500/50000] Training Loss: 0.00000400 Testing Loss: 0.00000384 Validation loss 1: 0.00004207\n",
      "Epoch [4600/50000] Training Loss: 0.00000743 Testing Loss: 0.00000731 Validation loss 1: 0.00012445\n",
      "Epoch [4700/50000] Training Loss: 0.00000386 Testing Loss: 0.00000382 Validation loss 1: 0.00003822\n",
      "Epoch [4800/50000] Training Loss: 0.00000658 Testing Loss: 0.00000803 Validation loss 1: 0.00011974\n",
      "Epoch [4900/50000] Training Loss: 0.00000817 Testing Loss: 0.00000762 Validation loss 1: 0.00010417\n",
      "Epoch [5000/50000] Training Loss: 0.00000396 Testing Loss: 0.00000382 Validation loss 1: 0.00004144\n",
      "Epoch [5100/50000] Training Loss: 0.00000384 Testing Loss: 0.00000365 Validation loss 1: 0.00003911\n",
      "Epoch [5200/50000] Training Loss: 0.00000355 Testing Loss: 0.00000372 Validation loss 1: 0.00003913\n",
      "Epoch [5300/50000] Training Loss: 0.00000511 Testing Loss: 0.00000651 Validation loss 1: 0.00011589\n",
      "Epoch [5400/50000] Training Loss: 0.00000777 Testing Loss: 0.00000895 Validation loss 1: 0.00013735\n",
      "Epoch [5500/50000] Training Loss: 0.00000477 Testing Loss: 0.00000478 Validation loss 1: 0.00007492\n",
      "Epoch [5600/50000] Training Loss: 0.00000316 Testing Loss: 0.00000333 Validation loss 1: 0.00002481\n",
      "Epoch [5700/50000] Training Loss: 0.00000396 Testing Loss: 0.00000456 Validation loss 1: 0.00005810\n",
      "Epoch [5800/50000] Training Loss: 0.00000794 Testing Loss: 0.00000560 Validation loss 1: 0.00008897\n",
      "Epoch [5900/50000] Training Loss: 0.00001854 Testing Loss: 0.00002401 Validation loss 1: 0.00041238\n",
      "Epoch [6000/50000] Training Loss: 0.00000357 Testing Loss: 0.00000357 Validation loss 1: 0.00003995\n",
      "Epoch [6100/50000] Training Loss: 0.00001128 Testing Loss: 0.00001531 Validation loss 1: 0.00031784\n",
      "Epoch [6200/50000] Training Loss: 0.00000793 Testing Loss: 0.00000838 Validation loss 1: 0.00011619\n",
      "Epoch [6300/50000] Training Loss: 0.00000691 Testing Loss: 0.00000675 Validation loss 1: 0.00011290\n",
      "Epoch 06327: reducing learning rate of group 0 to 7.0000e-04.\n",
      "Epoch [6400/50000] Training Loss: 0.00000244 Testing Loss: 0.00000243 Validation loss 1: 0.00001493\n",
      "Epoch [6500/50000] Training Loss: 0.00000240 Testing Loss: 0.00000239 Validation loss 1: 0.00001424\n",
      "Epoch [6600/50000] Training Loss: 0.00000236 Testing Loss: 0.00000235 Validation loss 1: 0.00001387\n",
      "Epoch [6700/50000] Training Loss: 0.00000233 Testing Loss: 0.00000232 Validation loss 1: 0.00001349\n",
      "Epoch [6800/50000] Training Loss: 0.00000230 Testing Loss: 0.00000229 Validation loss 1: 0.00001312\n",
      "Epoch [6900/50000] Training Loss: 0.00000228 Testing Loss: 0.00000226 Validation loss 1: 0.00001278\n",
      "Epoch [7000/50000] Training Loss: 0.00000225 Testing Loss: 0.00000224 Validation loss 1: 0.00001244\n",
      "Epoch [7100/50000] Training Loss: 0.00000255 Testing Loss: 0.00000252 Validation loss 1: 0.00001750\n",
      "Epoch [7200/50000] Training Loss: 0.00000683 Testing Loss: 0.00000641 Validation loss 1: 0.00010223\n",
      "Epoch [7300/50000] Training Loss: 0.00000517 Testing Loss: 0.00000440 Validation loss 1: 0.00006230\n",
      "Epoch [7400/50000] Training Loss: 0.00000303 Testing Loss: 0.00000303 Validation loss 1: 0.00002871\n",
      "Epoch [7500/50000] Training Loss: 0.00000224 Testing Loss: 0.00000221 Validation loss 1: 0.00001330\n",
      "Epoch [7600/50000] Training Loss: 0.00000396 Testing Loss: 0.00000326 Validation loss 1: 0.00004318\n",
      "Epoch [7700/50000] Training Loss: 0.00000315 Testing Loss: 0.00000333 Validation loss 1: 0.00003558\n",
      "Epoch [7800/50000] Training Loss: 0.00000230 Testing Loss: 0.00000228 Validation loss 1: 0.00001418\n",
      "Epoch [7900/50000] Training Loss: 0.00001160 Testing Loss: 0.00001130 Validation loss 1: 0.00022253\n",
      "Epoch [8000/50000] Training Loss: 0.00000330 Testing Loss: 0.00000267 Validation loss 1: 0.00002159\n",
      "Epoch [8100/50000] Training Loss: 0.00000240 Testing Loss: 0.00000254 Validation loss 1: 0.00002309\n",
      "Epoch [8200/50000] Training Loss: 0.00000215 Testing Loss: 0.00000211 Validation loss 1: 0.00001251\n",
      "Epoch [8300/50000] Training Loss: 0.00000202 Testing Loss: 0.00000200 Validation loss 1: 0.00000991\n",
      "Epoch [8400/50000] Training Loss: 0.00000206 Testing Loss: 0.00000205 Validation loss 1: 0.00001321\n",
      "Epoch [8500/50000] Training Loss: 0.00000206 Testing Loss: 0.00000207 Validation loss 1: 0.00001173\n",
      "Epoch [8600/50000] Training Loss: 0.00000573 Testing Loss: 0.00000654 Validation loss 1: 0.00012926\n",
      "Epoch [8700/50000] Training Loss: 0.00001030 Testing Loss: 0.00001038 Validation loss 1: 0.00019229\n",
      "Epoch [8800/50000] Training Loss: 0.00000452 Testing Loss: 0.00000403 Validation loss 1: 0.00005674\n",
      "Epoch [8900/50000] Training Loss: 0.00000330 Testing Loss: 0.00000578 Validation loss 1: 0.00009986\n",
      "Epoch [9000/50000] Training Loss: 0.00000639 Testing Loss: 0.00000442 Validation loss 1: 0.00006911\n",
      "Epoch [9100/50000] Training Loss: 0.00000216 Testing Loss: 0.00000220 Validation loss 1: 0.00001388\n",
      "Epoch [9200/50000] Training Loss: 0.00000811 Testing Loss: 0.00000491 Validation loss 1: 0.00007766\n",
      "Epoch [9300/50000] Training Loss: 0.00000263 Testing Loss: 0.00000267 Validation loss 1: 0.00002998\n",
      "Epoch [9400/50000] Training Loss: 0.00000334 Testing Loss: 0.00000301 Validation loss 1: 0.00003818\n",
      "Epoch [9500/50000] Training Loss: 0.00001412 Testing Loss: 0.00001542 Validation loss 1: 0.00029142\n",
      "Epoch [9600/50000] Training Loss: 0.00000337 Testing Loss: 0.00000346 Validation loss 1: 0.00005504\n",
      "Epoch [9700/50000] Training Loss: 0.00000189 Testing Loss: 0.00000185 Validation loss 1: 0.00000974\n",
      "Epoch [9800/50000] Training Loss: 0.00000873 Testing Loss: 0.00001034 Validation loss 1: 0.00016345\n",
      "Epoch [9900/50000] Training Loss: 0.00000282 Testing Loss: 0.00000295 Validation loss 1: 0.00003631\n",
      "Epoch [10000/50000] Training Loss: 0.00000195 Testing Loss: 0.00000182 Validation loss 1: 0.00001009\n",
      "Epoch [10100/50000] Training Loss: 0.00000878 Testing Loss: 0.00000698 Validation loss 1: 0.00012660\n",
      "Epoch [10200/50000] Training Loss: 0.00000311 Testing Loss: 0.00000247 Validation loss 1: 0.00002221\n",
      "Epoch [10300/50000] Training Loss: 0.00000219 Testing Loss: 0.00000199 Validation loss 1: 0.00001586\n",
      "Epoch [10400/50000] Training Loss: 0.00000176 Testing Loss: 0.00000176 Validation loss 1: 0.00000796\n",
      "Epoch [10500/50000] Training Loss: 0.00000189 Testing Loss: 0.00000186 Validation loss 1: 0.00001325\n",
      "Epoch [10600/50000] Training Loss: 0.00000187 Testing Loss: 0.00000180 Validation loss 1: 0.00000934\n",
      "Epoch [10700/50000] Training Loss: 0.00000914 Testing Loss: 0.00000800 Validation loss 1: 0.00015287\n",
      "Epoch [10800/50000] Training Loss: 0.00000253 Testing Loss: 0.00000257 Validation loss 1: 0.00002331\n",
      "Epoch [10900/50000] Training Loss: 0.00000170 Testing Loss: 0.00000169 Validation loss 1: 0.00000756\n",
      "Epoch [11000/50000] Training Loss: 0.00000193 Testing Loss: 0.00000200 Validation loss 1: 0.00001387\n",
      "Epoch [11100/50000] Training Loss: 0.00000230 Testing Loss: 0.00000236 Validation loss 1: 0.00002201\n",
      "Epoch [11200/50000] Training Loss: 0.00000829 Testing Loss: 0.00000918 Validation loss 1: 0.00013733\n",
      "Epoch [11300/50000] Training Loss: 0.00000181 Testing Loss: 0.00000183 Validation loss 1: 0.00001195\n",
      "Epoch [11400/50000] Training Loss: 0.00000172 Testing Loss: 0.00000173 Validation loss 1: 0.00001025\n",
      "Epoch 11489: reducing learning rate of group 0 to 4.9000e-04.\n",
      "Epoch [11500/50000] Training Loss: 0.00000246 Testing Loss: 0.00000266 Validation loss 1: 0.00002270\n",
      "Epoch [11600/50000] Training Loss: 0.00000159 Testing Loss: 0.00000157 Validation loss 1: 0.00000636\n",
      "Epoch [11700/50000] Training Loss: 0.00000158 Testing Loss: 0.00000156 Validation loss 1: 0.00000629\n",
      "Epoch [11800/50000] Training Loss: 0.00000157 Testing Loss: 0.00000155 Validation loss 1: 0.00000621\n",
      "Epoch [11900/50000] Training Loss: 0.00000156 Testing Loss: 0.00000154 Validation loss 1: 0.00000613\n",
      "Epoch [12000/50000] Training Loss: 0.00000155 Testing Loss: 0.00000153 Validation loss 1: 0.00000605\n",
      "Epoch [12100/50000] Training Loss: 0.00000154 Testing Loss: 0.00000152 Validation loss 1: 0.00000597\n",
      "Epoch [12200/50000] Training Loss: 0.00000153 Testing Loss: 0.00000151 Validation loss 1: 0.00000591\n",
      "Epoch [12300/50000] Training Loss: 0.00000175 Testing Loss: 0.00000171 Validation loss 1: 0.00000834\n",
      "Epoch [12400/50000] Training Loss: 0.00000261 Testing Loss: 0.00000239 Validation loss 1: 0.00002683\n",
      "Epoch [12500/50000] Training Loss: 0.00000155 Testing Loss: 0.00000153 Validation loss 1: 0.00000633\n",
      "Epoch [12600/50000] Training Loss: 0.00000152 Testing Loss: 0.00000153 Validation loss 1: 0.00000692\n",
      "Epoch [12700/50000] Training Loss: 0.00000171 Testing Loss: 0.00000177 Validation loss 1: 0.00000938\n",
      "Epoch [12800/50000] Training Loss: 0.00000197 Testing Loss: 0.00000203 Validation loss 1: 0.00002213\n",
      "Epoch [12900/50000] Training Loss: 0.00000148 Testing Loss: 0.00000146 Validation loss 1: 0.00000576\n",
      "Epoch [13000/50000] Training Loss: 0.00000148 Testing Loss: 0.00000146 Validation loss 1: 0.00000548\n",
      "Epoch [13100/50000] Training Loss: 0.00000151 Testing Loss: 0.00000149 Validation loss 1: 0.00000629\n",
      "Epoch [13200/50000] Training Loss: 0.00000152 Testing Loss: 0.00000151 Validation loss 1: 0.00000644\n",
      "Epoch [13300/50000] Training Loss: 0.00000155 Testing Loss: 0.00000152 Validation loss 1: 0.00000705\n",
      "Epoch [13400/50000] Training Loss: 0.00000202 Testing Loss: 0.00000215 Validation loss 1: 0.00001662\n",
      "Epoch [13500/50000] Training Loss: 0.00000167 Testing Loss: 0.00000160 Validation loss 1: 0.00000921\n",
      "Epoch [13600/50000] Training Loss: 0.00000147 Testing Loss: 0.00000148 Validation loss 1: 0.00000699\n",
      "Epoch [13700/50000] Training Loss: 0.00000185 Testing Loss: 0.00000174 Validation loss 1: 0.00001105\n",
      "Epoch [13800/50000] Training Loss: 0.00000164 Testing Loss: 0.00000155 Validation loss 1: 0.00000888\n",
      "Epoch [13900/50000] Training Loss: 0.00000170 Testing Loss: 0.00000163 Validation loss 1: 0.00000894\n",
      "Epoch [14000/50000] Training Loss: 0.00000142 Testing Loss: 0.00000141 Validation loss 1: 0.00000572\n",
      "Epoch [14100/50000] Training Loss: 0.00000442 Testing Loss: 0.00000415 Validation loss 1: 0.00007392\n",
      "Epoch [14200/50000] Training Loss: 0.00000215 Testing Loss: 0.00000238 Validation loss 1: 0.00003254\n",
      "Epoch [14300/50000] Training Loss: 0.00000168 Testing Loss: 0.00000166 Validation loss 1: 0.00001247\n",
      "Epoch [14400/50000] Training Loss: 0.00000195 Testing Loss: 0.00000211 Validation loss 1: 0.00003373\n",
      "Epoch [14500/50000] Training Loss: 0.00000233 Testing Loss: 0.00000274 Validation loss 1: 0.00003488\n",
      "Epoch [14600/50000] Training Loss: 0.00000262 Testing Loss: 0.00000310 Validation loss 1: 0.00003917\n",
      "Epoch [14700/50000] Training Loss: 0.00000136 Testing Loss: 0.00000134 Validation loss 1: 0.00000483\n",
      "Epoch [14800/50000] Training Loss: 0.00000266 Testing Loss: 0.00000208 Validation loss 1: 0.00002101\n",
      "Epoch [14900/50000] Training Loss: 0.00000137 Testing Loss: 0.00000139 Validation loss 1: 0.00000568\n",
      "Epoch [15000/50000] Training Loss: 0.00000142 Testing Loss: 0.00000142 Validation loss 1: 0.00000802\n",
      "Epoch [15100/50000] Training Loss: 0.00000631 Testing Loss: 0.00000669 Validation loss 1: 0.00013571\n",
      "Epoch [15200/50000] Training Loss: 0.00000165 Testing Loss: 0.00000238 Validation loss 1: 0.00002380\n",
      "Epoch [15300/50000] Training Loss: 0.00000215 Testing Loss: 0.00000218 Validation loss 1: 0.00002483\n",
      "Epoch [15400/50000] Training Loss: 0.00000559 Testing Loss: 0.00000697 Validation loss 1: 0.00012917\n",
      "Epoch [15500/50000] Training Loss: 0.00000753 Testing Loss: 0.00000680 Validation loss 1: 0.00013757\n",
      "Epoch [15600/50000] Training Loss: 0.00000131 Testing Loss: 0.00000130 Validation loss 1: 0.00000458\n",
      "Epoch [15700/50000] Training Loss: 0.00000239 Testing Loss: 0.00000257 Validation loss 1: 0.00003250\n",
      "Epoch [15800/50000] Training Loss: 0.00000450 Testing Loss: 0.00000347 Validation loss 1: 0.00004828\n",
      "Epoch [15900/50000] Training Loss: 0.00000138 Testing Loss: 0.00000132 Validation loss 1: 0.00000560\n",
      "Epoch [16000/50000] Training Loss: 0.00000159 Testing Loss: 0.00000166 Validation loss 1: 0.00000989\n",
      "Epoch [16100/50000] Training Loss: 0.00000134 Testing Loss: 0.00000136 Validation loss 1: 0.00000757\n",
      "Epoch [16200/50000] Training Loss: 0.00000155 Testing Loss: 0.00000161 Validation loss 1: 0.00001255\n",
      "Epoch [16300/50000] Training Loss: 0.00000160 Testing Loss: 0.00000183 Validation loss 1: 0.00001724\n",
      "Epoch [16400/50000] Training Loss: 0.00000154 Testing Loss: 0.00000141 Validation loss 1: 0.00000745\n",
      "Epoch [16500/50000] Training Loss: 0.00000252 Testing Loss: 0.00000209 Validation loss 1: 0.00002829\n",
      "Epoch [16600/50000] Training Loss: 0.00000159 Testing Loss: 0.00000143 Validation loss 1: 0.00000946\n",
      "Epoch [16700/50000] Training Loss: 0.00000155 Testing Loss: 0.00000162 Validation loss 1: 0.00001153\n",
      "Epoch [16800/50000] Training Loss: 0.00000135 Testing Loss: 0.00000129 Validation loss 1: 0.00000566\n",
      "Epoch [16900/50000] Training Loss: 0.00000154 Testing Loss: 0.00000154 Validation loss 1: 0.00001272\n",
      "Epoch [17000/50000] Training Loss: 0.00000179 Testing Loss: 0.00000158 Validation loss 1: 0.00001282\n",
      "Epoch [17100/50000] Training Loss: 0.00000148 Testing Loss: 0.00000144 Validation loss 1: 0.00000965\n",
      "Epoch [17200/50000] Training Loss: 0.00000155 Testing Loss: 0.00000160 Validation loss 1: 0.00001041\n",
      "Epoch [17300/50000] Training Loss: 0.00000199 Testing Loss: 0.00000182 Validation loss 1: 0.00001436\n",
      "Epoch [17400/50000] Training Loss: 0.00000173 Testing Loss: 0.00000198 Validation loss 1: 0.00002840\n",
      "Epoch [17500/50000] Training Loss: 0.00000473 Testing Loss: 0.00000459 Validation loss 1: 0.00006214\n",
      "Epoch [17600/50000] Training Loss: 0.00000340 Testing Loss: 0.00000341 Validation loss 1: 0.00004720\n",
      "Epoch [17700/50000] Training Loss: 0.00000135 Testing Loss: 0.00000131 Validation loss 1: 0.00000532\n",
      "Epoch [17800/50000] Training Loss: 0.00000161 Testing Loss: 0.00000169 Validation loss 1: 0.00001533\n",
      "Epoch [17900/50000] Training Loss: 0.00000379 Testing Loss: 0.00000407 Validation loss 1: 0.00006172\n",
      "Epoch [18000/50000] Training Loss: 0.00000181 Testing Loss: 0.00000214 Validation loss 1: 0.00002766\n",
      "Epoch [18100/50000] Training Loss: 0.00000216 Testing Loss: 0.00000247 Validation loss 1: 0.00003690\n",
      "Epoch [18200/50000] Training Loss: 0.00000169 Testing Loss: 0.00000191 Validation loss 1: 0.00002587\n",
      "Epoch [18300/50000] Training Loss: 0.00000123 Testing Loss: 0.00000125 Validation loss 1: 0.00000486\n",
      "Epoch [18400/50000] Training Loss: 0.00000129 Testing Loss: 0.00000132 Validation loss 1: 0.00000579\n",
      "Epoch [18500/50000] Training Loss: 0.00000129 Testing Loss: 0.00000129 Validation loss 1: 0.00000644\n",
      "Epoch [18600/50000] Training Loss: 0.00000153 Testing Loss: 0.00000156 Validation loss 1: 0.00001534\n",
      "Epoch [18700/50000] Training Loss: 0.00000176 Testing Loss: 0.00000208 Validation loss 1: 0.00002792\n",
      "Epoch [18800/50000] Training Loss: 0.00000181 Testing Loss: 0.00000197 Validation loss 1: 0.00001332\n",
      "Epoch [18900/50000] Training Loss: 0.00000178 Testing Loss: 0.00000193 Validation loss 1: 0.00002817\n",
      "Epoch [19000/50000] Training Loss: 0.00000161 Testing Loss: 0.00000142 Validation loss 1: 0.00001032\n",
      "Epoch [19100/50000] Training Loss: 0.00000119 Testing Loss: 0.00000118 Validation loss 1: 0.00000420\n",
      "Epoch [19200/50000] Training Loss: 0.00000215 Testing Loss: 0.00000208 Validation loss 1: 0.00002394\n",
      "Epoch [19300/50000] Training Loss: 0.00000159 Testing Loss: 0.00000160 Validation loss 1: 0.00000994\n",
      "Epoch 19381: reducing learning rate of group 0 to 3.4300e-04.\n",
      "Epoch [19400/50000] Training Loss: 0.00000114 Testing Loss: 0.00000116 Validation loss 1: 0.00000376\n",
      "Epoch [19500/50000] Training Loss: 0.00000114 Testing Loss: 0.00000113 Validation loss 1: 0.00000358\n",
      "Epoch [19600/50000] Training Loss: 0.00000114 Testing Loss: 0.00000113 Validation loss 1: 0.00000356\n",
      "Epoch [19700/50000] Training Loss: 0.00000113 Testing Loss: 0.00000112 Validation loss 1: 0.00000354\n",
      "Epoch [19800/50000] Training Loss: 0.00000113 Testing Loss: 0.00000112 Validation loss 1: 0.00000352\n",
      "Epoch [19900/50000] Training Loss: 0.00000113 Testing Loss: 0.00000112 Validation loss 1: 0.00000350\n",
      "Epoch [20000/50000] Training Loss: 0.00000112 Testing Loss: 0.00000111 Validation loss 1: 0.00000348\n",
      "Epoch [20100/50000] Training Loss: 0.00000113 Testing Loss: 0.00000112 Validation loss 1: 0.00000393\n",
      "Epoch [20200/50000] Training Loss: 0.00000113 Testing Loss: 0.00000112 Validation loss 1: 0.00000362\n",
      "Epoch [20300/50000] Training Loss: 0.00000156 Testing Loss: 0.00000144 Validation loss 1: 0.00000968\n",
      "Epoch [20400/50000] Training Loss: 0.00000116 Testing Loss: 0.00000117 Validation loss 1: 0.00000657\n",
      "Epoch [20500/50000] Training Loss: 0.00000111 Testing Loss: 0.00000110 Validation loss 1: 0.00000366\n",
      "Epoch [20600/50000] Training Loss: 0.00000111 Testing Loss: 0.00000110 Validation loss 1: 0.00000346\n",
      "Epoch [20700/50000] Training Loss: 0.00000111 Testing Loss: 0.00000110 Validation loss 1: 0.00000338\n",
      "Epoch [20800/50000] Training Loss: 0.00000111 Testing Loss: 0.00000110 Validation loss 1: 0.00000363\n",
      "Epoch [20900/50000] Training Loss: 0.00000161 Testing Loss: 0.00000153 Validation loss 1: 0.00001071\n",
      "Epoch [21000/50000] Training Loss: 0.00000121 Testing Loss: 0.00000120 Validation loss 1: 0.00000596\n",
      "Epoch [21100/50000] Training Loss: 0.00000109 Testing Loss: 0.00000109 Validation loss 1: 0.00000346\n",
      "Epoch [21200/50000] Training Loss: 0.00000114 Testing Loss: 0.00000114 Validation loss 1: 0.00000523\n",
      "Epoch [21300/50000] Training Loss: 0.00000138 Testing Loss: 0.00000149 Validation loss 1: 0.00001085\n",
      "Epoch [21400/50000] Training Loss: 0.00000108 Testing Loss: 0.00000107 Validation loss 1: 0.00000334\n",
      "Epoch [21500/50000] Training Loss: 0.00000124 Testing Loss: 0.00000125 Validation loss 1: 0.00000978\n",
      "Epoch [21600/50000] Training Loss: 0.00000138 Testing Loss: 0.00000148 Validation loss 1: 0.00001123\n",
      "Epoch [21700/50000] Training Loss: 0.00000162 Testing Loss: 0.00000185 Validation loss 1: 0.00001808\n",
      "Epoch [21800/50000] Training Loss: 0.00000119 Testing Loss: 0.00000123 Validation loss 1: 0.00000749\n",
      "Epoch [21900/50000] Training Loss: 0.00000186 Testing Loss: 0.00000131 Validation loss 1: 0.00000855\n",
      "Epoch [22000/50000] Training Loss: 0.00000227 Testing Loss: 0.00000232 Validation loss 1: 0.00003433\n",
      "Epoch [22100/50000] Training Loss: 0.00000111 Testing Loss: 0.00000113 Validation loss 1: 0.00000517\n",
      "Epoch [22200/50000] Training Loss: 0.00000123 Testing Loss: 0.00000116 Validation loss 1: 0.00000523\n",
      "Epoch [22300/50000] Training Loss: 0.00000111 Testing Loss: 0.00000111 Validation loss 1: 0.00000420\n",
      "Epoch [22400/50000] Training Loss: 0.00000379 Testing Loss: 0.00000354 Validation loss 1: 0.00005131\n",
      "Epoch [22500/50000] Training Loss: 0.00000163 Testing Loss: 0.00000267 Validation loss 1: 0.00004353\n",
      "Epoch [22600/50000] Training Loss: 0.00000126 Testing Loss: 0.00000136 Validation loss 1: 0.00001054\n",
      "Epoch [22700/50000] Training Loss: 0.00000149 Testing Loss: 0.00000150 Validation loss 1: 0.00001547\n",
      "Epoch [22800/50000] Training Loss: 0.00000109 Testing Loss: 0.00000107 Validation loss 1: 0.00000445\n",
      "Epoch [22900/50000] Training Loss: 0.00000416 Testing Loss: 0.00000279 Validation loss 1: 0.00003273\n",
      "Epoch [23000/50000] Training Loss: 0.00000145 Testing Loss: 0.00000137 Validation loss 1: 0.00000886\n",
      "Epoch [23100/50000] Training Loss: 0.00000123 Testing Loss: 0.00000116 Validation loss 1: 0.00000636\n",
      "Epoch [23200/50000] Training Loss: 0.00000453 Testing Loss: 0.00000421 Validation loss 1: 0.00007808\n",
      "Epoch [23300/50000] Training Loss: 0.00000105 Testing Loss: 0.00000104 Validation loss 1: 0.00000359\n",
      "Epoch [23400/50000] Training Loss: 0.00000108 Testing Loss: 0.00000107 Validation loss 1: 0.00000449\n",
      "Epoch [23500/50000] Training Loss: 0.00000123 Testing Loss: 0.00000120 Validation loss 1: 0.00000750\n",
      "Epoch [23600/50000] Training Loss: 0.00000157 Testing Loss: 0.00000148 Validation loss 1: 0.00001363\n",
      "Epoch [23700/50000] Training Loss: 0.00000102 Testing Loss: 0.00000103 Validation loss 1: 0.00000298\n",
      "Epoch [23800/50000] Training Loss: 0.00000131 Testing Loss: 0.00000130 Validation loss 1: 0.00000930\n",
      "Epoch [23900/50000] Training Loss: 0.00000104 Testing Loss: 0.00000102 Validation loss 1: 0.00000305\n",
      "Epoch [24000/50000] Training Loss: 0.00000102 Testing Loss: 0.00000101 Validation loss 1: 0.00000301\n",
      "Epoch [24100/50000] Training Loss: 0.00000103 Testing Loss: 0.00000102 Validation loss 1: 0.00000317\n",
      "Epoch [24200/50000] Training Loss: 0.00000101 Testing Loss: 0.00000101 Validation loss 1: 0.00000313\n",
      "Epoch [24300/50000] Training Loss: 0.00000129 Testing Loss: 0.00000120 Validation loss 1: 0.00000788\n",
      "Epoch [24400/50000] Training Loss: 0.00000104 Testing Loss: 0.00000102 Validation loss 1: 0.00000295\n",
      "Epoch [24500/50000] Training Loss: 0.00000571 Testing Loss: 0.00000438 Validation loss 1: 0.00007503\n",
      "Epoch [24600/50000] Training Loss: 0.00000151 Testing Loss: 0.00000175 Validation loss 1: 0.00001944\n",
      "Epoch [24700/50000] Training Loss: 0.00000112 Testing Loss: 0.00000112 Validation loss 1: 0.00000678\n",
      "Epoch [24800/50000] Training Loss: 0.00000100 Testing Loss: 0.00000100 Validation loss 1: 0.00000283\n",
      "Epoch [24900/50000] Training Loss: 0.00000258 Testing Loss: 0.00000206 Validation loss 1: 0.00002952\n",
      "Epoch [25000/50000] Training Loss: 0.00000226 Testing Loss: 0.00000203 Validation loss 1: 0.00002870\n",
      "Epoch [25100/50000] Training Loss: 0.00000117 Testing Loss: 0.00000109 Validation loss 1: 0.00000604\n",
      "Epoch [25200/50000] Training Loss: 0.00000113 Testing Loss: 0.00000105 Validation loss 1: 0.00000399\n",
      "Epoch [25300/50000] Training Loss: 0.00000133 Testing Loss: 0.00000123 Validation loss 1: 0.00000821\n",
      "Epoch [25400/50000] Training Loss: 0.00000281 Testing Loss: 0.00000205 Validation loss 1: 0.00002176\n",
      "Epoch [25500/50000] Training Loss: 0.00000133 Testing Loss: 0.00000144 Validation loss 1: 0.00001003\n",
      "Epoch [25600/50000] Training Loss: 0.00000109 Testing Loss: 0.00000108 Validation loss 1: 0.00000599\n",
      "Epoch [25700/50000] Training Loss: 0.00000109 Testing Loss: 0.00000112 Validation loss 1: 0.00000574\n",
      "Epoch [25800/50000] Training Loss: 0.00000159 Testing Loss: 0.00000111 Validation loss 1: 0.00000618\n",
      "Epoch [25900/50000] Training Loss: 0.00000119 Testing Loss: 0.00000114 Validation loss 1: 0.00000669\n",
      "Epoch [26000/50000] Training Loss: 0.00000150 Testing Loss: 0.00000147 Validation loss 1: 0.00001708\n",
      "Epoch [26100/50000] Training Loss: 0.00000171 Testing Loss: 0.00000178 Validation loss 1: 0.00002124\n",
      "Epoch [26200/50000] Training Loss: 0.00000337 Testing Loss: 0.00000354 Validation loss 1: 0.00004894\n",
      "Epoch [26300/50000] Training Loss: 0.00000117 Testing Loss: 0.00000118 Validation loss 1: 0.00000895\n",
      "Epoch [26400/50000] Training Loss: 0.00000100 Testing Loss: 0.00000099 Validation loss 1: 0.00000386\n",
      "Epoch [26500/50000] Training Loss: 0.00000106 Testing Loss: 0.00000104 Validation loss 1: 0.00000462\n",
      "Epoch [26600/50000] Training Loss: 0.00000426 Testing Loss: 0.00000358 Validation loss 1: 0.00005664\n",
      "Epoch [26700/50000] Training Loss: 0.00000204 Testing Loss: 0.00000174 Validation loss 1: 0.00001614\n",
      "Epoch [26800/50000] Training Loss: 0.00000103 Testing Loss: 0.00000100 Validation loss 1: 0.00000419\n",
      "Epoch [26900/50000] Training Loss: 0.00000115 Testing Loss: 0.00000120 Validation loss 1: 0.00000703\n",
      "Epoch [27000/50000] Training Loss: 0.00000474 Testing Loss: 0.00000528 Validation loss 1: 0.00008682\n",
      "Epoch [27100/50000] Training Loss: 0.00000257 Testing Loss: 0.00000221 Validation loss 1: 0.00003261\n",
      "Epoch [27200/50000] Training Loss: 0.00000120 Testing Loss: 0.00000130 Validation loss 1: 0.00000890\n",
      "Epoch [27300/50000] Training Loss: 0.00000097 Testing Loss: 0.00000095 Validation loss 1: 0.00000279\n",
      "Epoch [27400/50000] Training Loss: 0.00000097 Testing Loss: 0.00000097 Validation loss 1: 0.00000335\n",
      "Epoch [27500/50000] Training Loss: 0.00000096 Testing Loss: 0.00000096 Validation loss 1: 0.00000295\n",
      "Epoch [27600/50000] Training Loss: 0.00000265 Testing Loss: 0.00000285 Validation loss 1: 0.00004538\n",
      "Epoch [27700/50000] Training Loss: 0.00000182 Testing Loss: 0.00000127 Validation loss 1: 0.00001289\n",
      "Epoch [27800/50000] Training Loss: 0.00000098 Testing Loss: 0.00000098 Validation loss 1: 0.00000403\n",
      "Epoch [27900/50000] Training Loss: 0.00000094 Testing Loss: 0.00000093 Validation loss 1: 0.00000264\n",
      "Epoch [28000/50000] Training Loss: 0.00000105 Testing Loss: 0.00000103 Validation loss 1: 0.00000475\n",
      "Epoch [28100/50000] Training Loss: 0.00000099 Testing Loss: 0.00000100 Validation loss 1: 0.00000408\n",
      "Epoch [28200/50000] Training Loss: 0.00000125 Testing Loss: 0.00000124 Validation loss 1: 0.00000970\n",
      "Epoch [28300/50000] Training Loss: 0.00000148 Testing Loss: 0.00000159 Validation loss 1: 0.00002650\n",
      "Epoch [28400/50000] Training Loss: 0.00000247 Testing Loss: 0.00000257 Validation loss 1: 0.00004777\n",
      "Epoch [28500/50000] Training Loss: 0.00000321 Testing Loss: 0.00000224 Validation loss 1: 0.00003089\n",
      "Epoch [28600/50000] Training Loss: 0.00000098 Testing Loss: 0.00000097 Validation loss 1: 0.00000382\n",
      "Epoch [28700/50000] Training Loss: 0.00000105 Testing Loss: 0.00000101 Validation loss 1: 0.00000516\n",
      "Epoch [28800/50000] Training Loss: 0.00000093 Testing Loss: 0.00000093 Validation loss 1: 0.00000305\n",
      "Epoch [28900/50000] Training Loss: 0.00000185 Testing Loss: 0.00000196 Validation loss 1: 0.00002544\n",
      "Epoch [29000/50000] Training Loss: 0.00000162 Testing Loss: 0.00000118 Validation loss 1: 0.00000815\n",
      "Epoch [29100/50000] Training Loss: 0.00000097 Testing Loss: 0.00000101 Validation loss 1: 0.00000519\n",
      "Epoch [29200/50000] Training Loss: 0.00000092 Testing Loss: 0.00000092 Validation loss 1: 0.00000295\n",
      "Epoch [29300/50000] Training Loss: 0.00000092 Testing Loss: 0.00000091 Validation loss 1: 0.00000262\n",
      "Epoch [29400/50000] Training Loss: 0.00000092 Testing Loss: 0.00000092 Validation loss 1: 0.00000271\n",
      "Epoch [29500/50000] Training Loss: 0.00000168 Testing Loss: 0.00000164 Validation loss 1: 0.00001502\n",
      "Epoch [29600/50000] Training Loss: 0.00000144 Testing Loss: 0.00000154 Validation loss 1: 0.00001671\n",
      "Epoch [29700/50000] Training Loss: 0.00000142 Testing Loss: 0.00000159 Validation loss 1: 0.00002374\n",
      "Epoch [29800/50000] Training Loss: 0.00000115 Testing Loss: 0.00000102 Validation loss 1: 0.00000572\n",
      "Epoch [29900/50000] Training Loss: 0.00000091 Testing Loss: 0.00000092 Validation loss 1: 0.00000309\n",
      "Epoch [30000/50000] Training Loss: 0.00000168 Testing Loss: 0.00000155 Validation loss 1: 0.00002106\n",
      "Epoch [30100/50000] Training Loss: 0.00000095 Testing Loss: 0.00000094 Validation loss 1: 0.00000362\n",
      "Epoch [30200/50000] Training Loss: 0.00000093 Testing Loss: 0.00000093 Validation loss 1: 0.00000370\n",
      "Epoch [30300/50000] Training Loss: 0.00000091 Testing Loss: 0.00000090 Validation loss 1: 0.00000299\n",
      "Epoch [30400/50000] Training Loss: 0.00000099 Testing Loss: 0.00000103 Validation loss 1: 0.00000543\n",
      "Epoch [30500/50000] Training Loss: 0.00000097 Testing Loss: 0.00000092 Validation loss 1: 0.00000382\n",
      "Epoch [30600/50000] Training Loss: 0.00000147 Testing Loss: 0.00000150 Validation loss 1: 0.00001743\n",
      "Epoch [30700/50000] Training Loss: 0.00000096 Testing Loss: 0.00000093 Validation loss 1: 0.00000342\n",
      "Epoch [30800/50000] Training Loss: 0.00000089 Testing Loss: 0.00000088 Validation loss 1: 0.00000245\n",
      "Epoch [30900/50000] Training Loss: 0.00000090 Testing Loss: 0.00000089 Validation loss 1: 0.00000255\n",
      "Epoch [31000/50000] Training Loss: 0.00000089 Testing Loss: 0.00000091 Validation loss 1: 0.00000342\n",
      "Epoch [31100/50000] Training Loss: 0.00000103 Testing Loss: 0.00000107 Validation loss 1: 0.00000580\n",
      "Epoch [31200/50000] Training Loss: 0.00000111 Testing Loss: 0.00000116 Validation loss 1: 0.00000692\n",
      "Epoch [31300/50000] Training Loss: 0.00000122 Testing Loss: 0.00000122 Validation loss 1: 0.00001041\n",
      "Epoch [31400/50000] Training Loss: 0.00000088 Testing Loss: 0.00000088 Validation loss 1: 0.00000231\n",
      "Epoch [31500/50000] Training Loss: 0.00000171 Testing Loss: 0.00000204 Validation loss 1: 0.00003204\n",
      "Epoch [31600/50000] Training Loss: 0.00000166 Testing Loss: 0.00000103 Validation loss 1: 0.00000585\n",
      "Epoch [31700/50000] Training Loss: 0.00000087 Testing Loss: 0.00000087 Validation loss 1: 0.00000251\n",
      "Epoch [31800/50000] Training Loss: 0.00000095 Testing Loss: 0.00000094 Validation loss 1: 0.00000380\n",
      "Epoch [31900/50000] Training Loss: 0.00000090 Testing Loss: 0.00000088 Validation loss 1: 0.00000247\n",
      "Epoch [32000/50000] Training Loss: 0.00000091 Testing Loss: 0.00000087 Validation loss 1: 0.00000254\n",
      "Epoch [32100/50000] Training Loss: 0.00000093 Testing Loss: 0.00000092 Validation loss 1: 0.00000385\n",
      "Epoch [32200/50000] Training Loss: 0.00000088 Testing Loss: 0.00000087 Validation loss 1: 0.00000238\n",
      "Epoch [32300/50000] Training Loss: 0.00000090 Testing Loss: 0.00000089 Validation loss 1: 0.00000275\n",
      "Epoch [32400/50000] Training Loss: 0.00000093 Testing Loss: 0.00000096 Validation loss 1: 0.00000411\n",
      "Epoch [32500/50000] Training Loss: 0.00000094 Testing Loss: 0.00000094 Validation loss 1: 0.00000430\n",
      "Epoch [32600/50000] Training Loss: 0.00000087 Testing Loss: 0.00000087 Validation loss 1: 0.00000240\n",
      "Epoch [32700/50000] Training Loss: 0.00000113 Testing Loss: 0.00000106 Validation loss 1: 0.00000690\n",
      "Epoch [32800/50000] Training Loss: 0.00000092 Testing Loss: 0.00000095 Validation loss 1: 0.00000541\n",
      "Epoch [32900/50000] Training Loss: 0.00000087 Testing Loss: 0.00000088 Validation loss 1: 0.00000321\n",
      "Epoch [33000/50000] Training Loss: 0.00000085 Testing Loss: 0.00000085 Validation loss 1: 0.00000229\n",
      "Epoch [33100/50000] Training Loss: 0.00000236 Testing Loss: 0.00000105 Validation loss 1: 0.00000906\n",
      "Epoch [33200/50000] Training Loss: 0.00000122 Testing Loss: 0.00000112 Validation loss 1: 0.00001391\n",
      "Epoch [33300/50000] Training Loss: 0.00000094 Testing Loss: 0.00000092 Validation loss 1: 0.00000403\n",
      "Epoch [33400/50000] Training Loss: 0.00000088 Testing Loss: 0.00000088 Validation loss 1: 0.00000308\n",
      "Epoch [33500/50000] Training Loss: 0.00000091 Testing Loss: 0.00000090 Validation loss 1: 0.00000477\n",
      "Epoch [33600/50000] Training Loss: 0.00000085 Testing Loss: 0.00000084 Validation loss 1: 0.00000235\n",
      "Epoch [33700/50000] Training Loss: 0.00000099 Testing Loss: 0.00000101 Validation loss 1: 0.00000643\n",
      "Epoch [33800/50000] Training Loss: 0.00000185 Testing Loss: 0.00000177 Validation loss 1: 0.00002169\n",
      "Epoch [33900/50000] Training Loss: 0.00000085 Testing Loss: 0.00000084 Validation loss 1: 0.00000237\n",
      "Epoch [34000/50000] Training Loss: 0.00000086 Testing Loss: 0.00000086 Validation loss 1: 0.00000258\n",
      "Epoch [34100/50000] Training Loss: 0.00000084 Testing Loss: 0.00000083 Validation loss 1: 0.00000210\n",
      "Epoch [34200/50000] Training Loss: 0.00000151 Testing Loss: 0.00000204 Validation loss 1: 0.00002644\n",
      "Epoch [34300/50000] Training Loss: 0.00000105 Testing Loss: 0.00000115 Validation loss 1: 0.00000839\n",
      "Epoch [34400/50000] Training Loss: 0.00000084 Testing Loss: 0.00000083 Validation loss 1: 0.00000228\n",
      "Epoch [34500/50000] Training Loss: 0.00000107 Testing Loss: 0.00000103 Validation loss 1: 0.00000617\n",
      "Epoch [34600/50000] Training Loss: 0.00000136 Testing Loss: 0.00000129 Validation loss 1: 0.00000839\n",
      "Epoch [34700/50000] Training Loss: 0.00000088 Testing Loss: 0.00000088 Validation loss 1: 0.00000292\n",
      "Epoch [34800/50000] Training Loss: 0.00000083 Testing Loss: 0.00000082 Validation loss 1: 0.00000213\n",
      "Epoch [34900/50000] Training Loss: 0.00000093 Testing Loss: 0.00000093 Validation loss 1: 0.00000527\n",
      "Epoch [35000/50000] Training Loss: 0.00000096 Testing Loss: 0.00000101 Validation loss 1: 0.00000632\n",
      "Epoch [35100/50000] Training Loss: 0.00000152 Testing Loss: 0.00000127 Validation loss 1: 0.00001657\n",
      "Epoch [35200/50000] Training Loss: 0.00000084 Testing Loss: 0.00000085 Validation loss 1: 0.00000276\n",
      "Epoch [35300/50000] Training Loss: 0.00000116 Testing Loss: 0.00000112 Validation loss 1: 0.00000656\n",
      "Epoch [35400/50000] Training Loss: 0.00000107 Testing Loss: 0.00000105 Validation loss 1: 0.00000676\n",
      "Epoch [35500/50000] Training Loss: 0.00000148 Testing Loss: 0.00000164 Validation loss 1: 0.00001797\n",
      "Epoch [35600/50000] Training Loss: 0.00000091 Testing Loss: 0.00000090 Validation loss 1: 0.00000370\n",
      "Epoch [35700/50000] Training Loss: 0.00000092 Testing Loss: 0.00000089 Validation loss 1: 0.00000437\n",
      "Epoch [35800/50000] Training Loss: 0.00000083 Testing Loss: 0.00000082 Validation loss 1: 0.00000214\n",
      "Epoch [35900/50000] Training Loss: 0.00000094 Testing Loss: 0.00000098 Validation loss 1: 0.00000537\n",
      "Epoch [36000/50000] Training Loss: 0.00000226 Testing Loss: 0.00000225 Validation loss 1: 0.00003147\n",
      "Epoch [36100/50000] Training Loss: 0.00000100 Testing Loss: 0.00000106 Validation loss 1: 0.00000877\n",
      "Epoch [36200/50000] Training Loss: 0.00000093 Testing Loss: 0.00000095 Validation loss 1: 0.00000456\n",
      "Epoch [36300/50000] Training Loss: 0.00000082 Testing Loss: 0.00000081 Validation loss 1: 0.00000204\n",
      "Epoch [36400/50000] Training Loss: 0.00000088 Testing Loss: 0.00000091 Validation loss 1: 0.00000462\n",
      "Epoch [36500/50000] Training Loss: 0.00000155 Testing Loss: 0.00000117 Validation loss 1: 0.00000979\n",
      "Epoch [36600/50000] Training Loss: 0.00000095 Testing Loss: 0.00000095 Validation loss 1: 0.00000446\n",
      "Epoch [36700/50000] Training Loss: 0.00000409 Testing Loss: 0.00000447 Validation loss 1: 0.00007607\n",
      "Epoch [36800/50000] Training Loss: 0.00000080 Testing Loss: 0.00000080 Validation loss 1: 0.00000205\n",
      "Epoch [36900/50000] Training Loss: 0.00000092 Testing Loss: 0.00000094 Validation loss 1: 0.00000488\n",
      "Epoch [37000/50000] Training Loss: 0.00000081 Testing Loss: 0.00000081 Validation loss 1: 0.00000232\n",
      "Epoch [37100/50000] Training Loss: 0.00000529 Testing Loss: 0.00000494 Validation loss 1: 0.00008140\n",
      "Epoch [37200/50000] Training Loss: 0.00000143 Testing Loss: 0.00000151 Validation loss 1: 0.00001641\n",
      "Epoch [37300/50000] Training Loss: 0.00000082 Testing Loss: 0.00000081 Validation loss 1: 0.00000251\n",
      "Epoch 37388: reducing learning rate of group 0 to 2.4010e-04.\n",
      "Epoch [37400/50000] Training Loss: 0.00000080 Testing Loss: 0.00000096 Validation loss 1: 0.00000517\n",
      "Epoch [37500/50000] Training Loss: 0.00000079 Testing Loss: 0.00000079 Validation loss 1: 0.00000194\n",
      "Epoch [37600/50000] Training Loss: 0.00000079 Testing Loss: 0.00000079 Validation loss 1: 0.00000192\n",
      "Epoch [37700/50000] Training Loss: 0.00000079 Testing Loss: 0.00000079 Validation loss 1: 0.00000192\n",
      "Epoch [37800/50000] Training Loss: 0.00000079 Testing Loss: 0.00000078 Validation loss 1: 0.00000192\n",
      "Epoch [37900/50000] Training Loss: 0.00000079 Testing Loss: 0.00000078 Validation loss 1: 0.00000191\n",
      "Epoch [38000/50000] Training Loss: 0.00000079 Testing Loss: 0.00000078 Validation loss 1: 0.00000191\n",
      "Epoch [38100/50000] Training Loss: 0.00000078 Testing Loss: 0.00000078 Validation loss 1: 0.00000190\n",
      "Epoch [38200/50000] Training Loss: 0.00000078 Testing Loss: 0.00000078 Validation loss 1: 0.00000190\n",
      "Epoch [38300/50000] Training Loss: 0.00000079 Testing Loss: 0.00000079 Validation loss 1: 0.00000203\n",
      "Epoch [38400/50000] Training Loss: 0.00000191 Testing Loss: 0.00000185 Validation loss 1: 0.00002808\n",
      "Epoch [38500/50000] Training Loss: 0.00000080 Testing Loss: 0.00000086 Validation loss 1: 0.00000402\n",
      "Epoch [38600/50000] Training Loss: 0.00000087 Testing Loss: 0.00000087 Validation loss 1: 0.00000312\n",
      "Epoch [38700/50000] Training Loss: 0.00000079 Testing Loss: 0.00000079 Validation loss 1: 0.00000240\n",
      "Epoch [38800/50000] Training Loss: 0.00000079 Testing Loss: 0.00000079 Validation loss 1: 0.00000205\n",
      "Epoch [38900/50000] Training Loss: 0.00000084 Testing Loss: 0.00000085 Validation loss 1: 0.00000352\n",
      "Epoch [39000/50000] Training Loss: 0.00000213 Testing Loss: 0.00000204 Validation loss 1: 0.00002393\n",
      "Epoch [39100/50000] Training Loss: 0.00000113 Testing Loss: 0.00000128 Validation loss 1: 0.00001298\n",
      "Epoch [39200/50000] Training Loss: 0.00000092 Testing Loss: 0.00000091 Validation loss 1: 0.00000558\n",
      "Epoch [39300/50000] Training Loss: 0.00000079 Testing Loss: 0.00000078 Validation loss 1: 0.00000226\n",
      "Epoch [39400/50000] Training Loss: 0.00000078 Testing Loss: 0.00000078 Validation loss 1: 0.00000211\n",
      "Epoch [39500/50000] Training Loss: 0.00000077 Testing Loss: 0.00000077 Validation loss 1: 0.00000185\n",
      "Epoch [39600/50000] Training Loss: 0.00000077 Testing Loss: 0.00000077 Validation loss 1: 0.00000192\n",
      "Epoch [39700/50000] Training Loss: 0.00000117 Testing Loss: 0.00000143 Validation loss 1: 0.00001582\n",
      "Epoch [39800/50000] Training Loss: 0.00000096 Testing Loss: 0.00000091 Validation loss 1: 0.00000582\n",
      "Epoch [39900/50000] Training Loss: 0.00000078 Testing Loss: 0.00000077 Validation loss 1: 0.00000216\n",
      "Epoch [40000/50000] Training Loss: 0.00000079 Testing Loss: 0.00000079 Validation loss 1: 0.00000266\n",
      "Epoch [40100/50000] Training Loss: 0.00000080 Testing Loss: 0.00000082 Validation loss 1: 0.00000266\n",
      "Epoch [40200/50000] Training Loss: 0.00000110 Testing Loss: 0.00000115 Validation loss 1: 0.00001051\n",
      "Epoch [40300/50000] Training Loss: 0.00000079 Testing Loss: 0.00000077 Validation loss 1: 0.00000184\n",
      "Epoch [40400/50000] Training Loss: 0.00000076 Testing Loss: 0.00000076 Validation loss 1: 0.00000179\n",
      "Epoch [40500/50000] Training Loss: 0.00000078 Testing Loss: 0.00000078 Validation loss 1: 0.00000254\n",
      "Epoch [40600/50000] Training Loss: 0.00000076 Testing Loss: 0.00000076 Validation loss 1: 0.00000185\n",
      "Epoch [40700/50000] Training Loss: 0.00000076 Testing Loss: 0.00000075 Validation loss 1: 0.00000182\n",
      "Epoch [40800/50000] Training Loss: 0.00000139 Testing Loss: 0.00000156 Validation loss 1: 0.00001766\n",
      "Epoch [40900/50000] Training Loss: 0.00000249 Testing Loss: 0.00000208 Validation loss 1: 0.00003075\n",
      "Epoch [41000/50000] Training Loss: 0.00000079 Testing Loss: 0.00000079 Validation loss 1: 0.00000257\n",
      "Epoch [41100/50000] Training Loss: 0.00000087 Testing Loss: 0.00000085 Validation loss 1: 0.00000420\n",
      "Epoch [41200/50000] Training Loss: 0.00000082 Testing Loss: 0.00000085 Validation loss 1: 0.00000421\n",
      "Epoch [41300/50000] Training Loss: 0.00000081 Testing Loss: 0.00000083 Validation loss 1: 0.00000387\n",
      "Epoch [41400/50000] Training Loss: 0.00000107 Testing Loss: 0.00000098 Validation loss 1: 0.00000692\n",
      "Epoch [41500/50000] Training Loss: 0.00000187 Testing Loss: 0.00000133 Validation loss 1: 0.00001332\n",
      "Epoch [41600/50000] Training Loss: 0.00000115 Testing Loss: 0.00000095 Validation loss 1: 0.00000696\n",
      "Epoch [41700/50000] Training Loss: 0.00000079 Testing Loss: 0.00000084 Validation loss 1: 0.00000306\n",
      "Epoch [41800/50000] Training Loss: 0.00000075 Testing Loss: 0.00000075 Validation loss 1: 0.00000186\n",
      "Epoch [41900/50000] Training Loss: 0.00000091 Testing Loss: 0.00000087 Validation loss 1: 0.00000790\n",
      "Epoch [42000/50000] Training Loss: 0.00000077 Testing Loss: 0.00000076 Validation loss 1: 0.00000225\n",
      "Epoch [42100/50000] Training Loss: 0.00000076 Testing Loss: 0.00000075 Validation loss 1: 0.00000200\n",
      "Epoch [42200/50000] Training Loss: 0.00000079 Testing Loss: 0.00000082 Validation loss 1: 0.00000318\n",
      "Epoch [42300/50000] Training Loss: 0.00000142 Testing Loss: 0.00000146 Validation loss 1: 0.00002130\n",
      "Epoch [42400/50000] Training Loss: 0.00000109 Testing Loss: 0.00000107 Validation loss 1: 0.00000791\n",
      "Epoch [42500/50000] Training Loss: 0.00000093 Testing Loss: 0.00000088 Validation loss 1: 0.00000516\n",
      "Epoch [42600/50000] Training Loss: 0.00000076 Testing Loss: 0.00000076 Validation loss 1: 0.00000257\n",
      "Epoch [42700/50000] Training Loss: 0.00000077 Testing Loss: 0.00000078 Validation loss 1: 0.00000276\n",
      "Epoch [42800/50000] Training Loss: 0.00000075 Testing Loss: 0.00000074 Validation loss 1: 0.00000179\n",
      "Epoch [42900/50000] Training Loss: 0.00000073 Testing Loss: 0.00000073 Validation loss 1: 0.00000175\n",
      "Epoch [43000/50000] Training Loss: 0.00000081 Testing Loss: 0.00000078 Validation loss 1: 0.00000329\n",
      "Epoch [43100/50000] Training Loss: 0.00000151 Testing Loss: 0.00000152 Validation loss 1: 0.00001832\n",
      "Epoch [43200/50000] Training Loss: 0.00000077 Testing Loss: 0.00000081 Validation loss 1: 0.00000377\n",
      "Epoch [43300/50000] Training Loss: 0.00000077 Testing Loss: 0.00000077 Validation loss 1: 0.00000311\n",
      "Epoch [43400/50000] Training Loss: 0.00000076 Testing Loss: 0.00000076 Validation loss 1: 0.00000241\n",
      "Epoch [43500/50000] Training Loss: 0.00000075 Testing Loss: 0.00000075 Validation loss 1: 0.00000209\n",
      "Epoch [43600/50000] Training Loss: 0.00000075 Testing Loss: 0.00000075 Validation loss 1: 0.00000194\n",
      "Epoch [43700/50000] Training Loss: 0.00000083 Testing Loss: 0.00000080 Validation loss 1: 0.00000446\n",
      "Epoch [43800/50000] Training Loss: 0.00000086 Testing Loss: 0.00000086 Validation loss 1: 0.00000473\n",
      "Epoch [43900/50000] Training Loss: 0.00000087 Testing Loss: 0.00000088 Validation loss 1: 0.00000654\n",
      "Epoch [44000/50000] Training Loss: 0.00000073 Testing Loss: 0.00000073 Validation loss 1: 0.00000189\n",
      "Epoch [44100/50000] Training Loss: 0.00000086 Testing Loss: 0.00000086 Validation loss 1: 0.00000460\n",
      "Epoch [44200/50000] Training Loss: 0.00000086 Testing Loss: 0.00000089 Validation loss 1: 0.00000387\n",
      "Epoch [44300/50000] Training Loss: 0.00000086 Testing Loss: 0.00000084 Validation loss 1: 0.00000467\n",
      "Epoch [44400/50000] Training Loss: 0.00000072 Testing Loss: 0.00000072 Validation loss 1: 0.00000165\n",
      "Epoch [44500/50000] Training Loss: 0.00000074 Testing Loss: 0.00000074 Validation loss 1: 0.00000214\n",
      "Epoch [44600/50000] Training Loss: 0.00000080 Testing Loss: 0.00000082 Validation loss 1: 0.00000320\n",
      "Epoch [44700/50000] Training Loss: 0.00000073 Testing Loss: 0.00000075 Validation loss 1: 0.00000259\n",
      "Epoch [44800/50000] Training Loss: 0.00000072 Testing Loss: 0.00000075 Validation loss 1: 0.00000250\n",
      "Epoch [44900/50000] Training Loss: 0.00000082 Testing Loss: 0.00000080 Validation loss 1: 0.00000410\n",
      "Epoch [45000/50000] Training Loss: 0.00000085 Testing Loss: 0.00000084 Validation loss 1: 0.00000427\n",
      "Epoch [45100/50000] Training Loss: 0.00000077 Testing Loss: 0.00000079 Validation loss 1: 0.00000338\n",
      "Epoch [45200/50000] Training Loss: 0.00000073 Testing Loss: 0.00000073 Validation loss 1: 0.00000234\n",
      "Epoch [45300/50000] Training Loss: 0.00000073 Testing Loss: 0.00000072 Validation loss 1: 0.00000184\n",
      "Epoch [45400/50000] Training Loss: 0.00000078 Testing Loss: 0.00000077 Validation loss 1: 0.00000260\n",
      "Epoch [45500/50000] Training Loss: 0.00000155 Testing Loss: 0.00000139 Validation loss 1: 0.00001251\n",
      "Epoch [45600/50000] Training Loss: 0.00000073 Testing Loss: 0.00000071 Validation loss 1: 0.00000169\n",
      "Epoch [45700/50000] Training Loss: 0.00000081 Testing Loss: 0.00000079 Validation loss 1: 0.00000285\n",
      "Epoch [45800/50000] Training Loss: 0.00000079 Testing Loss: 0.00000081 Validation loss 1: 0.00000426\n",
      "Epoch [45900/50000] Training Loss: 0.00000080 Testing Loss: 0.00000076 Validation loss 1: 0.00000242\n",
      "Epoch [46000/50000] Training Loss: 0.00000077 Testing Loss: 0.00000077 Validation loss 1: 0.00000325\n",
      "Epoch [46100/50000] Training Loss: 0.00000078 Testing Loss: 0.00000079 Validation loss 1: 0.00000354\n",
      "Epoch [46200/50000] Training Loss: 0.00000070 Testing Loss: 0.00000070 Validation loss 1: 0.00000161\n",
      "Epoch [46300/50000] Training Loss: 0.00000075 Testing Loss: 0.00000074 Validation loss 1: 0.00000245\n",
      "Epoch [46400/50000] Training Loss: 0.00000089 Testing Loss: 0.00000092 Validation loss 1: 0.00000620\n",
      "Epoch [46500/50000] Training Loss: 0.00000082 Testing Loss: 0.00000081 Validation loss 1: 0.00000378\n",
      "Epoch [46600/50000] Training Loss: 0.00000070 Testing Loss: 0.00000070 Validation loss 1: 0.00000163\n",
      "Epoch [46700/50000] Training Loss: 0.00000110 Testing Loss: 0.00000111 Validation loss 1: 0.00001110\n",
      "Epoch [46800/50000] Training Loss: 0.00000070 Testing Loss: 0.00000071 Validation loss 1: 0.00000193\n",
      "Epoch [46900/50000] Training Loss: 0.00000078 Testing Loss: 0.00000078 Validation loss 1: 0.00000358\n",
      "Epoch [47000/50000] Training Loss: 0.00000070 Testing Loss: 0.00000070 Validation loss 1: 0.00000182\n",
      "Epoch [47100/50000] Training Loss: 0.00000074 Testing Loss: 0.00000077 Validation loss 1: 0.00000310\n",
      "Epoch [47200/50000] Training Loss: 0.00000077 Testing Loss: 0.00000077 Validation loss 1: 0.00000335\n",
      "Epoch [47300/50000] Training Loss: 0.00000083 Testing Loss: 0.00000086 Validation loss 1: 0.00000770\n",
      "Epoch [47400/50000] Training Loss: 0.00000083 Testing Loss: 0.00000074 Validation loss 1: 0.00000277\n",
      "Epoch [47500/50000] Training Loss: 0.00000110 Testing Loss: 0.00000105 Validation loss 1: 0.00001027\n",
      "Epoch [47600/50000] Training Loss: 0.00000073 Testing Loss: 0.00000072 Validation loss 1: 0.00000225\n",
      "Epoch [47700/50000] Training Loss: 0.00000070 Testing Loss: 0.00000070 Validation loss 1: 0.00000162\n",
      "Epoch [47800/50000] Training Loss: 0.00000069 Testing Loss: 0.00000069 Validation loss 1: 0.00000157\n",
      "Epoch [47900/50000] Training Loss: 0.00000069 Testing Loss: 0.00000069 Validation loss 1: 0.00000166\n",
      "Epoch [48000/50000] Training Loss: 0.00000070 Testing Loss: 0.00000070 Validation loss 1: 0.00000178\n",
      "Epoch [48100/50000] Training Loss: 0.00000070 Testing Loss: 0.00000070 Validation loss 1: 0.00000181\n",
      "Epoch [48200/50000] Training Loss: 0.00000095 Testing Loss: 0.00000103 Validation loss 1: 0.00000683\n",
      "Epoch [48300/50000] Training Loss: 0.00000074 Testing Loss: 0.00000074 Validation loss 1: 0.00000277\n",
      "Epoch [48400/50000] Training Loss: 0.00000079 Testing Loss: 0.00000082 Validation loss 1: 0.00000535\n",
      "Epoch [48500/50000] Training Loss: 0.00000072 Testing Loss: 0.00000071 Validation loss 1: 0.00000229\n",
      "Epoch [48600/50000] Training Loss: 0.00000069 Testing Loss: 0.00000069 Validation loss 1: 0.00000164\n",
      "Epoch [48700/50000] Training Loss: 0.00000082 Testing Loss: 0.00000079 Validation loss 1: 0.00000399\n",
      "Epoch [48800/50000] Training Loss: 0.00000070 Testing Loss: 0.00000070 Validation loss 1: 0.00000187\n",
      "Epoch [48900/50000] Training Loss: 0.00000079 Testing Loss: 0.00000082 Validation loss 1: 0.00000537\n",
      "Epoch [49000/50000] Training Loss: 0.00000071 Testing Loss: 0.00000069 Validation loss 1: 0.00000158\n",
      "Epoch [49100/50000] Training Loss: 0.00000097 Testing Loss: 0.00000102 Validation loss 1: 0.00001177\n",
      "Epoch [49200/50000] Training Loss: 0.00000071 Testing Loss: 0.00000071 Validation loss 1: 0.00000196\n",
      "Epoch [49300/50000] Training Loss: 0.00000150 Testing Loss: 0.00000169 Validation loss 1: 0.00003132\n",
      "Epoch [49400/50000] Training Loss: 0.00000085 Testing Loss: 0.00000086 Validation loss 1: 0.00000508\n",
      "Epoch [49500/50000] Training Loss: 0.00000085 Testing Loss: 0.00000080 Validation loss 1: 0.00000413\n",
      "Epoch [49600/50000] Training Loss: 0.00000071 Testing Loss: 0.00000072 Validation loss 1: 0.00000306\n",
      "Epoch [49700/50000] Training Loss: 0.00000085 Testing Loss: 0.00000078 Validation loss 1: 0.00000294\n",
      "Epoch [49800/50000] Training Loss: 0.00000088 Testing Loss: 0.00000081 Validation loss 1: 0.00000479\n",
      "Epoch [49900/50000] Training Loss: 0.00000200 Testing Loss: 0.00000182 Validation loss 1: 0.00002443\n",
      "Epoch [50000/50000] Training Loss: 0.00000090 Testing Loss: 0.00000087 Validation loss 1: 0.00000690\n"
     ]
    }
   ],
   "source": [
    "net = GreenFun(N*N, N*N).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 600, factor=0.7, verbose=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 2000, verbose=False, delta=1e-8, path='net.pth')\n",
    "num_epochs = 50000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = net(train_f, x, train_tau)\n",
    "    \n",
    "    loss = criterion(outputs, train_u)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_test = net(test_f, x, test_tau)\n",
    "        loss_test = criterion(outputs_test, test_u)\n",
    "\n",
    "        if(epoch+1) % 100 == 0:\n",
    "            outputs_validation1 = net(f_validation1, x, tau_validation1)\n",
    "            loss_validation1 = criterion(outputs_validation1, U_validation1)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {loss.item():.8f} Testing Loss: {loss_test.item():.8f} Validation loss 1: {loss_validation1.item():.8f}\")\n",
    "\n",
    "    # 调整学习率\n",
    "    scheduler.step(loss_test)\n",
    "\n",
    "    early_stopping(loss_test, net)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau       L2 Norm        Max Norm       Relative L2 Norm    Relative Max Norm\n",
      "0.055     7.139e-05      0.00025707     0.00105198          0.00192087\n",
      "0.065     6.736e-05      0.00022984     0.00099262          0.0017174\n",
      "0.075     6.486e-05      0.0002189      0.00095588          0.00163566\n",
      "0.085     6.602e-05      0.0002436      0.00097284          0.0018202\n",
      "0.095     6.893e-05      0.00025473     0.0010158           0.00190338\n",
      "--------------------------------------------------------------------------------\n",
      "0.105     0.0022787      0.00462009     0.03358004          0.03452149\n",
      "0.115     0.00684355     0.01358466     0.10084999          0.10150508\n",
      "0.125     0.01141294     0.02255805     0.16818683          0.16855458\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net.pth\", map_location = device))\n",
    "\n",
    "tau_validation4 = np.array([0.055, 0.065, 0.075, 0.085, 0.095, 0.105, 0.115, 0.125])\n",
    "lll = len(tau_validation4)\n",
    "f_validation4 = np.zeros((lll, N, N))\n",
    "U_validation4 = np.zeros((lll, N, N))\n",
    "\n",
    "for k in range(lll):\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            xx = 1 * j / (N - 1)\n",
    "            yy = 1 * i / (N - 1)\n",
    "\n",
    "            t = tau_validation4[k]\n",
    "        \n",
    "            U_validation4[k, i, j] = xx * (1- xx) * yy * (1- yy) * np.exp(0.6 * xx + 0.8 * yy)\n",
    "            f_validation4[k, i, j] = np.exp(0.6 * xx + 0.8 * yy) * (xx**2*yy**2 + 2.2*xx**2*yy + 1.4*xx*yy**2 + 0.4*xx**2 + 0.8*yy**2 - 4.6*xx*yy - 0.4*xx - 0.8*yy)\n",
    "            f_validation4[k, i, j] = U_validation4[k, i, j] - t * f_validation4[k, i, j]\n",
    "            \n",
    "\n",
    "U_validation4 = torch.tensor(U_validation4, dtype=torch.float32).view(-1, N * N).to(device)\n",
    "f_validation4 = torch.tensor(f_validation4, dtype=torch.float32).view(-1, N * N).to(device)\n",
    "tau_validation4 = torch.tensor(tau_validation4, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "outputs_validation4 = net(f_validation4, x, tau_validation4)\n",
    "error = torch.abs(U_validation4 - outputs_validation4)\n",
    "\n",
    "# 初始化存储范数的列表\n",
    "norms = []\n",
    "\n",
    "# 计算每一行的二范数和最大范数\n",
    "for i in range(error.shape[0]):\n",
    "    row = error[i, :]\n",
    "    l2_norm = torch.norm(row, p=2).item() / np.sqrt(len(row))\n",
    "    max_norm = torch.norm(row, p=float('inf')).item()\n",
    "    l2_norm_relative = torch.norm(row, p=2).item() / torch.norm(U_validation4[i, :], p=2).item()\n",
    "    max_norm_relative = torch.norm(row, p=float('inf')).item() / torch.norm(U_validation4[i, :], p=float('inf')).item()\n",
    "    norms.append([round(tau_validation4[i].cpu().item(), 4), round(l2_norm, 8), round(max_norm, 8), round(l2_norm_relative, 8), round(max_norm_relative, 8)])\n",
    "\n",
    "# 打印表格\n",
    "flag = 0\n",
    "print(f\"{'tau':<10}{'L2 Norm':<15}{'Max Norm':<15}{'Relative L2 Norm':<20}{'Relative Max Norm'}\")\n",
    "for row in norms:\n",
    "    if row[0] > 0.1 and flag == 0:\n",
    "        print(\"--\"*40)\n",
    "        flag = 1\n",
    "    print(f\"{row[0]:<10}{row[1]:<15}{row[2]:<15}{row[3]:<20}{row[4]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyc_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
