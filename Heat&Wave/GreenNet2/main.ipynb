{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU memory: 24217.31 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu_info.name}\")\n",
    "    print(f\"GPU memory: {gpu_info.total_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GreenFun(nn.Module):\n",
    "    def __init__(self, N, N_quadrature):     \n",
    "        super(GreenFun, self).__init__()\n",
    "        self.N = N\n",
    "        self.N_quad = N_quadrature\n",
    "        self.tau_layer = nn.Sequential(nn.Linear(1, (N_quadrature - 2) ** 2), nn.ReLU(), nn.Linear((N_quadrature - 2) ** 2, (N_quadrature - 2) ** 2), nn.ReLU(), nn.Linear((N_quadrature - 2) ** 2, (N_quadrature - 2) ** 2), nn.ReLU(), nn.Linear((N_quadrature - 2) ** 2, (N_quadrature - 2) ** 2))\n",
    "        self.G_layer = nn.Sequential(nn.Linear(2, (N_quadrature - 2) ** 2), nn.ReLU(), nn.Linear((N_quadrature - 2) ** 2, (N_quadrature - 2) ** 2), nn.ReLU(), nn.Linear((N_quadrature - 2) ** 2, (N_quadrature - 2) ** 2), nn.ReLU(), nn.Linear((N_quadrature - 2) ** 2, (N_quadrature - 2) ** 2))\n",
    "\n",
    "    def forward(self, f, x, tau):    # f: (batch_size, N_quad, N_quad), x: (N*N, 2), tau: (batch_size, 1)\n",
    "        f = f[:, 1:-1, 1:-1].contiguous().view(-1, (self.N_quad - 2) ** 2)\n",
    "        T = self.tau_layer(torch.sqrt(tau))\n",
    "        G = self.G_layer(x.reshape(self.N, self.N, 2)[1:-1, 1:-1, :].reshape(-1, 2))     # G is (N, N_quad) with G(i, j) = G((x_i, y_i); (x_quad_j, y_quad_j))\n",
    "        output = torch.matmul(f * T, G.t()) \n",
    "        output = output / self.N_quad \n",
    "        # print(output.shape)\n",
    "        output = output.view(-1, self.N - 2, self.N - 2)\n",
    "        return F.pad(output, (1, 1, 1, 1), mode='constant', value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, N, N_quadrature):     \n",
    "        super(MyNet, self).__init__()\n",
    "        self.N = N\n",
    "        self.N_quad = N_quadrature\n",
    "        self.N2_ = (N - 2) ** 2\n",
    "        self.N_quad2_ = (N - 2) ** 2\n",
    "        self.f_layer1 = nn.Sequential(nn.Linear(self.N_quad2_, self.N2_), nn.ReLU(), nn.Linear(self.N2_, self.N2_), nn.ReLU(), nn.Linear(self.N2_, self.N2_))\n",
    "        self.f_layer2 = nn.Sequential(nn.Linear(self.N_quad2_, self.N2_), nn.ReLU(), nn.Linear(self.N2_, self.N2_), nn.ReLU(), nn.Linear(self.N2_, self.N2_))\n",
    "\n",
    "    def forward(self, f, tau):   # f: (batch_size, N_quad, N_quad), tau: (batch_size, 1)\n",
    "        f = f[:, 1:-1, 1:-1].contiguous().view(-1, (self.N_quad - 2) ** 2)\n",
    "        output = f + tau * self.f_layer1(f) + tau * tau * self.f_layer2(f)     \n",
    "        output = output.view(-1, self.N - 2, self.N - 2)\n",
    "        return F.pad(output, (1, 1, 1, 1), mode='constant', value=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Helmholtz equation\n",
    "$\\begin{cases} u - \\tau \\Delta u  = f, & x \\in \\Omega \\\\ u(x) = 0, & x \\in \\partial \\Omega \\end{cases}$\n",
    "with $\\Omega = [0, 1]^2$.\n",
    "\n",
    "$\\tau \\in [1/128, 1/32]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import random\n",
    "\n",
    "def generate_gaussian_source_terms(num_samples, nx, ny, sigma):\n",
    "    source_terms = []\n",
    "    for _ in range(num_samples):\n",
    "        f = np.random.randn(nx, ny)\n",
    "        f = gaussian_filter(f, sigma=sigma)\n",
    "        source_terms.append(f)\n",
    "    return source_terms\n",
    "\n",
    "\n",
    "def generate_functional_source_terms(num_samples, nx, ny):\n",
    "    source_terms = []\n",
    "    x = np.linspace(0, 1, nx)\n",
    "    y = np.linspace(0, 1, ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        f = a * np.sin(b * np.pi * X) * np.sin(c * np.pi * Y)\n",
    "        source_terms.append(f)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        f = a * np.cos(b * np.pi * X) * np.cos(c * np.pi * Y)\n",
    "        source_terms.append(f)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        f = a * np.sin(b * np.pi * X) * np.cos(c * np.pi * Y)\n",
    "        source_terms.append(f)\n",
    "\n",
    "    for _ in range(num_samples - num_samples // 4 - num_samples // 4 - num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        f = a * np.cos(b * np.pi * X) * np.sin(c * np.pi * Y)\n",
    "        source_terms.append(f)\n",
    "\n",
    "    return source_terms\n",
    "\n",
    "from scipy.fft import dstn, idstn\n",
    "def solve_by_FFT2(tau, f, nx, ny, dx, dy):\n",
    "    u = np.zeros((nx,ny))\n",
    "    lam = np.zeros((nx-2,ny-2))\n",
    "    for i in range(nx-2):\n",
    "        for j in range(ny-2):\n",
    "            lam[i,j] = 2 * (np.cos((i+1)*np.pi*dx)+np.cos((j+1)*np.pi*dy)-2) / (dx*dy)\n",
    "    \n",
    "    u[1:-1, 1:-1] = idstn(dstn(f[1:-1,1:-1],type=1)/(1 - tau * lam),type=1)\n",
    "    \n",
    "    return 1, u\n",
    "\n",
    "\n",
    "def select(u, nx, ny, m):       # get low resolution data from high resolution data\n",
    "    result = np.zeros((len(u), nx, ny))\n",
    "    for l in range(len(u)):\n",
    "        for i in range(nx):\n",
    "            for j in range(ny):\n",
    "                result[l, i, j] = u[l, m * i, m * j]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_poisson_data1(tau, n, nx, ny):\n",
    "    m = 2       # for getting higher resolution data\n",
    "    dx, dy = 1 / (nx - 1), 1 / (ny - 1)\n",
    "    nnx = m * (nx - 1) + 1\n",
    "    nny = m * (ny - 1) + 1\n",
    "    ddx, ddy = 1 / (nnx - 1), 1 / (nny - 1)\n",
    "    \n",
    "    source_terms1 = generate_gaussian_source_terms(n // 10, nx, ny, 1)\n",
    "    source_terms2 = generate_gaussian_source_terms(n // 10, nx, ny, 2)\n",
    "    source_terms3 = generate_gaussian_source_terms(n // 10, nx, ny, 3)\n",
    "    source_terms4 = generate_gaussian_source_terms(n // 10, nx, ny, 4)\n",
    "    source_terms5 = generate_gaussian_source_terms(n // 10, nx, ny, 5)\n",
    "    source_terms6 = generate_functional_source_terms(n - 5 * (n // 10), nnx, nny)\n",
    "    \n",
    "    solutions1 = []\n",
    "    solutions2 = []\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for f in source_terms1:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms2:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms3:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms4:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms5:\n",
    "        success, u = solve_by_FFT2(tau, f, nx, ny, dx, dy)\n",
    "        if success == 1:\n",
    "            solutions1.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    for f in source_terms6:\n",
    "        success, u = solve_by_FFT2(tau, f, nnx, nny, ddx, ddy)\n",
    "        if success == 1:\n",
    "            solutions2.append(u)\n",
    "\n",
    "        count += 1\n",
    "        if (count % 100 == 0):\n",
    "            print(\"finish\", count, \"of\", n)\n",
    "\n",
    "    a = np.array(source_terms1 + source_terms2 + source_terms3 + source_terms4 + source_terms5)\n",
    "    b = select(np.array(source_terms6), nx, ny, m)\n",
    "    c = np.array(solutions1)\n",
    "    d = select(np.array(solutions2), nx, ny, m)\n",
    "    \n",
    "    return np.stack([a, b], axis=0), np.stack([c, d], axis=0)\n",
    "\n",
    "\n",
    "def generate_poisson_data2(tau, n, nx, ny):\n",
    "    f = []\n",
    "    u = []\n",
    "    ttau = []\n",
    "\n",
    "    count = 1\n",
    "    for t in tau:\n",
    "        temp_tau = np.zeros((n, 1)) + t\n",
    "        temp_f, temp_u = generate_poisson_data1(t, n, nx, ny)\n",
    "        print(\"Finish\", count, \"of\", len(tau), \"tau's\")\n",
    "        f.append(temp_f)\n",
    "        u.append(temp_u)\n",
    "        ttau.append(temp_tau)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return np.concatenate(f, axis=0), np.concatenate(u, axis=0), np.concatenate(ttau, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0078125 , 0.00888889, 0.01003472, 0.01125   , 0.01253472,\n",
       "       0.01388889, 0.0153125 , 0.01680556, 0.01836806, 0.02      ,\n",
       "       0.02170139, 0.02347222, 0.0253125 , 0.02722222, 0.02920139,\n",
       "       0.03125   ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 41\n",
    "# tau = np.linspace(1/128, 1/32, 21)\n",
    "# tau = np.linspace(0.75/128, 4.25/128, 15)\n",
    "tau = np.linspace(1/8, 1/4, 16)\n",
    "tau = tau ** 2 / 2\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 1 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 2 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 3 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 4 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 5 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 6 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 7 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 8 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 9 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 10 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 11 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 12 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 13 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 14 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 15 of 16 tau's\n",
      "finish 100 of 2000\n",
      "finish 200 of 2000\n",
      "finish 300 of 2000\n",
      "finish 400 of 2000\n",
      "finish 500 of 2000\n",
      "finish 600 of 2000\n",
      "finish 700 of 2000\n",
      "finish 800 of 2000\n",
      "finish 900 of 2000\n",
      "finish 1000 of 2000\n",
      "finish 1100 of 2000\n",
      "finish 1200 of 2000\n",
      "finish 1300 of 2000\n",
      "finish 1400 of 2000\n",
      "finish 1500 of 2000\n",
      "finish 1600 of 2000\n",
      "finish 1700 of 2000\n",
      "finish 1800 of 2000\n",
      "finish 1900 of 2000\n",
      "finish 2000 of 2000\n",
      "Finish 16 of 16 tau's\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"f_np.npy\") and os.path.exists(\"u_np.npy\") and os.path.exists(\"tau_np.npy\"):\n",
    "    f_np, u_np, tau_np = np.load('f_np.npy'), np.load('u_np.npy'), np.load('tau_np.npy')\n",
    "else:\n",
    "    f_np, u_np, tau_np = generate_poisson_data2(tau, 2000, N, N)\n",
    "    np.save('f_np.npy', f_np)\n",
    "    np.save('u_np.npy', u_np)\n",
    "    np.save('tau_np.npy', tau_np)\n",
    "\n",
    "f, u, tau = torch.tensor(f_np, dtype=torch.float32).view(-1, N, N), torch.tensor(u_np, dtype=torch.float32).view(-1, N, N), torch.tensor(tau_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32000, 41, 41]),\n",
       " torch.Size([32000, 41, 41]),\n",
       " torch.Size([32000, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape, u.shape, tau.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "dataset = TensorDataset(f, tau, u)\n",
    "\n",
    "# 定义训练集和测试集的大小\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# 将数据集按比例分成训练集和测试集\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 获取训练集的数据\n",
    "train_f = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "train_tau = torch.stack([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "train_u = torch.stack([train_dataset[i][2] for i in range(len(train_dataset))])\n",
    "\n",
    "# 获取测试集的数据\n",
    "test_f = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])\n",
    "test_tau = torch.stack([test_dataset[i][1] for i in range(len(test_dataset))])\n",
    "test_u = torch.stack([test_dataset[i][2] for i in range(len(test_dataset))])\n",
    "\n",
    "train_f = train_f.to(device)\n",
    "train_tau = train_tau.to(device)\n",
    "train_u = train_u.to(device)\n",
    "test_f = test_f.to(device)\n",
    "test_tau = test_tau.to(device)\n",
    "test_u = test_u.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(N*N, 2).to(device)\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        x[i*N+j, 0] = 1 * j / (N - 1)\n",
    "        x[i*N+j, 1] = 1 * i / (N - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_validation1 = np.zeros((9, N, N))\n",
    "U_validation1 = np.zeros((9, N, N))\n",
    "tau_validation1 = np.array([1/128, 1/128, 1/128, 2/128, 2/128, 2/128, 4/128, 4/128, 4/128])\n",
    "\n",
    "for k1 in range(3):\n",
    "    for k2 in range(3):\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                xx = 1 * j / (N - 1)\n",
    "                yy = 1 * i / (N - 1)\n",
    "                t = tau_validation1[k1 * 3 + k2]\n",
    "                \n",
    "                U_validation1[k1 * 3 + k2, i, j] = np.sin((k1 + 1) * np.pi * xx) * np.sin((k2 + 1) * np.pi * yy)\n",
    "                f_validation1[k1 * 3 + k2, i, j] = - np.pi ** 2 * ((k1 + 1) * (k1 + 1) + (k2 + 1) * (k2 + 1)) * np.sin((k1 + 1) * np.pi * xx) * np.sin((k2 + 1) * np.pi * yy)\n",
    "                f_validation1[k1 * 3 + k2, i, j] = U_validation1[k1 * 3 + k2, i, j] - t * f_validation1[k1 * 3 + k2, i, j]\n",
    "\n",
    "\n",
    "U_validation1 = torch.tensor(U_validation1, dtype=torch.float32).to(device)\n",
    "f_validation1 = torch.tensor(f_validation1, dtype=torch.float32).to(device)\n",
    "tau_validation1 = torch.tensor(tau_validation1, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose, delta, path):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.8f} --> {val_loss:.8f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100000] Training Loss: 0.00613568 Testing Loss: 0.00550093 Validation loss 1: 0.00938235\n",
      "Epoch [200/100000] Training Loss: 0.00133725 Testing Loss: 0.00086866 Validation loss 1: 0.00105881\n",
      "Epoch [300/100000] Training Loss: 0.00037727 Testing Loss: 0.00036333 Validation loss 1: 0.00049576\n",
      "Epoch [400/100000] Training Loss: 0.00047959 Testing Loss: 0.00045882 Validation loss 1: 0.00080957\n",
      "Epoch [500/100000] Training Loss: 0.00038791 Testing Loss: 0.00035516 Validation loss 1: 0.00085487\n",
      "Epoch [600/100000] Training Loss: 0.00027814 Testing Loss: 0.00024744 Validation loss 1: 0.00036400\n",
      "Epoch [700/100000] Training Loss: 0.00015388 Testing Loss: 0.00015651 Validation loss 1: 0.00018129\n",
      "Epoch [800/100000] Training Loss: 0.00051243 Testing Loss: 0.00045914 Validation loss 1: 0.00097406\n",
      "Epoch [900/100000] Training Loss: 0.00016185 Testing Loss: 0.00018385 Validation loss 1: 0.00027430\n",
      "Epoch [1000/100000] Training Loss: 0.00013164 Testing Loss: 0.00011745 Validation loss 1: 0.00012924\n",
      "Epoch [1100/100000] Training Loss: 0.00016284 Testing Loss: 0.00019962 Validation loss 1: 0.00048449\n",
      "Epoch [1200/100000] Training Loss: 0.00040486 Testing Loss: 0.00056005 Validation loss 1: 0.00131038\n",
      "Epoch [1300/100000] Training Loss: 0.00026100 Testing Loss: 0.00031617 Validation loss 1: 0.00062933\n",
      "Epoch [1400/100000] Training Loss: 0.00009247 Testing Loss: 0.00009190 Validation loss 1: 0.00008238\n",
      "Epoch [1500/100000] Training Loss: 0.00012137 Testing Loss: 0.00012154 Validation loss 1: 0.00014212\n",
      "Epoch [1600/100000] Training Loss: 0.00008015 Testing Loss: 0.00008227 Validation loss 1: 0.00007417\n",
      "Epoch [1700/100000] Training Loss: 0.00012910 Testing Loss: 0.00013683 Validation loss 1: 0.00027776\n",
      "Epoch [1800/100000] Training Loss: 0.00015606 Testing Loss: 0.00016787 Validation loss 1: 0.00032748\n",
      "Epoch [1900/100000] Training Loss: 0.00037489 Testing Loss: 0.00023570 Validation loss 1: 0.00052024\n",
      "Epoch [2000/100000] Training Loss: 0.00021523 Testing Loss: 0.00013322 Validation loss 1: 0.00021304\n",
      "Epoch [2100/100000] Training Loss: 0.00007607 Testing Loss: 0.00007184 Validation loss 1: 0.00005906\n",
      "Epoch [2200/100000] Training Loss: 0.00014225 Testing Loss: 0.00016522 Validation loss 1: 0.00030579\n",
      "Epoch [2300/100000] Training Loss: 0.00012624 Testing Loss: 0.00013350 Validation loss 1: 0.00023036\n",
      "Epoch [2400/100000] Training Loss: 0.00038002 Testing Loss: 0.00041856 Validation loss 1: 0.00092822\n",
      "Epoch [2500/100000] Training Loss: 0.00012401 Testing Loss: 0.00009525 Validation loss 1: 0.00014451\n",
      "Epoch [2600/100000] Training Loss: 0.00011415 Testing Loss: 0.00011700 Validation loss 1: 0.00025138\n",
      "Epoch [2700/100000] Training Loss: 0.00008214 Testing Loss: 0.00008987 Validation loss 1: 0.00012482\n",
      "Epoch [2800/100000] Training Loss: 0.00015831 Testing Loss: 0.00012038 Validation loss 1: 0.00019090\n",
      "Epoch [2900/100000] Training Loss: 0.00006699 Testing Loss: 0.00007312 Validation loss 1: 0.00009231\n",
      "Epoch [3000/100000] Training Loss: 0.00013144 Testing Loss: 0.00012681 Validation loss 1: 0.00025477\n",
      "Epoch [3100/100000] Training Loss: 0.00019846 Testing Loss: 0.00018722 Validation loss 1: 0.00028925\n",
      "Epoch [3200/100000] Training Loss: 0.00014225 Testing Loss: 0.00014102 Validation loss 1: 0.00029050\n",
      "Epoch [3300/100000] Training Loss: 0.00006516 Testing Loss: 0.00006992 Validation loss 1: 0.00007703\n",
      "Epoch [3400/100000] Training Loss: 0.00012880 Testing Loss: 0.00012506 Validation loss 1: 0.00018920\n",
      "Epoch [3500/100000] Training Loss: 0.00008707 Testing Loss: 0.00008431 Validation loss 1: 0.00011583\n",
      "Epoch [3600/100000] Training Loss: 0.00006292 Testing Loss: 0.00006523 Validation loss 1: 0.00007760\n",
      "Epoch [3700/100000] Training Loss: 0.00008225 Testing Loss: 0.00008503 Validation loss 1: 0.00014669\n",
      "Epoch [3800/100000] Training Loss: 0.00011358 Testing Loss: 0.00012978 Validation loss 1: 0.00024993\n",
      "Epoch [3900/100000] Training Loss: 0.00009110 Testing Loss: 0.00010029 Validation loss 1: 0.00016760\n",
      "Epoch [4000/100000] Training Loss: 0.00011794 Testing Loss: 0.00014543 Validation loss 1: 0.00032256\n",
      "Epoch [4100/100000] Training Loss: 0.00003975 Testing Loss: 0.00003985 Validation loss 1: 0.00001556\n",
      "Epoch [4200/100000] Training Loss: 0.00004904 Testing Loss: 0.00004841 Validation loss 1: 0.00003624\n",
      "Epoch [4300/100000] Training Loss: 0.00006075 Testing Loss: 0.00005503 Validation loss 1: 0.00005036\n",
      "Epoch [4400/100000] Training Loss: 0.00003973 Testing Loss: 0.00003998 Validation loss 1: 0.00001753\n",
      "Epoch [4500/100000] Training Loss: 0.00014319 Testing Loss: 0.00020721 Validation loss 1: 0.00039814\n",
      "Epoch [4600/100000] Training Loss: 0.00011980 Testing Loss: 0.00009351 Validation loss 1: 0.00022438\n",
      "Epoch [4700/100000] Training Loss: 0.00010064 Testing Loss: 0.00012277 Validation loss 1: 0.00022215\n",
      "Epoch [4800/100000] Training Loss: 0.00008171 Testing Loss: 0.00008946 Validation loss 1: 0.00013858\n",
      "Epoch [4900/100000] Training Loss: 0.00022795 Testing Loss: 0.00023350 Validation loss 1: 0.00051815\n",
      "Epoch [5000/100000] Training Loss: 0.00008807 Testing Loss: 0.00008637 Validation loss 1: 0.00015718\n",
      "Epoch [5100/100000] Training Loss: 0.00004344 Testing Loss: 0.00004249 Validation loss 1: 0.00002739\n",
      "Epoch [5200/100000] Training Loss: 0.00003840 Testing Loss: 0.00004104 Validation loss 1: 0.00002213\n",
      "Epoch [5300/100000] Training Loss: 0.00004935 Testing Loss: 0.00004196 Validation loss 1: 0.00003070\n",
      "Epoch [5400/100000] Training Loss: 0.00004673 Testing Loss: 0.00005273 Validation loss 1: 0.00003074\n",
      "Epoch [5500/100000] Training Loss: 0.00010171 Testing Loss: 0.00012501 Validation loss 1: 0.00025964\n",
      "Epoch [5600/100000] Training Loss: 0.00003816 Testing Loss: 0.00003879 Validation loss 1: 0.00002381\n",
      "Epoch [5700/100000] Training Loss: 0.00044444 Testing Loss: 0.00037489 Validation loss 1: 0.00096510\n",
      "Epoch [5800/100000] Training Loss: 0.00006533 Testing Loss: 0.00006541 Validation loss 1: 0.00009782\n",
      "Epoch [5900/100000] Training Loss: 0.00005968 Testing Loss: 0.00006204 Validation loss 1: 0.00007362\n",
      "Epoch [6000/100000] Training Loss: 0.00004620 Testing Loss: 0.00004887 Validation loss 1: 0.00005976\n",
      "Epoch [6100/100000] Training Loss: 0.00029880 Testing Loss: 0.00032115 Validation loss 1: 0.00059703\n",
      "Epoch [6200/100000] Training Loss: 0.00033606 Testing Loss: 0.00029457 Validation loss 1: 0.00069514\n",
      "Epoch [6300/100000] Training Loss: 0.00007921 Testing Loss: 0.00009442 Validation loss 1: 0.00014511\n",
      "Epoch [6400/100000] Training Loss: 0.00022690 Testing Loss: 0.00020523 Validation loss 1: 0.00047881\n",
      "Epoch [6500/100000] Training Loss: 0.00004534 Testing Loss: 0.00004309 Validation loss 1: 0.00003568\n",
      "Epoch [6600/100000] Training Loss: 0.00004004 Testing Loss: 0.00003635 Validation loss 1: 0.00002301\n",
      "Epoch [6700/100000] Training Loss: 0.00003242 Testing Loss: 0.00003453 Validation loss 1: 0.00002186\n",
      "Epoch [6800/100000] Training Loss: 0.00007098 Testing Loss: 0.00007981 Validation loss 1: 0.00020413\n",
      "Epoch [6900/100000] Training Loss: 0.00061625 Testing Loss: 0.00035094 Validation loss 1: 0.00055888\n",
      "Epoch [7000/100000] Training Loss: 0.00002989 Testing Loss: 0.00003080 Validation loss 1: 0.00001184\n",
      "Epoch [7100/100000] Training Loss: 0.00003161 Testing Loss: 0.00003395 Validation loss 1: 0.00002357\n",
      "Epoch [7200/100000] Training Loss: 0.00006717 Testing Loss: 0.00005959 Validation loss 1: 0.00008879\n",
      "Epoch [7300/100000] Training Loss: 0.00003105 Testing Loss: 0.00003264 Validation loss 1: 0.00001737\n",
      "Epoch [7400/100000] Training Loss: 0.00002885 Testing Loss: 0.00003001 Validation loss 1: 0.00001219\n",
      "Epoch [7500/100000] Training Loss: 0.00003074 Testing Loss: 0.00003221 Validation loss 1: 0.00001775\n",
      "Epoch [7600/100000] Training Loss: 0.00003147 Testing Loss: 0.00002986 Validation loss 1: 0.00001357\n",
      "Epoch [7700/100000] Training Loss: 0.00014685 Testing Loss: 0.00010489 Validation loss 1: 0.00020443\n",
      "Epoch [7800/100000] Training Loss: 0.00012535 Testing Loss: 0.00011990 Validation loss 1: 0.00026872\n",
      "Epoch [7900/100000] Training Loss: 0.00002683 Testing Loss: 0.00002783 Validation loss 1: 0.00000897\n",
      "Epoch [8000/100000] Training Loss: 0.00004375 Testing Loss: 0.00003857 Validation loss 1: 0.00004530\n",
      "Epoch [8100/100000] Training Loss: 0.00004649 Testing Loss: 0.00005250 Validation loss 1: 0.00008255\n",
      "Epoch [8200/100000] Training Loss: 0.00007904 Testing Loss: 0.00008454 Validation loss 1: 0.00014312\n",
      "Epoch [8300/100000] Training Loss: 0.00018471 Testing Loss: 0.00010516 Validation loss 1: 0.00022536\n",
      "Epoch [8400/100000] Training Loss: 0.00007396 Testing Loss: 0.00009884 Validation loss 1: 0.00017593\n",
      "Epoch [8500/100000] Training Loss: 0.00007880 Testing Loss: 0.00009000 Validation loss 1: 0.00018088\n",
      "Epoch [8600/100000] Training Loss: 0.00018158 Testing Loss: 0.00016782 Validation loss 1: 0.00052096\n",
      "Epoch [8700/100000] Training Loss: 0.00005976 Testing Loss: 0.00005910 Validation loss 1: 0.00005095\n",
      "Epoch [8800/100000] Training Loss: 0.00004788 Testing Loss: 0.00004794 Validation loss 1: 0.00003492\n",
      "Epoch [8900/100000] Training Loss: 0.00004263 Testing Loss: 0.00004302 Validation loss 1: 0.00002851\n",
      "Epoch [9000/100000] Training Loss: 0.00004047 Testing Loss: 0.00004109 Validation loss 1: 0.00002719\n",
      "Epoch [9100/100000] Training Loss: 0.00013731 Testing Loss: 0.00015898 Validation loss 1: 0.00034172\n",
      "Epoch [9200/100000] Training Loss: 0.00013801 Testing Loss: 0.00013411 Validation loss 1: 0.00028923\n",
      "Epoch [9300/100000] Training Loss: 0.00003542 Testing Loss: 0.00003569 Validation loss 1: 0.00001950\n",
      "Epoch [9400/100000] Training Loss: 0.00003516 Testing Loss: 0.00003653 Validation loss 1: 0.00002401\n",
      "Epoch [9500/100000] Training Loss: 0.00004079 Testing Loss: 0.00004568 Validation loss 1: 0.00004776\n",
      "Epoch [9600/100000] Training Loss: 0.00003243 Testing Loss: 0.00003301 Validation loss 1: 0.00001645\n",
      "Epoch [9700/100000] Training Loss: 0.00009655 Testing Loss: 0.00010812 Validation loss 1: 0.00022728\n",
      "Epoch [9800/100000] Training Loss: 0.00003563 Testing Loss: 0.00003835 Validation loss 1: 0.00002823\n",
      "Epoch [9900/100000] Training Loss: 0.00013673 Testing Loss: 0.00008294 Validation loss 1: 0.00010518\n",
      "Epoch [10000/100000] Training Loss: 0.00013147 Testing Loss: 0.00015602 Validation loss 1: 0.00036190\n",
      "Epoch [10100/100000] Training Loss: 0.00002945 Testing Loss: 0.00003004 Validation loss 1: 0.00001293\n",
      "Epoch [10200/100000] Training Loss: 0.00003532 Testing Loss: 0.00003492 Validation loss 1: 0.00002964\n",
      "Epoch [10300/100000] Training Loss: 0.00007383 Testing Loss: 0.00012788 Validation loss 1: 0.00031922\n",
      "Epoch [10400/100000] Training Loss: 0.00003367 Testing Loss: 0.00003134 Validation loss 1: 0.00002137\n",
      "Epoch [10500/100000] Training Loss: 0.00002880 Testing Loss: 0.00002973 Validation loss 1: 0.00001472\n",
      "Epoch [10600/100000] Training Loss: 0.00003244 Testing Loss: 0.00003510 Validation loss 1: 0.00002734\n",
      "Epoch [10700/100000] Training Loss: 0.00003320 Testing Loss: 0.00003527 Validation loss 1: 0.00003079\n",
      "Epoch [10800/100000] Training Loss: 0.00007099 Testing Loss: 0.00007633 Validation loss 1: 0.00013435\n",
      "Epoch [10900/100000] Training Loss: 0.00010927 Testing Loss: 0.00008613 Validation loss 1: 0.00016357\n",
      "Epoch [11000/100000] Training Loss: 0.00008701 Testing Loss: 0.00011466 Validation loss 1: 0.00022151\n",
      "Epoch [11100/100000] Training Loss: 0.00003931 Testing Loss: 0.00003683 Validation loss 1: 0.00003226\n",
      "Epoch [11200/100000] Training Loss: 0.00007272 Testing Loss: 0.00006933 Validation loss 1: 0.00012368\n",
      "Epoch [11300/100000] Training Loss: 0.00015099 Testing Loss: 0.00018438 Validation loss 1: 0.00038393\n",
      "Epoch [11400/100000] Training Loss: 0.00008039 Testing Loss: 0.00004929 Validation loss 1: 0.00008540\n",
      "Epoch [11500/100000] Training Loss: 0.00002537 Testing Loss: 0.00002672 Validation loss 1: 0.00001346\n",
      "Epoch [11600/100000] Training Loss: 0.00004751 Testing Loss: 0.00004713 Validation loss 1: 0.00006333\n",
      "Epoch [11700/100000] Training Loss: 0.00007771 Testing Loss: 0.00009688 Validation loss 1: 0.00021477\n",
      "Epoch [11800/100000] Training Loss: 0.00003380 Testing Loss: 0.00002885 Validation loss 1: 0.00001971\n",
      "Epoch [11900/100000] Training Loss: 0.00002518 Testing Loss: 0.00002650 Validation loss 1: 0.00001144\n",
      "Epoch [12000/100000] Training Loss: 0.00006822 Testing Loss: 0.00008790 Validation loss 1: 0.00015594\n",
      "Epoch [12100/100000] Training Loss: 0.00004145 Testing Loss: 0.00004590 Validation loss 1: 0.00006709\n",
      "Epoch [12200/100000] Training Loss: 0.00002459 Testing Loss: 0.00002581 Validation loss 1: 0.00001382\n",
      "Epoch [12300/100000] Training Loss: 0.00004414 Testing Loss: 0.00004706 Validation loss 1: 0.00006709\n",
      "Epoch [12400/100000] Training Loss: 0.00005187 Testing Loss: 0.00007011 Validation loss 1: 0.00014185\n",
      "Epoch [12500/100000] Training Loss: 0.00002795 Testing Loss: 0.00002992 Validation loss 1: 0.00002625\n",
      "Epoch [12600/100000] Training Loss: 0.00031846 Testing Loss: 0.00018163 Validation loss 1: 0.00034771\n",
      "Epoch [12700/100000] Training Loss: 0.00004935 Testing Loss: 0.00003748 Validation loss 1: 0.00004270\n",
      "Epoch [12800/100000] Training Loss: 0.00003396 Testing Loss: 0.00003974 Validation loss 1: 0.00005313\n",
      "Epoch [12900/100000] Training Loss: 0.00004341 Testing Loss: 0.00003521 Validation loss 1: 0.00003666\n",
      "Epoch [13000/100000] Training Loss: 0.00002447 Testing Loss: 0.00002521 Validation loss 1: 0.00001144\n",
      "Epoch [13100/100000] Training Loss: 0.00015262 Testing Loss: 0.00005969 Validation loss 1: 0.00014739\n",
      "Epoch [13200/100000] Training Loss: 0.00003263 Testing Loss: 0.00002956 Validation loss 1: 0.00003006\n",
      "Epoch [13300/100000] Training Loss: 0.00003541 Testing Loss: 0.00003317 Validation loss 1: 0.00003524\n",
      "Epoch [13400/100000] Training Loss: 0.00002870 Testing Loss: 0.00002942 Validation loss 1: 0.00001687\n",
      "Epoch [13500/100000] Training Loss: 0.00002399 Testing Loss: 0.00002585 Validation loss 1: 0.00001450\n",
      "Epoch [13600/100000] Training Loss: 0.00004848 Testing Loss: 0.00005211 Validation loss 1: 0.00008827\n",
      "Epoch [13700/100000] Training Loss: 0.00002531 Testing Loss: 0.00002763 Validation loss 1: 0.00001583\n",
      "Epoch [13800/100000] Training Loss: 0.00005208 Testing Loss: 0.00005705 Validation loss 1: 0.00010213\n",
      "Epoch [13900/100000] Training Loss: 0.00002019 Testing Loss: 0.00002128 Validation loss 1: 0.00000647\n",
      "Epoch [14000/100000] Training Loss: 0.00004495 Testing Loss: 0.00003941 Validation loss 1: 0.00006262\n",
      "Epoch [14100/100000] Training Loss: 0.00006369 Testing Loss: 0.00004543 Validation loss 1: 0.00009113\n",
      "Epoch [14200/100000] Training Loss: 0.00010522 Testing Loss: 0.00009412 Validation loss 1: 0.00020580\n",
      "Epoch [14300/100000] Training Loss: 0.00003821 Testing Loss: 0.00004121 Validation loss 1: 0.00006232\n",
      "Epoch [14400/100000] Training Loss: 0.00002094 Testing Loss: 0.00002178 Validation loss 1: 0.00000909\n",
      "Epoch [14500/100000] Training Loss: 0.00002052 Testing Loss: 0.00002206 Validation loss 1: 0.00000988\n",
      "Epoch [14600/100000] Training Loss: 0.00002428 Testing Loss: 0.00002505 Validation loss 1: 0.00001323\n",
      "Epoch [14700/100000] Training Loss: 0.00002008 Testing Loss: 0.00002101 Validation loss 1: 0.00000660\n",
      "Epoch [14800/100000] Training Loss: 0.00005174 Testing Loss: 0.00006051 Validation loss 1: 0.00010003\n",
      "Epoch [14900/100000] Training Loss: 0.00002452 Testing Loss: 0.00002602 Validation loss 1: 0.00002225\n",
      "Epoch [15000/100000] Training Loss: 0.00006099 Testing Loss: 0.00007490 Validation loss 1: 0.00014917\n",
      "Epoch [15100/100000] Training Loss: 0.00005099 Testing Loss: 0.00005811 Validation loss 1: 0.00008064\n",
      "Epoch [15200/100000] Training Loss: 0.00002005 Testing Loss: 0.00002174 Validation loss 1: 0.00001090\n",
      "Epoch [15300/100000] Training Loss: 0.00002190 Testing Loss: 0.00002349 Validation loss 1: 0.00001498\n",
      "Epoch [15400/100000] Training Loss: 0.00002026 Testing Loss: 0.00002132 Validation loss 1: 0.00001035\n",
      "Epoch [15500/100000] Training Loss: 0.00206729 Testing Loss: 0.00252584 Validation loss 1: 0.00463330\n",
      "Epoch [15600/100000] Training Loss: 0.00002149 Testing Loss: 0.00002236 Validation loss 1: 0.00001246\n",
      "Epoch [15700/100000] Training Loss: 0.00001949 Testing Loss: 0.00002051 Validation loss 1: 0.00000828\n",
      "Epoch [15800/100000] Training Loss: 0.00002239 Testing Loss: 0.00002032 Validation loss 1: 0.00000792\n",
      "Epoch [15900/100000] Training Loss: 0.00001875 Testing Loss: 0.00001984 Validation loss 1: 0.00000700\n",
      "Epoch [16000/100000] Training Loss: 0.00002813 Testing Loss: 0.00002938 Validation loss 1: 0.00003496\n",
      "Epoch [16100/100000] Training Loss: 0.00002131 Testing Loss: 0.00002235 Validation loss 1: 0.00001724\n",
      "Epoch [16200/100000] Training Loss: 0.00002049 Testing Loss: 0.00002137 Validation loss 1: 0.00001078\n",
      "Epoch [16300/100000] Training Loss: 0.00001877 Testing Loss: 0.00001967 Validation loss 1: 0.00000694\n",
      "Epoch [16400/100000] Training Loss: 0.00003434 Testing Loss: 0.00003379 Validation loss 1: 0.00004702\n",
      "Epoch [16500/100000] Training Loss: 0.00001783 Testing Loss: 0.00001888 Validation loss 1: 0.00000550\n",
      "Epoch [16600/100000] Training Loss: 0.00001978 Testing Loss: 0.00002116 Validation loss 1: 0.00001168\n",
      "Epoch [16700/100000] Training Loss: 0.00002012 Testing Loss: 0.00002099 Validation loss 1: 0.00001032\n",
      "Epoch [16800/100000] Training Loss: 0.00001792 Testing Loss: 0.00001899 Validation loss 1: 0.00000635\n",
      "Epoch [16900/100000] Training Loss: 0.00002050 Testing Loss: 0.00002171 Validation loss 1: 0.00001363\n",
      "Epoch [17000/100000] Training Loss: 0.00001841 Testing Loss: 0.00001999 Validation loss 1: 0.00000829\n",
      "Epoch [17100/100000] Training Loss: 0.00001729 Testing Loss: 0.00001834 Validation loss 1: 0.00000527\n",
      "Epoch [17200/100000] Training Loss: 0.00001725 Testing Loss: 0.00001828 Validation loss 1: 0.00000529\n",
      "Epoch [17300/100000] Training Loss: 0.00001734 Testing Loss: 0.00001835 Validation loss 1: 0.00000554\n",
      "Epoch [17400/100000] Training Loss: 0.00001918 Testing Loss: 0.00002094 Validation loss 1: 0.00001214\n",
      "Epoch [17500/100000] Training Loss: 0.00001848 Testing Loss: 0.00001928 Validation loss 1: 0.00000957\n",
      "Epoch [17600/100000] Training Loss: 0.00002241 Testing Loss: 0.00002083 Validation loss 1: 0.00001300\n",
      "Epoch [17700/100000] Training Loss: 0.00001692 Testing Loss: 0.00001801 Validation loss 1: 0.00000525\n",
      "Epoch [17800/100000] Training Loss: 0.00004247 Testing Loss: 0.00005778 Validation loss 1: 0.00009513\n",
      "Epoch [17900/100000] Training Loss: 0.00001667 Testing Loss: 0.00001773 Validation loss 1: 0.00000497\n",
      "Epoch [18000/100000] Training Loss: 0.00001683 Testing Loss: 0.00001791 Validation loss 1: 0.00000548\n",
      "Epoch [18100/100000] Training Loss: 0.00022208 Testing Loss: 0.00019953 Validation loss 1: 0.00039890\n",
      "Epoch [18200/100000] Training Loss: 0.00007253 Testing Loss: 0.00006376 Validation loss 1: 0.00014190\n",
      "Epoch [18300/100000] Training Loss: 0.00002855 Testing Loss: 0.00003005 Validation loss 1: 0.00002972\n",
      "Epoch [18400/100000] Training Loss: 0.00004842 Testing Loss: 0.00004460 Validation loss 1: 0.00007935\n",
      "Epoch [18500/100000] Training Loss: 0.00001649 Testing Loss: 0.00001754 Validation loss 1: 0.00000490\n",
      "Epoch [18600/100000] Training Loss: 0.00002541 Testing Loss: 0.00002489 Validation loss 1: 0.00002559\n",
      "Epoch [18700/100000] Training Loss: 0.00003222 Testing Loss: 0.00002180 Validation loss 1: 0.00001540\n",
      "Epoch [18800/100000] Training Loss: 0.00001768 Testing Loss: 0.00001858 Validation loss 1: 0.00000850\n",
      "Epoch [18900/100000] Training Loss: 0.00002061 Testing Loss: 0.00002288 Validation loss 1: 0.00002037\n",
      "Epoch [19000/100000] Training Loss: 0.00001591 Testing Loss: 0.00001701 Validation loss 1: 0.00000471\n",
      "Epoch [19100/100000] Training Loss: 0.00002098 Testing Loss: 0.00002056 Validation loss 1: 0.00001324\n",
      "Epoch [19200/100000] Training Loss: 0.00001574 Testing Loss: 0.00001679 Validation loss 1: 0.00000452\n",
      "Epoch [19300/100000] Training Loss: 0.00001894 Testing Loss: 0.00001749 Validation loss 1: 0.00000640\n",
      "Epoch [19400/100000] Training Loss: 0.00366324 Testing Loss: 0.00527490 Validation loss 1: 0.01457690\n",
      "Epoch [19500/100000] Training Loss: 0.00002954 Testing Loss: 0.00002993 Validation loss 1: 0.00006192\n",
      "Epoch [19600/100000] Training Loss: 0.00002264 Testing Loss: 0.00002341 Validation loss 1: 0.00004382\n",
      "Epoch [19700/100000] Training Loss: 0.00002041 Testing Loss: 0.00002129 Validation loss 1: 0.00003849\n",
      "Epoch [19800/100000] Training Loss: 0.00001923 Testing Loss: 0.00002016 Validation loss 1: 0.00003415\n",
      "Epoch [19900/100000] Training Loss: 0.00001833 Testing Loss: 0.00001928 Validation loss 1: 0.00002714\n",
      "Epoch [20000/100000] Training Loss: 0.00002957 Testing Loss: 0.00003008 Validation loss 1: 0.00005649\n",
      "Epoch [20100/100000] Training Loss: 0.00005279 Testing Loss: 0.00006482 Validation loss 1: 0.00011748\n",
      "Epoch [20200/100000] Training Loss: 0.00002881 Testing Loss: 0.00003233 Validation loss 1: 0.00003765\n",
      "Epoch [20300/100000] Training Loss: 0.00002116 Testing Loss: 0.00002762 Validation loss 1: 0.00005864\n",
      "Epoch [20400/100000] Training Loss: 0.00009479 Testing Loss: 0.00007578 Validation loss 1: 0.00020087\n",
      "Epoch [20500/100000] Training Loss: 0.00001731 Testing Loss: 0.00001824 Validation loss 1: 0.00001974\n",
      "Epoch [20600/100000] Training Loss: 0.00004602 Testing Loss: 0.00005091 Validation loss 1: 0.00008830\n",
      "Epoch [20700/100000] Training Loss: 0.00001792 Testing Loss: 0.00001886 Validation loss 1: 0.00001509\n",
      "Epoch [20800/100000] Training Loss: 0.00004111 Testing Loss: 0.00003477 Validation loss 1: 0.00008244\n",
      "Epoch [20900/100000] Training Loss: 0.00008163 Testing Loss: 0.00010179 Validation loss 1: 0.00023936\n",
      "Epoch [21000/100000] Training Loss: 0.00001797 Testing Loss: 0.00001911 Validation loss 1: 0.00001200\n",
      "Epoch [21100/100000] Training Loss: 0.00001650 Testing Loss: 0.00001747 Validation loss 1: 0.00002961\n",
      "Epoch [21200/100000] Training Loss: 0.00001794 Testing Loss: 0.00001791 Validation loss 1: 0.00002701\n",
      "Epoch [21300/100000] Training Loss: 0.00011264 Testing Loss: 0.00003706 Validation loss 1: 0.00011036\n",
      "Epoch [21400/100000] Training Loss: 0.00001601 Testing Loss: 0.00001701 Validation loss 1: 0.00002453\n",
      "Epoch [21500/100000] Training Loss: 0.00007720 Testing Loss: 0.00004846 Validation loss 1: 0.00006518\n",
      "Epoch [21600/100000] Training Loss: 0.00004230 Testing Loss: 0.00004090 Validation loss 1: 0.00006196\n",
      "Epoch [21700/100000] Training Loss: 0.00001574 Testing Loss: 0.00001674 Validation loss 1: 0.00002258\n",
      "Epoch [21800/100000] Training Loss: 0.00002164 Testing Loss: 0.00002187 Validation loss 1: 0.00003890\n",
      "Epoch [21900/100000] Training Loss: 0.00001769 Testing Loss: 0.00001734 Validation loss 1: 0.00001510\n",
      "Epoch [22000/100000] Training Loss: 0.00001565 Testing Loss: 0.00001671 Validation loss 1: 0.00001726\n",
      "Epoch [22100/100000] Training Loss: 0.00001551 Testing Loss: 0.00001649 Validation loss 1: 0.00001518\n",
      "Epoch [22200/100000] Training Loss: 0.00001538 Testing Loss: 0.00001637 Validation loss 1: 0.00001631\n",
      "Epoch [22300/100000] Training Loss: 0.00001517 Testing Loss: 0.00001617 Validation loss 1: 0.00001174\n",
      "Epoch [22400/100000] Training Loss: 0.00006310 Testing Loss: 0.00005480 Validation loss 1: 0.00006348\n",
      "Epoch [22500/100000] Training Loss: 0.00001534 Testing Loss: 0.00001632 Validation loss 1: 0.00000540\n",
      "Epoch [22600/100000] Training Loss: 0.00001471 Testing Loss: 0.00001572 Validation loss 1: 0.00000475\n",
      "Epoch [22700/100000] Training Loss: 0.00001454 Testing Loss: 0.00001557 Validation loss 1: 0.00000474\n",
      "Epoch [22800/100000] Training Loss: 0.00002846 Testing Loss: 0.00001660 Validation loss 1: 0.00000791\n",
      "Epoch [22900/100000] Training Loss: 0.00002560 Testing Loss: 0.00003163 Validation loss 1: 0.00004815\n",
      "Epoch [23000/100000] Training Loss: 0.00003464 Testing Loss: 0.00002408 Validation loss 1: 0.00002866\n",
      "Epoch [23100/100000] Training Loss: 0.00002980 Testing Loss: 0.00004093 Validation loss 1: 0.00005504\n",
      "Epoch [23200/100000] Training Loss: 0.00001408 Testing Loss: 0.00001509 Validation loss 1: 0.00000406\n",
      "Epoch [23300/100000] Training Loss: 0.00013688 Testing Loss: 0.00013713 Validation loss 1: 0.00032969\n",
      "Epoch [23400/100000] Training Loss: 0.00001402 Testing Loss: 0.00001508 Validation loss 1: 0.00000479\n",
      "Epoch [23500/100000] Training Loss: 0.00008496 Testing Loss: 0.00004920 Validation loss 1: 0.00008990\n",
      "Epoch [23600/100000] Training Loss: 0.00003429 Testing Loss: 0.00004059 Validation loss 1: 0.00007511\n",
      "Epoch [23700/100000] Training Loss: 0.00021579 Testing Loss: 0.00019071 Validation loss 1: 0.00061433\n",
      "Epoch [23800/100000] Training Loss: 0.00001383 Testing Loss: 0.00001481 Validation loss 1: 0.00000403\n",
      "Epoch [23900/100000] Training Loss: 0.00001548 Testing Loss: 0.00001680 Validation loss 1: 0.00001014\n",
      "Epoch [24000/100000] Training Loss: 0.00006418 Testing Loss: 0.00006498 Validation loss 1: 0.00015875\n",
      "Epoch [24100/100000] Training Loss: 0.00001482 Testing Loss: 0.00001604 Validation loss 1: 0.00000835\n",
      "Epoch [24200/100000] Training Loss: 0.00001700 Testing Loss: 0.00002362 Validation loss 1: 0.00003041\n",
      "Epoch [24300/100000] Training Loss: 0.00001644 Testing Loss: 0.00001738 Validation loss 1: 0.00001740\n",
      "Epoch [24400/100000] Training Loss: 0.00001707 Testing Loss: 0.00001964 Validation loss 1: 0.00001473\n",
      "Epoch [24500/100000] Training Loss: 0.00001367 Testing Loss: 0.00001463 Validation loss 1: 0.00000475\n",
      "Epoch [24600/100000] Training Loss: 0.00001702 Testing Loss: 0.00001906 Validation loss 1: 0.00001236\n",
      "Epoch [24700/100000] Training Loss: 0.00001368 Testing Loss: 0.00001523 Validation loss 1: 0.00000533\n",
      "Epoch [24800/100000] Training Loss: 0.00002045 Testing Loss: 0.00002069 Validation loss 1: 0.00002766\n",
      "Epoch [24900/100000] Training Loss: 0.00003166 Testing Loss: 0.00003162 Validation loss 1: 0.00004519\n",
      "Epoch [25000/100000] Training Loss: 0.00002205 Testing Loss: 0.00001715 Validation loss 1: 0.00001173\n",
      "Epoch [25100/100000] Training Loss: 0.00006053 Testing Loss: 0.00005999 Validation loss 1: 0.00015387\n",
      "Epoch [25200/100000] Training Loss: 0.00002018 Testing Loss: 0.00001886 Validation loss 1: 0.00001378\n",
      "Epoch [25300/100000] Training Loss: 0.00001333 Testing Loss: 0.00001441 Validation loss 1: 0.00000507\n",
      "Epoch [25400/100000] Training Loss: 0.00001619 Testing Loss: 0.00001876 Validation loss 1: 0.00001429\n",
      "Epoch [25500/100000] Training Loss: 0.00001303 Testing Loss: 0.00001399 Validation loss 1: 0.00000381\n",
      "Epoch [25600/100000] Training Loss: 0.00001370 Testing Loss: 0.00001458 Validation loss 1: 0.00000560\n",
      "Epoch [25700/100000] Training Loss: 0.00001458 Testing Loss: 0.00001558 Validation loss 1: 0.00000504\n",
      "Epoch [25800/100000] Training Loss: 0.00001744 Testing Loss: 0.00001989 Validation loss 1: 0.00002217\n",
      "Epoch [25900/100000] Training Loss: 0.00016608 Testing Loss: 0.00073384 Validation loss 1: 0.00374057\n",
      "Epoch [26000/100000] Training Loss: 0.00001362 Testing Loss: 0.00001458 Validation loss 1: 0.00000472\n",
      "Epoch [26100/100000] Training Loss: 0.00001300 Testing Loss: 0.00001398 Validation loss 1: 0.00000416\n",
      "Epoch [26200/100000] Training Loss: 0.00001278 Testing Loss: 0.00001376 Validation loss 1: 0.00000397\n",
      "Epoch [26300/100000] Training Loss: 0.00001276 Testing Loss: 0.00001377 Validation loss 1: 0.00000415\n",
      "Epoch [26400/100000] Training Loss: 0.00001323 Testing Loss: 0.00001435 Validation loss 1: 0.00000552\n",
      "Epoch [26500/100000] Training Loss: 0.00001262 Testing Loss: 0.00001361 Validation loss 1: 0.00000392\n",
      "Epoch [26600/100000] Training Loss: 0.00001703 Testing Loss: 0.00001821 Validation loss 1: 0.00001467\n",
      "Epoch [26700/100000] Training Loss: 0.00005246 Testing Loss: 0.00004927 Validation loss 1: 0.00009371\n",
      "Epoch [26800/100000] Training Loss: 0.00001267 Testing Loss: 0.00001429 Validation loss 1: 0.00000540\n",
      "Epoch [26900/100000] Training Loss: 0.00003023 Testing Loss: 0.00003177 Validation loss 1: 0.00004361\n",
      "Epoch [27000/100000] Training Loss: 0.00001361 Testing Loss: 0.00001463 Validation loss 1: 0.00000701\n",
      "Epoch [27100/100000] Training Loss: 0.00001295 Testing Loss: 0.00001419 Validation loss 1: 0.00000625\n",
      "Epoch [27200/100000] Training Loss: 0.00001859 Testing Loss: 0.00002145 Validation loss 1: 0.00002731\n",
      "Epoch [27300/100000] Training Loss: 0.00001232 Testing Loss: 0.00001330 Validation loss 1: 0.00000371\n",
      "Epoch [27400/100000] Training Loss: 0.00001244 Testing Loss: 0.00001342 Validation loss 1: 0.00000420\n",
      "Epoch [27500/100000] Training Loss: 0.00001236 Testing Loss: 0.00001337 Validation loss 1: 0.00000404\n",
      "Epoch [27600/100000] Training Loss: 0.00001742 Testing Loss: 0.00002027 Validation loss 1: 0.00002377\n",
      "Epoch [27700/100000] Training Loss: 0.00001216 Testing Loss: 0.00001313 Validation loss 1: 0.00000364\n",
      "Epoch [27800/100000] Training Loss: 0.00001509 Testing Loss: 0.00001560 Validation loss 1: 0.00000694\n",
      "Epoch [27900/100000] Training Loss: 0.00004062 Testing Loss: 0.00004689 Validation loss 1: 0.00006306\n",
      "Epoch [28000/100000] Training Loss: 0.00001199 Testing Loss: 0.00001298 Validation loss 1: 0.00000363\n",
      "Epoch [28100/100000] Training Loss: 0.00022622 Testing Loss: 0.00030849 Validation loss 1: 0.00064036\n",
      "Epoch [28200/100000] Training Loss: 0.00001195 Testing Loss: 0.00001292 Validation loss 1: 0.00000347\n",
      "Epoch [28300/100000] Training Loss: 0.00001459 Testing Loss: 0.00001636 Validation loss 1: 0.00001220\n",
      "Epoch [28400/100000] Training Loss: 0.00006901 Testing Loss: 0.00003836 Validation loss 1: 0.00006396\n",
      "Epoch [28500/100000] Training Loss: 0.00001181 Testing Loss: 0.00001278 Validation loss 1: 0.00000342\n",
      "Epoch [28600/100000] Training Loss: 0.00001224 Testing Loss: 0.00001314 Validation loss 1: 0.00000398\n",
      "Epoch [28700/100000] Training Loss: 0.00002083 Testing Loss: 0.00002455 Validation loss 1: 0.00002993\n",
      "Epoch [28800/100000] Training Loss: 0.00001909 Testing Loss: 0.00001908 Validation loss 1: 0.00002060\n",
      "Epoch [28900/100000] Training Loss: 0.00002328 Testing Loss: 0.00002017 Validation loss 1: 0.00002170\n",
      "Epoch [29000/100000] Training Loss: 0.00001420 Testing Loss: 0.00001498 Validation loss 1: 0.00000735\n",
      "Epoch [29100/100000] Training Loss: 0.00001262 Testing Loss: 0.00001390 Validation loss 1: 0.00000679\n",
      "Epoch [29200/100000] Training Loss: 0.00003538 Testing Loss: 0.00001903 Validation loss 1: 0.00001633\n",
      "Epoch [29300/100000] Training Loss: 0.00001212 Testing Loss: 0.00001308 Validation loss 1: 0.00000434\n",
      "Epoch [29400/100000] Training Loss: 0.00001291 Testing Loss: 0.00001528 Validation loss 1: 0.00001074\n",
      "Epoch [29500/100000] Training Loss: 0.00002593 Testing Loss: 0.00002481 Validation loss 1: 0.00003836\n",
      "Epoch [29600/100000] Training Loss: 0.00002131 Testing Loss: 0.00001923 Validation loss 1: 0.00002466\n",
      "Epoch [29700/100000] Training Loss: 0.00001636 Testing Loss: 0.00001672 Validation loss 1: 0.00002029\n",
      "Epoch [29800/100000] Training Loss: 0.00001166 Testing Loss: 0.00001258 Validation loss 1: 0.00000344\n",
      "Epoch [29900/100000] Training Loss: 0.00001185 Testing Loss: 0.00001271 Validation loss 1: 0.00000362\n",
      "Epoch [30000/100000] Training Loss: 0.00001163 Testing Loss: 0.00001266 Validation loss 1: 0.00000383\n",
      "Epoch [30100/100000] Training Loss: 0.00001160 Testing Loss: 0.00001250 Validation loss 1: 0.00000352\n",
      "Epoch [30200/100000] Training Loss: 0.00001158 Testing Loss: 0.00001252 Validation loss 1: 0.00000343\n",
      "Epoch [30300/100000] Training Loss: 0.00001180 Testing Loss: 0.00001278 Validation loss 1: 0.00000373\n",
      "Epoch [30400/100000] Training Loss: 0.00001200 Testing Loss: 0.00001283 Validation loss 1: 0.00000450\n",
      "Epoch [30500/100000] Training Loss: 0.00001923 Testing Loss: 0.00002175 Validation loss 1: 0.00002567\n",
      "Epoch [30600/100000] Training Loss: 0.00001861 Testing Loss: 0.00001768 Validation loss 1: 0.00001486\n",
      "Epoch [30700/100000] Training Loss: 0.00007599 Testing Loss: 0.00008940 Validation loss 1: 0.00019483\n",
      "Epoch [30800/100000] Training Loss: 0.00001126 Testing Loss: 0.00001220 Validation loss 1: 0.00000319\n",
      "Epoch [30900/100000] Training Loss: 0.00001478 Testing Loss: 0.00001886 Validation loss 1: 0.00002063\n",
      "Epoch [31000/100000] Training Loss: 0.00001142 Testing Loss: 0.00001299 Validation loss 1: 0.00000528\n",
      "Epoch [31100/100000] Training Loss: 0.00001181 Testing Loss: 0.00001255 Validation loss 1: 0.00000467\n",
      "Epoch [31200/100000] Training Loss: 0.00003813 Testing Loss: 0.00002377 Validation loss 1: 0.00003075\n",
      "Epoch [31300/100000] Training Loss: 0.00017109 Testing Loss: 0.00018577 Validation loss 1: 0.00037742\n",
      "Epoch [31400/100000] Training Loss: 0.00001224 Testing Loss: 0.00001313 Validation loss 1: 0.00000472\n",
      "Epoch [31500/100000] Training Loss: 0.00001152 Testing Loss: 0.00001245 Validation loss 1: 0.00000376\n",
      "Epoch [31600/100000] Training Loss: 0.00001127 Testing Loss: 0.00001220 Validation loss 1: 0.00000349\n",
      "Epoch [31700/100000] Training Loss: 0.00001111 Testing Loss: 0.00001204 Validation loss 1: 0.00000334\n",
      "Epoch [31800/100000] Training Loss: 0.00001114 Testing Loss: 0.00001210 Validation loss 1: 0.00000357\n",
      "Epoch [31900/100000] Training Loss: 0.00001100 Testing Loss: 0.00001193 Validation loss 1: 0.00000322\n",
      "Epoch [32000/100000] Training Loss: 0.00001105 Testing Loss: 0.00001201 Validation loss 1: 0.00000333\n",
      "Epoch [32100/100000] Training Loss: 0.00001497 Testing Loss: 0.00001798 Validation loss 1: 0.00002012\n",
      "Epoch [32200/100000] Training Loss: 0.00001134 Testing Loss: 0.00001244 Validation loss 1: 0.00000493\n",
      "Epoch [32300/100000] Training Loss: 0.00001095 Testing Loss: 0.00001186 Validation loss 1: 0.00000327\n",
      "Epoch [32400/100000] Training Loss: 0.00001103 Testing Loss: 0.00001209 Validation loss 1: 0.00000406\n",
      "Epoch [32500/100000] Training Loss: 0.00001470 Testing Loss: 0.00001454 Validation loss 1: 0.00001061\n",
      "Epoch [32600/100000] Training Loss: 0.00001307 Testing Loss: 0.00001579 Validation loss 1: 0.00001438\n",
      "Epoch [32700/100000] Training Loss: 0.00002403 Testing Loss: 0.00002574 Validation loss 1: 0.00004645\n",
      "Epoch [32800/100000] Training Loss: 0.00001491 Testing Loss: 0.00001444 Validation loss 1: 0.00001087\n",
      "Epoch [32900/100000] Training Loss: 0.00001081 Testing Loss: 0.00001178 Validation loss 1: 0.00000342\n",
      "Epoch [33000/100000] Training Loss: 0.00001082 Testing Loss: 0.00001169 Validation loss 1: 0.00000317\n",
      "Epoch [33100/100000] Training Loss: 0.00002164 Testing Loss: 0.00002460 Validation loss 1: 0.00003733\n",
      "Epoch [33200/100000] Training Loss: 0.00001110 Testing Loss: 0.00001212 Validation loss 1: 0.00000426\n",
      "Epoch [33300/100000] Training Loss: 0.00007235 Testing Loss: 0.00008349 Validation loss 1: 0.00020191\n",
      "Epoch [33400/100000] Training Loss: 0.00001428 Testing Loss: 0.00001455 Validation loss 1: 0.00001016\n",
      "Epoch [33500/100000] Training Loss: 0.00001325 Testing Loss: 0.00001216 Validation loss 1: 0.00000394\n",
      "Epoch [33600/100000] Training Loss: 0.00008318 Testing Loss: 0.00007947 Validation loss 1: 0.00016928\n",
      "Epoch [33700/100000] Training Loss: 0.00006003 Testing Loss: 0.00005478 Validation loss 1: 0.00018345\n",
      "Epoch [33800/100000] Training Loss: 0.00001324 Testing Loss: 0.00001407 Validation loss 1: 0.00001019\n",
      "Epoch [33900/100000] Training Loss: 0.00010990 Testing Loss: 0.00062763 Validation loss 1: 0.00205666\n",
      "Epoch [34000/100000] Training Loss: 0.00001156 Testing Loss: 0.00001243 Validation loss 1: 0.00000442\n",
      "Epoch [34100/100000] Training Loss: 0.00001077 Testing Loss: 0.00001168 Validation loss 1: 0.00000350\n",
      "Epoch [34200/100000] Training Loss: 0.00001053 Testing Loss: 0.00001145 Validation loss 1: 0.00000327\n",
      "Epoch [34300/100000] Training Loss: 0.00001687 Testing Loss: 0.00001401 Validation loss 1: 0.00001016\n",
      "Epoch [34400/100000] Training Loss: 0.00001133 Testing Loss: 0.00001698 Validation loss 1: 0.00001749\n",
      "Epoch [34500/100000] Training Loss: 0.00001049 Testing Loss: 0.00001151 Validation loss 1: 0.00000375\n",
      "Epoch [34600/100000] Training Loss: 0.00004629 Testing Loss: 0.00004853 Validation loss 1: 0.00016592\n",
      "Epoch [34700/100000] Training Loss: 0.00001231 Testing Loss: 0.00001179 Validation loss 1: 0.00000439\n",
      "Epoch [34800/100000] Training Loss: 0.00001440 Testing Loss: 0.00001352 Validation loss 1: 0.00000854\n",
      "Epoch [34900/100000] Training Loss: 0.00001066 Testing Loss: 0.00001151 Validation loss 1: 0.00000402\n",
      "Epoch [35000/100000] Training Loss: 0.00001052 Testing Loss: 0.00001141 Validation loss 1: 0.00000353\n",
      "Epoch [35100/100000] Training Loss: 0.00001859 Testing Loss: 0.00002281 Validation loss 1: 0.00003476\n",
      "Epoch [35200/100000] Training Loss: 0.00001926 Testing Loss: 0.00001996 Validation loss 1: 0.00002572\n",
      "Epoch [35300/100000] Training Loss: 0.00001477 Testing Loss: 0.00001333 Validation loss 1: 0.00000892\n",
      "Epoch [35400/100000] Training Loss: 0.00003233 Testing Loss: 0.00002874 Validation loss 1: 0.00004023\n",
      "Epoch [35500/100000] Training Loss: 0.00001033 Testing Loss: 0.00001127 Validation loss 1: 0.00000387\n",
      "Epoch [35600/100000] Training Loss: 0.00001023 Testing Loss: 0.00001124 Validation loss 1: 0.00000311\n",
      "Epoch [35700/100000] Training Loss: 0.00001046 Testing Loss: 0.00001150 Validation loss 1: 0.00000457\n",
      "Epoch [35800/100000] Training Loss: 0.00001266 Testing Loss: 0.00001490 Validation loss 1: 0.00001450\n",
      "Epoch [35900/100000] Training Loss: 0.00001025 Testing Loss: 0.00001136 Validation loss 1: 0.00000408\n",
      "Epoch [36000/100000] Training Loss: 0.00001002 Testing Loss: 0.00001093 Validation loss 1: 0.00000292\n",
      "Epoch [36100/100000] Training Loss: 0.00001233 Testing Loss: 0.00001292 Validation loss 1: 0.00000829\n",
      "Epoch [36200/100000] Training Loss: 0.00008516 Testing Loss: 0.00010156 Validation loss 1: 0.00021451\n",
      "Epoch [36300/100000] Training Loss: 0.00005080 Testing Loss: 0.00004286 Validation loss 1: 0.00008868\n",
      "Epoch [36400/100000] Training Loss: 0.00002281 Testing Loss: 0.00003341 Validation loss 1: 0.00006053\n",
      "Epoch [36500/100000] Training Loss: 0.00002626 Testing Loss: 0.00002166 Validation loss 1: 0.00003255\n",
      "Epoch [36600/100000] Training Loss: 0.00001087 Testing Loss: 0.00001193 Validation loss 1: 0.00000583\n",
      "Epoch [36700/100000] Training Loss: 0.00001122 Testing Loss: 0.00001228 Validation loss 1: 0.00000594\n",
      "Epoch [36800/100000] Training Loss: 0.00002939 Testing Loss: 0.00002833 Validation loss 1: 0.00003521\n",
      "Epoch [36900/100000] Training Loss: 0.00001692 Testing Loss: 0.00001751 Validation loss 1: 0.00001274\n",
      "Epoch [37000/100000] Training Loss: 0.00001428 Testing Loss: 0.00001500 Validation loss 1: 0.00000916\n",
      "Epoch [37100/100000] Training Loss: 0.00001308 Testing Loss: 0.00001385 Validation loss 1: 0.00000735\n",
      "Epoch [37200/100000] Training Loss: 0.00001237 Testing Loss: 0.00001316 Validation loss 1: 0.00000631\n",
      "Epoch [37300/100000] Training Loss: 0.00001254 Testing Loss: 0.00001273 Validation loss 1: 0.00000574\n",
      "Epoch [37400/100000] Training Loss: 0.00002018 Testing Loss: 0.00002513 Validation loss 1: 0.00003998\n",
      "Epoch [37500/100000] Training Loss: 0.00001259 Testing Loss: 0.00001217 Validation loss 1: 0.00000508\n",
      "Epoch [37600/100000] Training Loss: 0.00001102 Testing Loss: 0.00001186 Validation loss 1: 0.00000454\n",
      "Epoch [37700/100000] Training Loss: 0.00011216 Testing Loss: 0.00009207 Validation loss 1: 0.00021915\n",
      "Epoch [37800/100000] Training Loss: 0.00001128 Testing Loss: 0.00001169 Validation loss 1: 0.00000443\n",
      "Epoch [37900/100000] Training Loss: 0.00001899 Testing Loss: 0.00002437 Validation loss 1: 0.00003727\n",
      "Epoch [38000/100000] Training Loss: 0.00001467 Testing Loss: 0.00001440 Validation loss 1: 0.00001174\n",
      "Epoch [38100/100000] Training Loss: 0.00002622 Testing Loss: 0.00003273 Validation loss 1: 0.00006263\n",
      "Epoch [38200/100000] Training Loss: 0.00003331 Testing Loss: 0.00002584 Validation loss 1: 0.00004308\n",
      "Epoch [38300/100000] Training Loss: 0.00001577 Testing Loss: 0.00001507 Validation loss 1: 0.00001354\n",
      "Epoch [38400/100000] Training Loss: 0.00001621 Testing Loss: 0.00001908 Validation loss 1: 0.00002530\n",
      "Epoch [38500/100000] Training Loss: 0.00001454 Testing Loss: 0.00001184 Validation loss 1: 0.00000517\n",
      "Epoch [38600/100000] Training Loss: 0.00001051 Testing Loss: 0.00001141 Validation loss 1: 0.00000496\n",
      "Epoch [38700/100000] Training Loss: 0.00002878 Testing Loss: 0.00003420 Validation loss 1: 0.00006696\n",
      "Epoch [38800/100000] Training Loss: 0.00001377 Testing Loss: 0.00001336 Validation loss 1: 0.00001015\n",
      "Epoch [38900/100000] Training Loss: 0.00001207 Testing Loss: 0.00001276 Validation loss 1: 0.00000987\n",
      "Epoch [39000/100000] Training Loss: 0.00001063 Testing Loss: 0.00001160 Validation loss 1: 0.00000535\n",
      "Epoch [39100/100000] Training Loss: 0.00001644 Testing Loss: 0.00002103 Validation loss 1: 0.00003807\n",
      "Epoch [39200/100000] Training Loss: 0.00001175 Testing Loss: 0.00001352 Validation loss 1: 0.00001085\n",
      "Epoch [39300/100000] Training Loss: 0.00004291 Testing Loss: 0.00003575 Validation loss 1: 0.00007489\n",
      "Epoch [39400/100000] Training Loss: 0.00001396 Testing Loss: 0.00001216 Validation loss 1: 0.00000838\n",
      "Epoch [39500/100000] Training Loss: 0.00001078 Testing Loss: 0.00001191 Validation loss 1: 0.00000484\n",
      "Epoch [39600/100000] Training Loss: 0.00001145 Testing Loss: 0.00001304 Validation loss 1: 0.00001242\n",
      "Epoch [39700/100000] Training Loss: 0.00000970 Testing Loss: 0.00001055 Validation loss 1: 0.00000299\n",
      "Epoch [39800/100000] Training Loss: 0.00000997 Testing Loss: 0.00001101 Validation loss 1: 0.00000357\n",
      "Epoch [39900/100000] Training Loss: 0.00001415 Testing Loss: 0.00001580 Validation loss 1: 0.00002011\n",
      "Epoch [40000/100000] Training Loss: 0.00001043 Testing Loss: 0.00001078 Validation loss 1: 0.00000394\n",
      "Epoch [40100/100000] Training Loss: 0.00001107 Testing Loss: 0.00001248 Validation loss 1: 0.00001147\n",
      "Epoch [40200/100000] Training Loss: 0.00001722 Testing Loss: 0.00001325 Validation loss 1: 0.00001636\n",
      "Epoch [40300/100000] Training Loss: 0.00001000 Testing Loss: 0.00001095 Validation loss 1: 0.00000372\n",
      "Epoch [40400/100000] Training Loss: 0.00001028 Testing Loss: 0.00001059 Validation loss 1: 0.00000341\n",
      "Epoch [40500/100000] Training Loss: 0.00001001 Testing Loss: 0.00001125 Validation loss 1: 0.00000473\n",
      "Epoch [40600/100000] Training Loss: 0.00000994 Testing Loss: 0.00001084 Validation loss 1: 0.00000391\n",
      "Epoch [40700/100000] Training Loss: 0.00000973 Testing Loss: 0.00001055 Validation loss 1: 0.00000352\n",
      "Epoch [40800/100000] Training Loss: 0.00001291 Testing Loss: 0.00001453 Validation loss 1: 0.00001691\n",
      "Epoch [40900/100000] Training Loss: 0.00001076 Testing Loss: 0.00001183 Validation loss 1: 0.00000709\n",
      "Epoch [41000/100000] Training Loss: 0.00000980 Testing Loss: 0.00001077 Validation loss 1: 0.00000461\n",
      "Epoch [41100/100000] Training Loss: 0.00001516 Testing Loss: 0.00001272 Validation loss 1: 0.00000963\n",
      "Epoch [41200/100000] Training Loss: 0.00006006 Testing Loss: 0.00005773 Validation loss 1: 0.00016030\n",
      "Epoch [41300/100000] Training Loss: 0.00001137 Testing Loss: 0.00001217 Validation loss 1: 0.00000833\n",
      "Epoch [41400/100000] Training Loss: 0.00003444 Testing Loss: 0.00003345 Validation loss 1: 0.00006530\n",
      "Epoch [41500/100000] Training Loss: 0.00001224 Testing Loss: 0.00002135 Validation loss 1: 0.00003228\n",
      "Epoch [41600/100000] Training Loss: 0.00001145 Testing Loss: 0.00001184 Validation loss 1: 0.00000765\n",
      "Epoch [41700/100000] Training Loss: 0.00002045 Testing Loss: 0.00002325 Validation loss 1: 0.00004748\n",
      "Epoch [41800/100000] Training Loss: 0.00001836 Testing Loss: 0.00002235 Validation loss 1: 0.00003161\n",
      "Epoch [41900/100000] Training Loss: 0.00001307 Testing Loss: 0.00002418 Validation loss 1: 0.00004531\n",
      "Epoch [42000/100000] Training Loss: 0.00001109 Testing Loss: 0.00001149 Validation loss 1: 0.00000821\n",
      "Epoch [42100/100000] Training Loss: 0.00001053 Testing Loss: 0.00001127 Validation loss 1: 0.00000497\n",
      "Epoch [42200/100000] Training Loss: 0.00003332 Testing Loss: 0.00004139 Validation loss 1: 0.00009327\n",
      "Epoch [42300/100000] Training Loss: 0.00003817 Testing Loss: 0.00003799 Validation loss 1: 0.00009562\n",
      "Epoch [42400/100000] Training Loss: 0.00000943 Testing Loss: 0.00001032 Validation loss 1: 0.00000377\n",
      "Epoch [42500/100000] Training Loss: 0.00026520 Testing Loss: 0.00021713 Validation loss 1: 0.00063055\n",
      "Epoch [42600/100000] Training Loss: 0.00001049 Testing Loss: 0.00001123 Validation loss 1: 0.00000454\n",
      "Epoch [42700/100000] Training Loss: 0.00000951 Testing Loss: 0.00001034 Validation loss 1: 0.00000321\n",
      "Epoch [42800/100000] Training Loss: 0.00000928 Testing Loss: 0.00001011 Validation loss 1: 0.00000295\n",
      "Epoch [42900/100000] Training Loss: 0.00000988 Testing Loss: 0.00001069 Validation loss 1: 0.00000496\n",
      "Epoch [43000/100000] Training Loss: 0.00002354 Testing Loss: 0.00002248 Validation loss 1: 0.00003668\n",
      "Epoch [43100/100000] Training Loss: 0.00001660 Testing Loss: 0.00001233 Validation loss 1: 0.00001001\n",
      "Epoch [43200/100000] Training Loss: 0.00001603 Testing Loss: 0.00001728 Validation loss 1: 0.00003165\n",
      "Epoch [43300/100000] Training Loss: 0.00001046 Testing Loss: 0.00001110 Validation loss 1: 0.00000695\n",
      "Epoch [43400/100000] Training Loss: 0.00008879 Testing Loss: 0.00007693 Validation loss 1: 0.00019211\n",
      "Epoch [43500/100000] Training Loss: 0.00001263 Testing Loss: 0.00001348 Validation loss 1: 0.00001278\n",
      "Epoch [43600/100000] Training Loss: 0.00002430 Testing Loss: 0.00003149 Validation loss 1: 0.00005665\n",
      "Epoch [43700/100000] Training Loss: 0.00004179 Testing Loss: 0.00003876 Validation loss 1: 0.00008378\n",
      "Epoch [43800/100000] Training Loss: 0.00001063 Testing Loss: 0.00001119 Validation loss 1: 0.00000697\n",
      "Epoch [43900/100000] Training Loss: 0.00000904 Testing Loss: 0.00000986 Validation loss 1: 0.00000296\n",
      "Epoch [44000/100000] Training Loss: 0.00000936 Testing Loss: 0.00000997 Validation loss 1: 0.00000317\n",
      "Epoch [44100/100000] Training Loss: 0.00000927 Testing Loss: 0.00001004 Validation loss 1: 0.00000355\n",
      "Epoch [44200/100000] Training Loss: 0.00001063 Testing Loss: 0.00001208 Validation loss 1: 0.00000471\n",
      "Epoch [44300/100000] Training Loss: 0.00000944 Testing Loss: 0.00001058 Validation loss 1: 0.00000519\n",
      "Epoch [44400/100000] Training Loss: 0.00000983 Testing Loss: 0.00001054 Validation loss 1: 0.00000378\n",
      "Epoch [44500/100000] Training Loss: 0.00001945 Testing Loss: 0.00002483 Validation loss 1: 0.00003469\n",
      "Epoch [44600/100000] Training Loss: 0.00002103 Testing Loss: 0.00002068 Validation loss 1: 0.00003233\n",
      "Epoch [44700/100000] Training Loss: 0.00003390 Testing Loss: 0.00003785 Validation loss 1: 0.00008501\n",
      "Epoch [44800/100000] Training Loss: 0.00001019 Testing Loss: 0.00001093 Validation loss 1: 0.00000660\n",
      "Epoch [44900/100000] Training Loss: 0.00000889 Testing Loss: 0.00000981 Validation loss 1: 0.00000359\n",
      "Epoch [45000/100000] Training Loss: 0.00005034 Testing Loss: 0.00003741 Validation loss 1: 0.00006816\n",
      "Epoch [45100/100000] Training Loss: 0.00000992 Testing Loss: 0.00001022 Validation loss 1: 0.00000368\n",
      "Epoch [45200/100000] Training Loss: 0.00001161 Testing Loss: 0.00001282 Validation loss 1: 0.00001618\n",
      "Epoch [45300/100000] Training Loss: 0.00000855 Testing Loss: 0.00000940 Validation loss 1: 0.00000255\n",
      "Epoch [45400/100000] Training Loss: 0.00000890 Testing Loss: 0.00000985 Validation loss 1: 0.00000323\n",
      "Epoch [45500/100000] Training Loss: 0.00000885 Testing Loss: 0.00000970 Validation loss 1: 0.00000291\n",
      "Epoch [45600/100000] Training Loss: 0.00000857 Testing Loss: 0.00000940 Validation loss 1: 0.00000254\n",
      "Epoch [45700/100000] Training Loss: 0.00000857 Testing Loss: 0.00000941 Validation loss 1: 0.00000259\n",
      "Epoch [45800/100000] Training Loss: 0.00005744 Testing Loss: 0.00005351 Validation loss 1: 0.00011868\n",
      "Epoch [45900/100000] Training Loss: 0.00000976 Testing Loss: 0.00001079 Validation loss 1: 0.00000559\n",
      "Epoch [46000/100000] Training Loss: 0.00004558 Testing Loss: 0.00005471 Validation loss 1: 0.00012598\n",
      "Epoch [46100/100000] Training Loss: 0.00001022 Testing Loss: 0.00001070 Validation loss 1: 0.00000616\n",
      "Epoch [46200/100000] Training Loss: 0.00000860 Testing Loss: 0.00000942 Validation loss 1: 0.00000287\n",
      "Epoch [46300/100000] Training Loss: 0.00004183 Testing Loss: 0.00005530 Validation loss 1: 0.00009380\n",
      "Epoch [46400/100000] Training Loss: 0.00001060 Testing Loss: 0.00001159 Validation loss 1: 0.00000999\n",
      "Epoch [46500/100000] Training Loss: 0.00001392 Testing Loss: 0.00001526 Validation loss 1: 0.00001919\n",
      "Epoch [46600/100000] Training Loss: 0.00000848 Testing Loss: 0.00000932 Validation loss 1: 0.00000288\n",
      "Epoch [46700/100000] Training Loss: 0.00006156 Testing Loss: 0.00005981 Validation loss 1: 0.00013691\n",
      "Epoch [46800/100000] Training Loss: 0.00000920 Testing Loss: 0.00000993 Validation loss 1: 0.00000397\n",
      "Epoch [46900/100000] Training Loss: 0.00000850 Testing Loss: 0.00000933 Validation loss 1: 0.00000273\n",
      "Epoch [47000/100000] Training Loss: 0.00000834 Testing Loss: 0.00000917 Validation loss 1: 0.00000257\n",
      "Epoch [47100/100000] Training Loss: 0.00000869 Testing Loss: 0.00000947 Validation loss 1: 0.00000368\n",
      "Epoch [47200/100000] Training Loss: 0.00002058 Testing Loss: 0.00002442 Validation loss 1: 0.00006035\n",
      "Epoch [47300/100000] Training Loss: 0.00002370 Testing Loss: 0.00001063 Validation loss 1: 0.00000667\n",
      "Epoch [47400/100000] Training Loss: 0.00008984 Testing Loss: 0.00008449 Validation loss 1: 0.00024659\n",
      "Epoch [47500/100000] Training Loss: 0.00003494 Testing Loss: 0.00002204 Validation loss 1: 0.00003619\n",
      "Epoch [47600/100000] Training Loss: 0.00001515 Testing Loss: 0.00001924 Validation loss 1: 0.00003615\n",
      "Epoch [47700/100000] Training Loss: 0.00000831 Testing Loss: 0.00000936 Validation loss 1: 0.00000312\n",
      "Epoch [47800/100000] Training Loss: 0.00000829 Testing Loss: 0.00000910 Validation loss 1: 0.00000286\n",
      "Epoch [47900/100000] Training Loss: 0.00000819 Testing Loss: 0.00000901 Validation loss 1: 0.00000253\n",
      "Epoch [48000/100000] Training Loss: 0.00001951 Testing Loss: 0.00002306 Validation loss 1: 0.00002826\n",
      "Epoch [48100/100000] Training Loss: 0.00000973 Testing Loss: 0.00001057 Validation loss 1: 0.00000398\n",
      "Epoch [48200/100000] Training Loss: 0.00000973 Testing Loss: 0.00001064 Validation loss 1: 0.00000593\n",
      "Epoch [48300/100000] Training Loss: 0.00000813 Testing Loss: 0.00000899 Validation loss 1: 0.00000277\n",
      "Epoch [48400/100000] Training Loss: 0.00000832 Testing Loss: 0.00000926 Validation loss 1: 0.00000346\n",
      "Epoch [48500/100000] Training Loss: 0.00003257 Testing Loss: 0.00004786 Validation loss 1: 0.00010062\n",
      "Epoch [48600/100000] Training Loss: 0.00000916 Testing Loss: 0.00001700 Validation loss 1: 0.00002033\n",
      "Epoch [48700/100000] Training Loss: 0.00000804 Testing Loss: 0.00000887 Validation loss 1: 0.00000235\n",
      "Epoch [48800/100000] Training Loss: 0.00000829 Testing Loss: 0.00000921 Validation loss 1: 0.00000362\n",
      "Epoch [48900/100000] Training Loss: 0.00000951 Testing Loss: 0.00001038 Validation loss 1: 0.00000682\n",
      "Epoch [49000/100000] Training Loss: 0.00001700 Testing Loss: 0.00002062 Validation loss 1: 0.00003711\n",
      "Epoch [49100/100000] Training Loss: 0.00000881 Testing Loss: 0.00000986 Validation loss 1: 0.00000535\n",
      "Epoch [49200/100000] Training Loss: 0.00000814 Testing Loss: 0.00000932 Validation loss 1: 0.00000355\n",
      "Epoch [49300/100000] Training Loss: 0.00000815 Testing Loss: 0.00000913 Validation loss 1: 0.00000297\n",
      "Epoch [49400/100000] Training Loss: 0.00000828 Testing Loss: 0.00000919 Validation loss 1: 0.00000384\n",
      "Epoch [49500/100000] Training Loss: 0.00000940 Testing Loss: 0.00000990 Validation loss 1: 0.00000437\n",
      "Epoch [49600/100000] Training Loss: 0.00000831 Testing Loss: 0.00000919 Validation loss 1: 0.00000357\n",
      "Epoch [49700/100000] Training Loss: 0.00000963 Testing Loss: 0.00001031 Validation loss 1: 0.00000513\n",
      "Epoch [49800/100000] Training Loss: 0.00001501 Testing Loss: 0.00001970 Validation loss 1: 0.00002888\n",
      "Epoch [49900/100000] Training Loss: 0.00000813 Testing Loss: 0.00000903 Validation loss 1: 0.00000307\n",
      "Epoch [50000/100000] Training Loss: 0.00000808 Testing Loss: 0.00000893 Validation loss 1: 0.00000282\n",
      "Epoch [50100/100000] Training Loss: 0.00000787 Testing Loss: 0.00000867 Validation loss 1: 0.00000240\n",
      "Epoch [50200/100000] Training Loss: 0.00000845 Testing Loss: 0.00000904 Validation loss 1: 0.00000280\n",
      "Epoch [50300/100000] Training Loss: 0.00000837 Testing Loss: 0.00000919 Validation loss 1: 0.00000276\n",
      "Epoch [50400/100000] Training Loss: 0.00000901 Testing Loss: 0.00001026 Validation loss 1: 0.00000561\n",
      "Epoch [50500/100000] Training Loss: 0.00000775 Testing Loss: 0.00000858 Validation loss 1: 0.00000229\n",
      "Epoch [50600/100000] Training Loss: 0.00006672 Testing Loss: 0.00006470 Validation loss 1: 0.00014139\n",
      "Epoch [50700/100000] Training Loss: 0.00001583 Testing Loss: 0.00001235 Validation loss 1: 0.00001281\n",
      "Epoch [50800/100000] Training Loss: 0.00000837 Testing Loss: 0.00000944 Validation loss 1: 0.00000355\n",
      "Epoch [50900/100000] Training Loss: 0.00000846 Testing Loss: 0.00000910 Validation loss 1: 0.00000363\n",
      "Epoch [51000/100000] Training Loss: 0.00000786 Testing Loss: 0.00000875 Validation loss 1: 0.00000268\n",
      "Epoch [51100/100000] Training Loss: 0.00000775 Testing Loss: 0.00000857 Validation loss 1: 0.00000243\n",
      "Epoch [51200/100000] Training Loss: 0.00000851 Testing Loss: 0.00000904 Validation loss 1: 0.00000380\n",
      "Epoch [51300/100000] Training Loss: 0.00000927 Testing Loss: 0.00001132 Validation loss 1: 0.00000761\n",
      "Epoch [51400/100000] Training Loss: 0.00000764 Testing Loss: 0.00000845 Validation loss 1: 0.00000228\n",
      "Epoch [51500/100000] Training Loss: 0.00000770 Testing Loss: 0.00000854 Validation loss 1: 0.00000240\n",
      "Epoch [51600/100000] Training Loss: 0.00000769 Testing Loss: 0.00000854 Validation loss 1: 0.00000256\n",
      "Epoch [51700/100000] Training Loss: 0.00000764 Testing Loss: 0.00000847 Validation loss 1: 0.00000225\n",
      "Epoch [51800/100000] Training Loss: 0.00000769 Testing Loss: 0.00000850 Validation loss 1: 0.00000230\n",
      "Epoch [51900/100000] Training Loss: 0.00000908 Testing Loss: 0.00001053 Validation loss 1: 0.00000734\n",
      "Epoch [52000/100000] Training Loss: 0.00000794 Testing Loss: 0.00000904 Validation loss 1: 0.00000369\n",
      "Epoch [52100/100000] Training Loss: 0.00000757 Testing Loss: 0.00000838 Validation loss 1: 0.00000227\n",
      "Epoch [52200/100000] Training Loss: 0.00000768 Testing Loss: 0.00000853 Validation loss 1: 0.00000248\n",
      "Epoch [52300/100000] Training Loss: 0.00008982 Testing Loss: 0.00008666 Validation loss 1: 0.00017966\n",
      "Epoch [52400/100000] Training Loss: 0.00000759 Testing Loss: 0.00000845 Validation loss 1: 0.00000270\n",
      "Epoch [52500/100000] Training Loss: 0.00000850 Testing Loss: 0.00000946 Validation loss 1: 0.00000540\n",
      "Epoch [52600/100000] Training Loss: 0.00002064 Testing Loss: 0.00002668 Validation loss 1: 0.00005154\n",
      "Epoch [52700/100000] Training Loss: 0.00000892 Testing Loss: 0.00001249 Validation loss 1: 0.00001120\n",
      "Epoch [52800/100000] Training Loss: 0.00000781 Testing Loss: 0.00000899 Validation loss 1: 0.00000379\n",
      "Epoch [52900/100000] Training Loss: 0.00016714 Testing Loss: 0.00021773 Validation loss 1: 0.00045792\n",
      "Epoch [53000/100000] Training Loss: 0.00000747 Testing Loss: 0.00000829 Validation loss 1: 0.00000232\n",
      "Epoch [53100/100000] Training Loss: 0.00002118 Testing Loss: 0.00002154 Validation loss 1: 0.00002688\n",
      "Epoch [53200/100000] Training Loss: 0.00000762 Testing Loss: 0.00000845 Validation loss 1: 0.00000287\n",
      "Epoch [53300/100000] Training Loss: 0.00002936 Testing Loss: 0.00004025 Validation loss 1: 0.00012568\n",
      "Epoch [53400/100000] Training Loss: 0.00001150 Testing Loss: 0.00001272 Validation loss 1: 0.00001156\n",
      "Epoch [53500/100000] Training Loss: 0.00006254 Testing Loss: 0.00005367 Validation loss 1: 0.00012109\n",
      "Epoch [53600/100000] Training Loss: 0.00007040 Testing Loss: 0.00002805 Validation loss 1: 0.00007041\n",
      "Epoch [53700/100000] Training Loss: 0.00000740 Testing Loss: 0.00000820 Validation loss 1: 0.00000222\n",
      "Epoch [53800/100000] Training Loss: 0.00000807 Testing Loss: 0.00000874 Validation loss 1: 0.00000302\n",
      "Epoch [53900/100000] Training Loss: 0.00001187 Testing Loss: 0.00001316 Validation loss 1: 0.00001377\n",
      "Epoch [54000/100000] Training Loss: 0.00002680 Testing Loss: 0.00002823 Validation loss 1: 0.00005584\n",
      "Epoch [54100/100000] Training Loss: 0.00001325 Testing Loss: 0.00001590 Validation loss 1: 0.00002324\n",
      "Epoch [54200/100000] Training Loss: 0.00000873 Testing Loss: 0.00000867 Validation loss 1: 0.00000301\n",
      "Epoch [54300/100000] Training Loss: 0.00000790 Testing Loss: 0.00000881 Validation loss 1: 0.00000259\n",
      "Epoch [54400/100000] Training Loss: 0.00000793 Testing Loss: 0.00000871 Validation loss 1: 0.00000341\n",
      "Epoch [54500/100000] Training Loss: 0.00000802 Testing Loss: 0.00000849 Validation loss 1: 0.00000335\n",
      "Epoch [54600/100000] Training Loss: 0.00000756 Testing Loss: 0.00000830 Validation loss 1: 0.00000234\n",
      "Epoch [54700/100000] Training Loss: 0.00000739 Testing Loss: 0.00000822 Validation loss 1: 0.00000232\n",
      "Epoch [54800/100000] Training Loss: 0.00000771 Testing Loss: 0.00000849 Validation loss 1: 0.00000279\n",
      "Epoch [54900/100000] Training Loss: 0.00001178 Testing Loss: 0.00001356 Validation loss 1: 0.00002218\n",
      "Epoch [55000/100000] Training Loss: 0.00002414 Testing Loss: 0.00002682 Validation loss 1: 0.00005173\n",
      "Epoch [55100/100000] Training Loss: 0.00002099 Testing Loss: 0.00002259 Validation loss 1: 0.00005204\n",
      "Epoch [55200/100000] Training Loss: 0.00000765 Testing Loss: 0.00000872 Validation loss 1: 0.00000370\n",
      "Epoch [55300/100000] Training Loss: 0.00000734 Testing Loss: 0.00000814 Validation loss 1: 0.00000253\n",
      "Epoch [55400/100000] Training Loss: 0.00004518 Testing Loss: 0.00004688 Validation loss 1: 0.00015144\n",
      "Epoch [55500/100000] Training Loss: 0.00000724 Testing Loss: 0.00000803 Validation loss 1: 0.00000241\n",
      "Epoch [55600/100000] Training Loss: 0.00000754 Testing Loss: 0.00000817 Validation loss 1: 0.00000241\n",
      "Epoch [55700/100000] Training Loss: 0.00000748 Testing Loss: 0.00000833 Validation loss 1: 0.00000290\n",
      "Epoch [55800/100000] Training Loss: 0.00000723 Testing Loss: 0.00000804 Validation loss 1: 0.00000262\n",
      "Epoch [55900/100000] Training Loss: 0.00002354 Testing Loss: 0.00001710 Validation loss 1: 0.00002661\n",
      "Epoch [56000/100000] Training Loss: 0.00000813 Testing Loss: 0.00000860 Validation loss 1: 0.00000358\n",
      "Epoch [56100/100000] Training Loss: 0.00000837 Testing Loss: 0.00000865 Validation loss 1: 0.00000429\n",
      "Epoch [56200/100000] Training Loss: 0.00002543 Testing Loss: 0.00002057 Validation loss 1: 0.00003580\n",
      "Epoch [56300/100000] Training Loss: 0.00000755 Testing Loss: 0.00000828 Validation loss 1: 0.00000358\n",
      "Epoch [56400/100000] Training Loss: 0.00002558 Testing Loss: 0.00003058 Validation loss 1: 0.00006082\n",
      "Epoch [56500/100000] Training Loss: 0.00001069 Testing Loss: 0.00001170 Validation loss 1: 0.00001811\n",
      "Epoch [56600/100000] Training Loss: 0.00000743 Testing Loss: 0.00000805 Validation loss 1: 0.00000270\n",
      "Epoch [56700/100000] Training Loss: 0.00000735 Testing Loss: 0.00000809 Validation loss 1: 0.00000267\n",
      "Epoch [56800/100000] Training Loss: 0.00000726 Testing Loss: 0.00000808 Validation loss 1: 0.00000245\n",
      "Epoch [56900/100000] Training Loss: 0.00000798 Testing Loss: 0.00000993 Validation loss 1: 0.00000590\n",
      "Epoch [57000/100000] Training Loss: 0.00000704 Testing Loss: 0.00000783 Validation loss 1: 0.00000215\n",
      "Epoch [57100/100000] Training Loss: 0.00000730 Testing Loss: 0.00000804 Validation loss 1: 0.00000252\n",
      "Epoch [57200/100000] Training Loss: 0.00000705 Testing Loss: 0.00000785 Validation loss 1: 0.00000220\n",
      "Epoch [57300/100000] Training Loss: 0.00000736 Testing Loss: 0.00000832 Validation loss 1: 0.00000371\n",
      "Epoch [57400/100000] Training Loss: 0.00000992 Testing Loss: 0.00001066 Validation loss 1: 0.00000961\n",
      "Epoch [57500/100000] Training Loss: 0.00000753 Testing Loss: 0.00000807 Validation loss 1: 0.00000277\n",
      "Epoch [57600/100000] Training Loss: 0.00002320 Testing Loss: 0.00002507 Validation loss 1: 0.00006892\n",
      "Epoch [57700/100000] Training Loss: 0.00001221 Testing Loss: 0.00001224 Validation loss 1: 0.00001120\n",
      "Epoch [57800/100000] Training Loss: 0.00001309 Testing Loss: 0.00001380 Validation loss 1: 0.00001622\n",
      "Epoch [57900/100000] Training Loss: 0.00000755 Testing Loss: 0.00000843 Validation loss 1: 0.00000415\n",
      "Epoch [58000/100000] Training Loss: 0.00000732 Testing Loss: 0.00000852 Validation loss 1: 0.00000396\n",
      "Epoch [58100/100000] Training Loss: 0.00000696 Testing Loss: 0.00000778 Validation loss 1: 0.00000244\n",
      "Epoch [58200/100000] Training Loss: 0.00008205 Testing Loss: 0.00008520 Validation loss 1: 0.00020843\n",
      "Epoch [58300/100000] Training Loss: 0.00001220 Testing Loss: 0.00001262 Validation loss 1: 0.00001513\n",
      "Epoch [58400/100000] Training Loss: 0.00000783 Testing Loss: 0.00000852 Validation loss 1: 0.00000427\n",
      "Epoch [58500/100000] Training Loss: 0.00000693 Testing Loss: 0.00000771 Validation loss 1: 0.00000212\n",
      "Epoch [58600/100000] Training Loss: 0.00012603 Testing Loss: 0.00008957 Validation loss 1: 0.00020721\n",
      "Epoch [58700/100000] Training Loss: 0.00000823 Testing Loss: 0.00000809 Validation loss 1: 0.00000290\n",
      "Epoch [58800/100000] Training Loss: 0.00005592 Testing Loss: 0.00006322 Validation loss 1: 0.00013438\n",
      "Epoch [58900/100000] Training Loss: 0.00000929 Testing Loss: 0.00000996 Validation loss 1: 0.00000796\n",
      "Epoch [59000/100000] Training Loss: 0.00000773 Testing Loss: 0.00000887 Validation loss 1: 0.00000630\n",
      "Epoch [59100/100000] Training Loss: 0.00000954 Testing Loss: 0.00001032 Validation loss 1: 0.00001181\n",
      "Epoch [59200/100000] Training Loss: 0.00002816 Testing Loss: 0.00003239 Validation loss 1: 0.00006246\n",
      "Epoch [59300/100000] Training Loss: 0.00000914 Testing Loss: 0.00001376 Validation loss 1: 0.00001583\n",
      "Epoch [59400/100000] Training Loss: 0.00000810 Testing Loss: 0.00000902 Validation loss 1: 0.00000507\n",
      "Epoch [59500/100000] Training Loss: 0.00000752 Testing Loss: 0.00000778 Validation loss 1: 0.00000257\n",
      "Epoch [59600/100000] Training Loss: 0.00000757 Testing Loss: 0.00000819 Validation loss 1: 0.00000290\n",
      "Epoch [59700/100000] Training Loss: 0.00000716 Testing Loss: 0.00000784 Validation loss 1: 0.00000272\n",
      "Epoch [59800/100000] Training Loss: 0.00004695 Testing Loss: 0.00005214 Validation loss 1: 0.00012428\n",
      "Epoch [59900/100000] Training Loss: 0.00000719 Testing Loss: 0.00000926 Validation loss 1: 0.00000765\n",
      "Epoch [60000/100000] Training Loss: 0.00001233 Testing Loss: 0.00001085 Validation loss 1: 0.00001201\n",
      "Epoch [60100/100000] Training Loss: 0.00000825 Testing Loss: 0.00000870 Validation loss 1: 0.00000519\n",
      "Epoch [60200/100000] Training Loss: 0.00001399 Testing Loss: 0.00001446 Validation loss 1: 0.00002310\n",
      "Epoch [60300/100000] Training Loss: 0.00007715 Testing Loss: 0.00008143 Validation loss 1: 0.00026336\n",
      "Epoch [60400/100000] Training Loss: 0.00001257 Testing Loss: 0.00001315 Validation loss 1: 0.00007479\n",
      "Epoch [60500/100000] Training Loss: 0.00001102 Testing Loss: 0.00001166 Validation loss 1: 0.00007003\n",
      "Epoch [60600/100000] Training Loss: 0.00001047 Testing Loss: 0.00001113 Validation loss 1: 0.00006794\n",
      "Epoch [60700/100000] Training Loss: 0.00001253 Testing Loss: 0.00001174 Validation loss 1: 0.00007936\n",
      "Epoch [60800/100000] Training Loss: 0.00001001 Testing Loss: 0.00001069 Validation loss 1: 0.00006632\n",
      "Epoch [60900/100000] Training Loss: 0.00000987 Testing Loss: 0.00001056 Validation loss 1: 0.00006587\n",
      "Epoch [61000/100000] Training Loss: 0.00000979 Testing Loss: 0.00001048 Validation loss 1: 0.00006582\n",
      "Epoch [61100/100000] Training Loss: 0.00001012 Testing Loss: 0.00001067 Validation loss 1: 0.00006609\n",
      "Epoch [61200/100000] Training Loss: 0.00001118 Testing Loss: 0.00001125 Validation loss 1: 0.00006707\n",
      "Epoch [61300/100000] Training Loss: 0.00001567 Testing Loss: 0.00001957 Validation loss 1: 0.00009382\n",
      "Epoch [61400/100000] Training Loss: 0.00003384 Testing Loss: 0.00004325 Validation loss 1: 0.00016812\n",
      "Epoch [61500/100000] Training Loss: 0.00000950 Testing Loss: 0.00001021 Validation loss 1: 0.00006527\n",
      "Epoch [61600/100000] Training Loss: 0.00001548 Testing Loss: 0.00001299 Validation loss 1: 0.00007278\n",
      "Epoch [61700/100000] Training Loss: 0.00000985 Testing Loss: 0.00001105 Validation loss 1: 0.00006847\n",
      "Epoch [61800/100000] Training Loss: 0.00000961 Testing Loss: 0.00001029 Validation loss 1: 0.00006694\n",
      "Epoch [61900/100000] Training Loss: 0.00002420 Testing Loss: 0.00001744 Validation loss 1: 0.00008728\n",
      "Epoch [62000/100000] Training Loss: 0.00001426 Testing Loss: 0.00001210 Validation loss 1: 0.00006627\n",
      "Epoch [62100/100000] Training Loss: 0.00000935 Testing Loss: 0.00001008 Validation loss 1: 0.00006489\n",
      "Epoch [62200/100000] Training Loss: 0.00000945 Testing Loss: 0.00001019 Validation loss 1: 0.00006248\n",
      "Epoch [62300/100000] Training Loss: 0.00000718 Testing Loss: 0.00000794 Validation loss 1: 0.00000260\n",
      "Epoch [62400/100000] Training Loss: 0.00000686 Testing Loss: 0.00000762 Validation loss 1: 0.00000206\n",
      "Epoch [62500/100000] Training Loss: 0.00001289 Testing Loss: 0.00001638 Validation loss 1: 0.00002064\n",
      "Epoch [62600/100000] Training Loss: 0.00001152 Testing Loss: 0.00001049 Validation loss 1: 0.00001019\n",
      "Epoch [62700/100000] Training Loss: 0.00000806 Testing Loss: 0.00000911 Validation loss 1: 0.00000754\n",
      "Epoch [62800/100000] Training Loss: 0.00001155 Testing Loss: 0.00001369 Validation loss 1: 0.00002747\n",
      "Epoch [62900/100000] Training Loss: 0.00000689 Testing Loss: 0.00000761 Validation loss 1: 0.00000226\n",
      "Epoch 62958: reducing learning rate of group 0 to 7.0000e-04.\n",
      "Epoch [63000/100000] Training Loss: 0.00000670 Testing Loss: 0.00000745 Validation loss 1: 0.00000198\n",
      "Epoch [63100/100000] Training Loss: 0.00000663 Testing Loss: 0.00000739 Validation loss 1: 0.00000191\n",
      "Epoch [63200/100000] Training Loss: 0.00000660 Testing Loss: 0.00000735 Validation loss 1: 0.00000190\n",
      "Epoch [63300/100000] Training Loss: 0.00000657 Testing Loss: 0.00000732 Validation loss 1: 0.00000188\n",
      "Epoch [63400/100000] Training Loss: 0.00000654 Testing Loss: 0.00000729 Validation loss 1: 0.00000186\n",
      "Epoch [63500/100000] Training Loss: 0.00000651 Testing Loss: 0.00000726 Validation loss 1: 0.00000185\n",
      "Epoch [63600/100000] Training Loss: 0.00000667 Testing Loss: 0.00000751 Validation loss 1: 0.00000304\n",
      "Epoch [63700/100000] Training Loss: 0.00001159 Testing Loss: 0.00001488 Validation loss 1: 0.00002071\n",
      "Epoch [63800/100000] Training Loss: 0.00006408 Testing Loss: 0.00007140 Validation loss 1: 0.00014420\n",
      "Epoch [63900/100000] Training Loss: 0.00000647 Testing Loss: 0.00000722 Validation loss 1: 0.00000185\n",
      "Epoch [64000/100000] Training Loss: 0.00001105 Testing Loss: 0.00001140 Validation loss 1: 0.00001314\n",
      "Epoch [64100/100000] Training Loss: 0.00000818 Testing Loss: 0.00000945 Validation loss 1: 0.00000770\n",
      "Epoch [64200/100000] Training Loss: 0.00000674 Testing Loss: 0.00000729 Validation loss 1: 0.00000206\n",
      "Epoch [64300/100000] Training Loss: 0.00000727 Testing Loss: 0.00000795 Validation loss 1: 0.00000392\n",
      "Epoch [64400/100000] Training Loss: 0.00000669 Testing Loss: 0.00000764 Validation loss 1: 0.00000242\n",
      "Epoch [64500/100000] Training Loss: 0.00000763 Testing Loss: 0.00000788 Validation loss 1: 0.00000287\n",
      "Epoch [64600/100000] Training Loss: 0.00000654 Testing Loss: 0.00000729 Validation loss 1: 0.00000197\n",
      "Epoch [64700/100000] Training Loss: 0.00000645 Testing Loss: 0.00000720 Validation loss 1: 0.00000192\n",
      "Epoch [64800/100000] Training Loss: 0.00000640 Testing Loss: 0.00000714 Validation loss 1: 0.00000190\n",
      "Epoch [64900/100000] Training Loss: 0.00001243 Testing Loss: 0.00001060 Validation loss 1: 0.00001326\n",
      "Epoch [65000/100000] Training Loss: 0.00000679 Testing Loss: 0.00000743 Validation loss 1: 0.00000289\n",
      "Epoch [65100/100000] Training Loss: 0.00000707 Testing Loss: 0.00000781 Validation loss 1: 0.00000401\n",
      "Epoch [65200/100000] Training Loss: 0.00001323 Testing Loss: 0.00001504 Validation loss 1: 0.00002851\n",
      "Epoch [65300/100000] Training Loss: 0.00000681 Testing Loss: 0.00000754 Validation loss 1: 0.00000323\n",
      "Epoch [65400/100000] Training Loss: 0.00000753 Testing Loss: 0.00000822 Validation loss 1: 0.00000493\n",
      "Epoch [65500/100000] Training Loss: 0.00000860 Testing Loss: 0.00000785 Validation loss 1: 0.00000404\n",
      "Epoch [65600/100000] Training Loss: 0.00000632 Testing Loss: 0.00000711 Validation loss 1: 0.00000221\n",
      "Epoch [65700/100000] Training Loss: 0.00000974 Testing Loss: 0.00001006 Validation loss 1: 0.00001201\n",
      "Epoch [65800/100000] Training Loss: 0.00002123 Testing Loss: 0.00002776 Validation loss 1: 0.00006331\n",
      "Epoch [65900/100000] Training Loss: 0.00002779 Testing Loss: 0.00001721 Validation loss 1: 0.00002425\n",
      "Epoch [66000/100000] Training Loss: 0.00000906 Testing Loss: 0.00001174 Validation loss 1: 0.00001373\n",
      "Epoch [66100/100000] Training Loss: 0.00000683 Testing Loss: 0.00000775 Validation loss 1: 0.00000369\n",
      "Epoch [66200/100000] Training Loss: 0.00000679 Testing Loss: 0.00000747 Validation loss 1: 0.00000310\n",
      "Epoch [66300/100000] Training Loss: 0.00001814 Testing Loss: 0.00001432 Validation loss 1: 0.00002111\n",
      "Epoch [66400/100000] Training Loss: 0.00011014 Testing Loss: 0.00011307 Validation loss 1: 0.00024585\n",
      "Epoch [66500/100000] Training Loss: 0.00000624 Testing Loss: 0.00000698 Validation loss 1: 0.00000198\n",
      "Epoch [66600/100000] Training Loss: 0.00000617 Testing Loss: 0.00000691 Validation loss 1: 0.00000194\n",
      "Epoch [66700/100000] Training Loss: 0.00003114 Testing Loss: 0.00002605 Validation loss 1: 0.00005938\n",
      "Epoch [66800/100000] Training Loss: 0.00000716 Testing Loss: 0.00000812 Validation loss 1: 0.00000374\n",
      "Epoch [66900/100000] Training Loss: 0.00000626 Testing Loss: 0.00000697 Validation loss 1: 0.00000220\n",
      "Epoch [67000/100000] Training Loss: 0.00000890 Testing Loss: 0.00000920 Validation loss 1: 0.00000905\n",
      "Epoch [67100/100000] Training Loss: 0.00000818 Testing Loss: 0.00000917 Validation loss 1: 0.00000733\n",
      "Epoch [67200/100000] Training Loss: 0.00002100 Testing Loss: 0.00002283 Validation loss 1: 0.00004658\n",
      "Epoch [67300/100000] Training Loss: 0.00000639 Testing Loss: 0.00000695 Validation loss 1: 0.00000222\n",
      "Epoch [67400/100000] Training Loss: 0.00000608 Testing Loss: 0.00000683 Validation loss 1: 0.00000188\n",
      "Epoch [67500/100000] Training Loss: 0.00000610 Testing Loss: 0.00000684 Validation loss 1: 0.00000192\n",
      "Epoch [67600/100000] Training Loss: 0.00005460 Testing Loss: 0.00006639 Validation loss 1: 0.00014453\n",
      "Epoch [67700/100000] Training Loss: 0.00000605 Testing Loss: 0.00000678 Validation loss 1: 0.00000189\n",
      "Epoch [67800/100000] Training Loss: 0.00000877 Testing Loss: 0.00001039 Validation loss 1: 0.00001197\n",
      "Epoch [67900/100000] Training Loss: 0.00000859 Testing Loss: 0.00000846 Validation loss 1: 0.00000356\n",
      "Epoch [68000/100000] Training Loss: 0.00000775 Testing Loss: 0.00000801 Validation loss 1: 0.00000495\n",
      "Epoch [68100/100000] Training Loss: 0.00000620 Testing Loss: 0.00000689 Validation loss 1: 0.00000227\n",
      "Epoch [68200/100000] Training Loss: 0.00000606 Testing Loss: 0.00000678 Validation loss 1: 0.00000199\n",
      "Epoch [68300/100000] Training Loss: 0.00000603 Testing Loss: 0.00000676 Validation loss 1: 0.00000187\n",
      "Epoch [68400/100000] Training Loss: 0.00000625 Testing Loss: 0.00000696 Validation loss 1: 0.00000258\n",
      "Epoch [68500/100000] Training Loss: 0.00003016 Testing Loss: 0.00002165 Validation loss 1: 0.00004354\n",
      "Epoch [68600/100000] Training Loss: 0.00001045 Testing Loss: 0.00001181 Validation loss 1: 0.00001534\n",
      "Epoch [68700/100000] Training Loss: 0.00000792 Testing Loss: 0.00000959 Validation loss 1: 0.00001019\n",
      "Epoch [68800/100000] Training Loss: 0.00000686 Testing Loss: 0.00000778 Validation loss 1: 0.00000508\n",
      "Epoch [68900/100000] Training Loss: 0.00000683 Testing Loss: 0.00000723 Validation loss 1: 0.00000306\n",
      "Epoch [69000/100000] Training Loss: 0.00000615 Testing Loss: 0.00000698 Validation loss 1: 0.00000277\n",
      "Epoch [69100/100000] Training Loss: 0.00000619 Testing Loss: 0.00000696 Validation loss 1: 0.00000240\n",
      "Epoch [69200/100000] Training Loss: 0.00000591 Testing Loss: 0.00000665 Validation loss 1: 0.00000187\n",
      "Epoch [69300/100000] Training Loss: 0.00000601 Testing Loss: 0.00000678 Validation loss 1: 0.00000228\n",
      "Epoch [69400/100000] Training Loss: 0.00007436 Testing Loss: 0.00008983 Validation loss 1: 0.00018522\n",
      "Epoch [69500/100000] Training Loss: 0.00000587 Testing Loss: 0.00000660 Validation loss 1: 0.00000179\n",
      "Epoch [69600/100000] Training Loss: 0.00000874 Testing Loss: 0.00000879 Validation loss 1: 0.00000864\n",
      "Epoch [69700/100000] Training Loss: 0.00000615 Testing Loss: 0.00000680 Validation loss 1: 0.00000242\n",
      "Epoch [69800/100000] Training Loss: 0.00000646 Testing Loss: 0.00000707 Validation loss 1: 0.00000360\n",
      "Epoch [69900/100000] Training Loss: 0.00000634 Testing Loss: 0.00000698 Validation loss 1: 0.00000238\n",
      "Epoch [70000/100000] Training Loss: 0.00001518 Testing Loss: 0.00001792 Validation loss 1: 0.00003197\n",
      "Epoch [70100/100000] Training Loss: 0.00000675 Testing Loss: 0.00000760 Validation loss 1: 0.00000460\n",
      "Epoch [70200/100000] Training Loss: 0.00001217 Testing Loss: 0.00001388 Validation loss 1: 0.00002161\n",
      "Epoch [70300/100000] Training Loss: 0.00000651 Testing Loss: 0.00000686 Validation loss 1: 0.00000291\n",
      "Epoch [70400/100000] Training Loss: 0.00000584 Testing Loss: 0.00000666 Validation loss 1: 0.00000221\n",
      "Epoch [70500/100000] Training Loss: 0.00000723 Testing Loss: 0.00000774 Validation loss 1: 0.00000243\n",
      "Epoch [70600/100000] Training Loss: 0.00000669 Testing Loss: 0.00000773 Validation loss 1: 0.00000499\n",
      "Epoch [70700/100000] Training Loss: 0.00000667 Testing Loss: 0.00000716 Validation loss 1: 0.00000358\n",
      "Epoch [70800/100000] Training Loss: 0.00002371 Testing Loss: 0.00002822 Validation loss 1: 0.00006028\n",
      "Epoch [70900/100000] Training Loss: 0.00000629 Testing Loss: 0.00000704 Validation loss 1: 0.00000311\n",
      "Epoch [71000/100000] Training Loss: 0.00000587 Testing Loss: 0.00000669 Validation loss 1: 0.00000218\n",
      "Epoch [71100/100000] Training Loss: 0.00000672 Testing Loss: 0.00000779 Validation loss 1: 0.00000621\n",
      "Epoch [71200/100000] Training Loss: 0.00000837 Testing Loss: 0.00000845 Validation loss 1: 0.00000819\n",
      "Epoch [71300/100000] Training Loss: 0.00000596 Testing Loss: 0.00000684 Validation loss 1: 0.00000242\n",
      "Epoch [71400/100000] Training Loss: 0.00000579 Testing Loss: 0.00000654 Validation loss 1: 0.00000195\n",
      "Epoch [71500/100000] Training Loss: 0.00000580 Testing Loss: 0.00000653 Validation loss 1: 0.00000195\n",
      "Epoch [71600/100000] Training Loss: 0.00000606 Testing Loss: 0.00000667 Validation loss 1: 0.00000206\n",
      "Epoch [71700/100000] Training Loss: 0.00000641 Testing Loss: 0.00000737 Validation loss 1: 0.00000439\n",
      "Epoch [71800/100000] Training Loss: 0.00000591 Testing Loss: 0.00000661 Validation loss 1: 0.00000223\n",
      "Epoch [71900/100000] Training Loss: 0.00001667 Testing Loss: 0.00002003 Validation loss 1: 0.00003819\n",
      "Epoch [72000/100000] Training Loss: 0.00000638 Testing Loss: 0.00000709 Validation loss 1: 0.00000404\n",
      "Epoch [72100/100000] Training Loss: 0.00000573 Testing Loss: 0.00000647 Validation loss 1: 0.00000217\n",
      "Epoch [72200/100000] Training Loss: 0.00000570 Testing Loss: 0.00000642 Validation loss 1: 0.00000224\n",
      "Epoch [72300/100000] Training Loss: 0.00000834 Testing Loss: 0.00001009 Validation loss 1: 0.00001308\n",
      "Epoch [72400/100000] Training Loss: 0.00000729 Testing Loss: 0.00000889 Validation loss 1: 0.00000839\n",
      "Epoch [72500/100000] Training Loss: 0.00000579 Testing Loss: 0.00000654 Validation loss 1: 0.00000236\n",
      "Epoch [72600/100000] Training Loss: 0.00000572 Testing Loss: 0.00000644 Validation loss 1: 0.00000195\n",
      "Epoch [72700/100000] Training Loss: 0.00000568 Testing Loss: 0.00000638 Validation loss 1: 0.00000181\n",
      "Epoch [72800/100000] Training Loss: 0.00000573 Testing Loss: 0.00000649 Validation loss 1: 0.00000164\n",
      "Epoch [72900/100000] Training Loss: 0.00000563 Testing Loss: 0.00000634 Validation loss 1: 0.00000208\n",
      "Epoch [73000/100000] Training Loss: 0.00000601 Testing Loss: 0.00000692 Validation loss 1: 0.00000388\n",
      "Epoch [73100/100000] Training Loss: 0.00001236 Testing Loss: 0.00001181 Validation loss 1: 0.00001422\n",
      "Epoch [73200/100000] Training Loss: 0.00000607 Testing Loss: 0.00000672 Validation loss 1: 0.00000294\n",
      "Epoch [73300/100000] Training Loss: 0.00000630 Testing Loss: 0.00000730 Validation loss 1: 0.00000510\n",
      "Epoch [73400/100000] Training Loss: 0.00000615 Testing Loss: 0.00000686 Validation loss 1: 0.00000292\n",
      "Epoch [73500/100000] Training Loss: 0.00001051 Testing Loss: 0.00000855 Validation loss 1: 0.00000780\n",
      "Epoch [73600/100000] Training Loss: 0.00000579 Testing Loss: 0.00000661 Validation loss 1: 0.00000271\n",
      "Epoch [73700/100000] Training Loss: 0.00000559 Testing Loss: 0.00000631 Validation loss 1: 0.00000182\n",
      "Epoch [73800/100000] Training Loss: 0.00000586 Testing Loss: 0.00000669 Validation loss 1: 0.00000312\n",
      "Epoch [73900/100000] Training Loss: 0.00000898 Testing Loss: 0.00000737 Validation loss 1: 0.00000447\n",
      "Epoch [74000/100000] Training Loss: 0.00000711 Testing Loss: 0.00000775 Validation loss 1: 0.00000402\n",
      "Epoch [74100/100000] Training Loss: 0.00000555 Testing Loss: 0.00000626 Validation loss 1: 0.00000169\n",
      "Epoch [74200/100000] Training Loss: 0.00000725 Testing Loss: 0.00000723 Validation loss 1: 0.00000391\n",
      "Epoch [74300/100000] Training Loss: 0.00000707 Testing Loss: 0.00000757 Validation loss 1: 0.00000561\n",
      "Epoch [74400/100000] Training Loss: 0.00000559 Testing Loss: 0.00000634 Validation loss 1: 0.00000248\n",
      "Epoch [74500/100000] Training Loss: 0.00001853 Testing Loss: 0.00001781 Validation loss 1: 0.00003249\n",
      "Epoch [74600/100000] Training Loss: 0.00001205 Testing Loss: 0.00001249 Validation loss 1: 0.00001779\n",
      "Epoch [74700/100000] Training Loss: 0.00001509 Testing Loss: 0.00001484 Validation loss 1: 0.00002390\n",
      "Epoch [74800/100000] Training Loss: 0.00000677 Testing Loss: 0.00000768 Validation loss 1: 0.00000551\n",
      "Epoch [74900/100000] Training Loss: 0.00000616 Testing Loss: 0.00000731 Validation loss 1: 0.00000487\n",
      "Epoch [75000/100000] Training Loss: 0.00000632 Testing Loss: 0.00000684 Validation loss 1: 0.00000293\n",
      "Epoch [75100/100000] Training Loss: 0.00001390 Testing Loss: 0.00001433 Validation loss 1: 0.00002060\n",
      "Epoch [75200/100000] Training Loss: 0.00000584 Testing Loss: 0.00000666 Validation loss 1: 0.00000295\n",
      "Epoch [75300/100000] Training Loss: 0.00001131 Testing Loss: 0.00001421 Validation loss 1: 0.00002253\n",
      "Epoch [75400/100000] Training Loss: 0.00000589 Testing Loss: 0.00000662 Validation loss 1: 0.00000280\n",
      "Epoch [75500/100000] Training Loss: 0.00000783 Testing Loss: 0.00000738 Validation loss 1: 0.00000380\n",
      "Epoch [75600/100000] Training Loss: 0.00005646 Testing Loss: 0.00006210 Validation loss 1: 0.00014980\n",
      "Epoch [75700/100000] Training Loss: 0.00004513 Testing Loss: 0.00003895 Validation loss 1: 0.00009423\n",
      "Epoch [75800/100000] Training Loss: 0.00001434 Testing Loss: 0.00001981 Validation loss 1: 0.00005301\n",
      "Epoch [75900/100000] Training Loss: 0.00000600 Testing Loss: 0.00000695 Validation loss 1: 0.00000430\n",
      "Epoch [76000/100000] Training Loss: 0.00000924 Testing Loss: 0.00001139 Validation loss 1: 0.00001622\n",
      "Epoch [76100/100000] Training Loss: 0.00000550 Testing Loss: 0.00000617 Validation loss 1: 0.00000188\n",
      "Epoch [76200/100000] Training Loss: 0.00003763 Testing Loss: 0.00003001 Validation loss 1: 0.00006894\n",
      "Epoch [76300/100000] Training Loss: 0.00028342 Testing Loss: 0.00021669 Validation loss 1: 0.00049502\n",
      "Epoch [76400/100000] Training Loss: 0.00000549 Testing Loss: 0.00000619 Validation loss 1: 0.00000167\n",
      "Epoch [76500/100000] Training Loss: 0.00000541 Testing Loss: 0.00000611 Validation loss 1: 0.00000155\n",
      "Epoch [76600/100000] Training Loss: 0.00000772 Testing Loss: 0.00000970 Validation loss 1: 0.00001312\n",
      "Epoch [76700/100000] Training Loss: 0.00000574 Testing Loss: 0.00000665 Validation loss 1: 0.00000327\n",
      "Epoch [76800/100000] Training Loss: 0.00003347 Testing Loss: 0.00002485 Validation loss 1: 0.00005323\n",
      "Epoch [76900/100000] Training Loss: 0.00001281 Testing Loss: 0.00001329 Validation loss 1: 0.00002415\n",
      "Epoch [77000/100000] Training Loss: 0.00000578 Testing Loss: 0.00000634 Validation loss 1: 0.00000238\n",
      "Epoch [77100/100000] Training Loss: 0.00000543 Testing Loss: 0.00000609 Validation loss 1: 0.00000182\n",
      "Epoch [77200/100000] Training Loss: 0.00000569 Testing Loss: 0.00000654 Validation loss 1: 0.00000305\n",
      "Epoch [77300/100000] Training Loss: 0.00000766 Testing Loss: 0.00000759 Validation loss 1: 0.00000619\n",
      "Epoch [77400/100000] Training Loss: 0.00000631 Testing Loss: 0.00000712 Validation loss 1: 0.00000470\n",
      "Epoch [77500/100000] Training Loss: 0.00000557 Testing Loss: 0.00000635 Validation loss 1: 0.00000321\n",
      "Epoch [77600/100000] Training Loss: 0.00000765 Testing Loss: 0.00000796 Validation loss 1: 0.00000749\n",
      "Epoch [77700/100000] Training Loss: 0.00000554 Testing Loss: 0.00000630 Validation loss 1: 0.00000317\n",
      "Epoch [77800/100000] Training Loss: 0.00000532 Testing Loss: 0.00000601 Validation loss 1: 0.00000156\n",
      "Epoch [77900/100000] Training Loss: 0.00000751 Testing Loss: 0.00000930 Validation loss 1: 0.00000928\n",
      "Epoch [78000/100000] Training Loss: 0.00000531 Testing Loss: 0.00000601 Validation loss 1: 0.00000157\n",
      "Epoch [78100/100000] Training Loss: 0.00000598 Testing Loss: 0.00000636 Validation loss 1: 0.00000205\n",
      "Epoch [78200/100000] Training Loss: 0.00000538 Testing Loss: 0.00000606 Validation loss 1: 0.00000172\n",
      "Epoch [78300/100000] Training Loss: 0.00000910 Testing Loss: 0.00000722 Validation loss 1: 0.00000520\n",
      "Epoch [78400/100000] Training Loss: 0.00000743 Testing Loss: 0.00000771 Validation loss 1: 0.00000560\n",
      "Epoch [78500/100000] Training Loss: 0.00000760 Testing Loss: 0.00000790 Validation loss 1: 0.00000646\n",
      "Epoch [78600/100000] Training Loss: 0.00004446 Testing Loss: 0.00003919 Validation loss 1: 0.00009269\n",
      "Epoch [78700/100000] Training Loss: 0.00000666 Testing Loss: 0.00000677 Validation loss 1: 0.00000368\n",
      "Epoch [78800/100000] Training Loss: 0.00000532 Testing Loss: 0.00000601 Validation loss 1: 0.00000176\n",
      "Epoch [78900/100000] Training Loss: 0.00000795 Testing Loss: 0.00000957 Validation loss 1: 0.00001127\n",
      "Epoch [79000/100000] Training Loss: 0.00000525 Testing Loss: 0.00000594 Validation loss 1: 0.00000300\n",
      "Epoch [79100/100000] Training Loss: 0.00000525 Testing Loss: 0.00000593 Validation loss 1: 0.00000142\n",
      "Epoch [79200/100000] Training Loss: 0.00000526 Testing Loss: 0.00000595 Validation loss 1: 0.00000149\n",
      "Epoch [79300/100000] Training Loss: 0.00000527 Testing Loss: 0.00000596 Validation loss 1: 0.00000155\n",
      "Epoch [79400/100000] Training Loss: 0.00000536 Testing Loss: 0.00000620 Validation loss 1: 0.00000210\n",
      "Epoch [79500/100000] Training Loss: 0.00000524 Testing Loss: 0.00000593 Validation loss 1: 0.00000214\n",
      "Epoch [79600/100000] Training Loss: 0.00000538 Testing Loss: 0.00000614 Validation loss 1: 0.00000306\n",
      "Epoch [79700/100000] Training Loss: 0.00000711 Testing Loss: 0.00000868 Validation loss 1: 0.00000916\n",
      "Epoch [79800/100000] Training Loss: 0.00000650 Testing Loss: 0.00000682 Validation loss 1: 0.00000448\n",
      "Epoch [79900/100000] Training Loss: 0.00000781 Testing Loss: 0.00000804 Validation loss 1: 0.00000802\n",
      "Epoch [80000/100000] Training Loss: 0.00000553 Testing Loss: 0.00000616 Validation loss 1: 0.00000322\n",
      "Epoch [80100/100000] Training Loss: 0.00000545 Testing Loss: 0.00000626 Validation loss 1: 0.00000353\n",
      "Epoch [80200/100000] Training Loss: 0.00000538 Testing Loss: 0.00000607 Validation loss 1: 0.00000240\n",
      "Epoch [80300/100000] Training Loss: 0.00000587 Testing Loss: 0.00000680 Validation loss 1: 0.00000392\n",
      "Epoch [80400/100000] Training Loss: 0.00000519 Testing Loss: 0.00000588 Validation loss 1: 0.00000191\n",
      "Epoch [80500/100000] Training Loss: 0.00000533 Testing Loss: 0.00000594 Validation loss 1: 0.00000238\n",
      "Epoch [80600/100000] Training Loss: 0.00000519 Testing Loss: 0.00000586 Validation loss 1: 0.00000229\n",
      "Epoch [80700/100000] Training Loss: 0.00000532 Testing Loss: 0.00000603 Validation loss 1: 0.00000300\n",
      "Epoch [80800/100000] Training Loss: 0.00000759 Testing Loss: 0.00000950 Validation loss 1: 0.00001055\n",
      "Epoch [80900/100000] Training Loss: 0.00000515 Testing Loss: 0.00000584 Validation loss 1: 0.00000228\n",
      "Epoch [81000/100000] Training Loss: 0.00000528 Testing Loss: 0.00000602 Validation loss 1: 0.00000313\n",
      "Epoch [81100/100000] Training Loss: 0.00000521 Testing Loss: 0.00000590 Validation loss 1: 0.00000146\n",
      "Epoch [81200/100000] Training Loss: 0.00000723 Testing Loss: 0.00000659 Validation loss 1: 0.00000236\n",
      "Epoch [81300/100000] Training Loss: 0.00000517 Testing Loss: 0.00000586 Validation loss 1: 0.00000153\n",
      "Epoch [81400/100000] Training Loss: 0.00002693 Testing Loss: 0.00003501 Validation loss 1: 0.00008153\n",
      "Epoch [81500/100000] Training Loss: 0.00000739 Testing Loss: 0.00000926 Validation loss 1: 0.00001183\n",
      "Epoch [81600/100000] Training Loss: 0.00000578 Testing Loss: 0.00000647 Validation loss 1: 0.00000299\n",
      "Epoch [81700/100000] Training Loss: 0.00003038 Testing Loss: 0.00003787 Validation loss 1: 0.00008315\n",
      "Epoch [81800/100000] Training Loss: 0.00000663 Testing Loss: 0.00000804 Validation loss 1: 0.00000962\n",
      "Epoch [81900/100000] Training Loss: 0.00001427 Testing Loss: 0.00001697 Validation loss 1: 0.00002970\n",
      "Epoch [82000/100000] Training Loss: 0.00000717 Testing Loss: 0.00000798 Validation loss 1: 0.00000639\n",
      "Epoch [82100/100000] Training Loss: 0.00001094 Testing Loss: 0.00000955 Validation loss 1: 0.00001382\n",
      "Epoch [82200/100000] Training Loss: 0.00000514 Testing Loss: 0.00000581 Validation loss 1: 0.00000155\n",
      "Epoch [82300/100000] Training Loss: 0.00000538 Testing Loss: 0.00000619 Validation loss 1: 0.00000302\n",
      "Epoch [82400/100000] Training Loss: 0.00001531 Testing Loss: 0.00001945 Validation loss 1: 0.00003505\n",
      "Epoch [82500/100000] Training Loss: 0.00002369 Testing Loss: 0.00003286 Validation loss 1: 0.00007613\n",
      "Epoch [82600/100000] Training Loss: 0.00000506 Testing Loss: 0.00000574 Validation loss 1: 0.00000144\n",
      "Epoch [82700/100000] Training Loss: 0.00000511 Testing Loss: 0.00000581 Validation loss 1: 0.00000170\n",
      "Epoch [82800/100000] Training Loss: 0.00002272 Testing Loss: 0.00001406 Validation loss 1: 0.00002710\n",
      "Epoch [82900/100000] Training Loss: 0.00000550 Testing Loss: 0.00000607 Validation loss 1: 0.00000233\n",
      "Epoch [83000/100000] Training Loss: 0.00000525 Testing Loss: 0.00000599 Validation loss 1: 0.00000309\n",
      "Epoch [83100/100000] Training Loss: 0.00000841 Testing Loss: 0.00000781 Validation loss 1: 0.00000386\n",
      "Epoch [83200/100000] Training Loss: 0.00000718 Testing Loss: 0.00000855 Validation loss 1: 0.00000514\n",
      "Epoch [83300/100000] Training Loss: 0.00006943 Testing Loss: 0.00009754 Validation loss 1: 0.00023019\n",
      "Epoch [83400/100000] Training Loss: 0.00000502 Testing Loss: 0.00000570 Validation loss 1: 0.00000156\n",
      "Epoch [83500/100000] Training Loss: 0.00000531 Testing Loss: 0.00000579 Validation loss 1: 0.00000178\n",
      "Epoch [83600/100000] Training Loss: 0.00000519 Testing Loss: 0.00000582 Validation loss 1: 0.00000191\n",
      "Epoch [83700/100000] Training Loss: 0.00000572 Testing Loss: 0.00000627 Validation loss 1: 0.00000345\n",
      "Epoch [83800/100000] Training Loss: 0.00000648 Testing Loss: 0.00000754 Validation loss 1: 0.00000777\n",
      "Epoch [83900/100000] Training Loss: 0.00000594 Testing Loss: 0.00000672 Validation loss 1: 0.00000463\n",
      "Epoch [84000/100000] Training Loss: 0.00000529 Testing Loss: 0.00000612 Validation loss 1: 0.00000306\n",
      "Epoch [84100/100000] Training Loss: 0.00000512 Testing Loss: 0.00000589 Validation loss 1: 0.00000197\n",
      "Epoch [84200/100000] Training Loss: 0.00000498 Testing Loss: 0.00000566 Validation loss 1: 0.00000192\n",
      "Epoch [84300/100000] Training Loss: 0.00000526 Testing Loss: 0.00000605 Validation loss 1: 0.00000296\n",
      "Epoch [84400/100000] Training Loss: 0.00001411 Testing Loss: 0.00001788 Validation loss 1: 0.00003505\n",
      "Epoch [84500/100000] Training Loss: 0.00003436 Testing Loss: 0.00003467 Validation loss 1: 0.00008273\n",
      "Epoch [84600/100000] Training Loss: 0.00000801 Testing Loss: 0.00000692 Validation loss 1: 0.00000548\n",
      "Epoch [84700/100000] Training Loss: 0.00000694 Testing Loss: 0.00000847 Validation loss 1: 0.00000836\n",
      "Epoch [84800/100000] Training Loss: 0.00000527 Testing Loss: 0.00000598 Validation loss 1: 0.00000281\n",
      "Epoch [84900/100000] Training Loss: 0.00001135 Testing Loss: 0.00001319 Validation loss 1: 0.00001976\n",
      "Epoch [85000/100000] Training Loss: 0.00000598 Testing Loss: 0.00000715 Validation loss 1: 0.00000570\n",
      "Epoch [85100/100000] Training Loss: 0.00000526 Testing Loss: 0.00000583 Validation loss 1: 0.00000215\n",
      "Epoch [85200/100000] Training Loss: 0.00000706 Testing Loss: 0.00000841 Validation loss 1: 0.00001082\n",
      "Epoch [85300/100000] Training Loss: 0.00000863 Testing Loss: 0.00001354 Validation loss 1: 0.00002136\n",
      "Epoch [85400/100000] Training Loss: 0.00001335 Testing Loss: 0.00001182 Validation loss 1: 0.00001533\n",
      "Epoch [85500/100000] Training Loss: 0.00000589 Testing Loss: 0.00000628 Validation loss 1: 0.00000592\n",
      "Epoch [85600/100000] Training Loss: 0.00001129 Testing Loss: 0.00000913 Validation loss 1: 0.00001337\n",
      "Epoch [85700/100000] Training Loss: 0.00000518 Testing Loss: 0.00000589 Validation loss 1: 0.00000221\n",
      "Epoch [85800/100000] Training Loss: 0.00000805 Testing Loss: 0.00000963 Validation loss 1: 0.00001207\n",
      "Epoch [85900/100000] Training Loss: 0.00000549 Testing Loss: 0.00000624 Validation loss 1: 0.00000322\n",
      "Epoch [86000/100000] Training Loss: 0.00000515 Testing Loss: 0.00000572 Validation loss 1: 0.00000263\n",
      "Epoch [86100/100000] Training Loss: 0.00000491 Testing Loss: 0.00000558 Validation loss 1: 0.00000190\n",
      "Epoch [86200/100000] Training Loss: 0.00000953 Testing Loss: 0.00000743 Validation loss 1: 0.00000661\n",
      "Epoch [86300/100000] Training Loss: 0.00000616 Testing Loss: 0.00000713 Validation loss 1: 0.00000570\n",
      "Epoch [86400/100000] Training Loss: 0.00000536 Testing Loss: 0.00000593 Validation loss 1: 0.00000259\n",
      "Epoch [86500/100000] Training Loss: 0.00000494 Testing Loss: 0.00000565 Validation loss 1: 0.00000188\n",
      "Epoch [86600/100000] Training Loss: 0.00000538 Testing Loss: 0.00000614 Validation loss 1: 0.00000278\n",
      "Epoch [86700/100000] Training Loss: 0.00002971 Testing Loss: 0.00002297 Validation loss 1: 0.00005262\n",
      "Epoch [86800/100000] Training Loss: 0.00002893 Testing Loss: 0.00002172 Validation loss 1: 0.00004627\n",
      "Epoch [86900/100000] Training Loss: 0.00000616 Testing Loss: 0.00000586 Validation loss 1: 0.00000196\n",
      "Epoch [87000/100000] Training Loss: 0.00000538 Testing Loss: 0.00000623 Validation loss 1: 0.00000325\n",
      "Epoch [87100/100000] Training Loss: 0.00000922 Testing Loss: 0.00000996 Validation loss 1: 0.00001444\n",
      "Epoch [87200/100000] Training Loss: 0.00000673 Testing Loss: 0.00000797 Validation loss 1: 0.00000909\n",
      "Epoch [87300/100000] Training Loss: 0.00000486 Testing Loss: 0.00000554 Validation loss 1: 0.00000136\n",
      "Epoch [87400/100000] Training Loss: 0.00000522 Testing Loss: 0.00000597 Validation loss 1: 0.00000252\n",
      "Epoch [87500/100000] Training Loss: 0.00000544 Testing Loss: 0.00000590 Validation loss 1: 0.00000241\n",
      "Epoch [87600/100000] Training Loss: 0.00000837 Testing Loss: 0.00001007 Validation loss 1: 0.00001594\n",
      "Epoch [87700/100000] Training Loss: 0.00001760 Testing Loss: 0.00002390 Validation loss 1: 0.00004921\n",
      "Epoch [87800/100000] Training Loss: 0.00000618 Testing Loss: 0.00000644 Validation loss 1: 0.00000353\n",
      "Epoch [87900/100000] Training Loss: 0.00000550 Testing Loss: 0.00000647 Validation loss 1: 0.00000425\n",
      "Epoch [88000/100000] Training Loss: 0.00000533 Testing Loss: 0.00000630 Validation loss 1: 0.00000441\n",
      "Epoch [88100/100000] Training Loss: 0.00000482 Testing Loss: 0.00000548 Validation loss 1: 0.00000136\n",
      "Epoch [88200/100000] Training Loss: 0.00000556 Testing Loss: 0.00000607 Validation loss 1: 0.00000228\n",
      "Epoch [88300/100000] Training Loss: 0.00000488 Testing Loss: 0.00000554 Validation loss 1: 0.00000136\n",
      "Epoch [88400/100000] Training Loss: 0.00000483 Testing Loss: 0.00000550 Validation loss 1: 0.00000132\n",
      "Epoch [88500/100000] Training Loss: 0.00000481 Testing Loss: 0.00000547 Validation loss 1: 0.00000131\n",
      "Epoch [88600/100000] Training Loss: 0.00002415 Testing Loss: 0.00002212 Validation loss 1: 0.00004090\n",
      "Epoch [88700/100000] Training Loss: 0.00001566 Testing Loss: 0.00001229 Validation loss 1: 0.00002526\n",
      "Epoch [88800/100000] Training Loss: 0.00000490 Testing Loss: 0.00000565 Validation loss 1: 0.00000179\n",
      "Epoch [88900/100000] Training Loss: 0.00001056 Testing Loss: 0.00001391 Validation loss 1: 0.00002361\n",
      "Epoch [89000/100000] Training Loss: 0.00000915 Testing Loss: 0.00000978 Validation loss 1: 0.00001462\n",
      "Epoch [89100/100000] Training Loss: 0.00000495 Testing Loss: 0.00000563 Validation loss 1: 0.00000192\n",
      "Epoch [89200/100000] Training Loss: 0.00000564 Testing Loss: 0.00000686 Validation loss 1: 0.00000496\n",
      "Epoch [89300/100000] Training Loss: 0.00001068 Testing Loss: 0.00000880 Validation loss 1: 0.00001221\n",
      "Epoch [89400/100000] Training Loss: 0.00000488 Testing Loss: 0.00000575 Validation loss 1: 0.00000215\n",
      "Epoch [89500/100000] Training Loss: 0.00000571 Testing Loss: 0.00000688 Validation loss 1: 0.00000522\n",
      "Epoch [89600/100000] Training Loss: 0.00000589 Testing Loss: 0.00000847 Validation loss 1: 0.00001073\n",
      "Epoch [89700/100000] Training Loss: 0.00000490 Testing Loss: 0.00000552 Validation loss 1: 0.00000150\n",
      "Epoch [89800/100000] Training Loss: 0.00000481 Testing Loss: 0.00000549 Validation loss 1: 0.00000165\n",
      "Epoch [89900/100000] Training Loss: 0.00000924 Testing Loss: 0.00000860 Validation loss 1: 0.00001005\n",
      "Epoch [90000/100000] Training Loss: 0.00000484 Testing Loss: 0.00000553 Validation loss 1: 0.00000164\n",
      "Epoch [90100/100000] Training Loss: 0.00000547 Testing Loss: 0.00000619 Validation loss 1: 0.00000339\n",
      "Epoch [90200/100000] Training Loss: 0.00000594 Testing Loss: 0.00000702 Validation loss 1: 0.00000513\n",
      "Epoch [90300/100000] Training Loss: 0.00000541 Testing Loss: 0.00000689 Validation loss 1: 0.00000526\n",
      "Epoch [90400/100000] Training Loss: 0.00001761 Testing Loss: 0.00002217 Validation loss 1: 0.00005156\n",
      "Epoch [90500/100000] Training Loss: 0.00000908 Testing Loss: 0.00001165 Validation loss 1: 0.00001660\n",
      "Epoch [90600/100000] Training Loss: 0.00002239 Testing Loss: 0.00003097 Validation loss 1: 0.00009106\n",
      "Epoch [90700/100000] Training Loss: 0.00000541 Testing Loss: 0.00000642 Validation loss 1: 0.00000401\n",
      "Epoch [90800/100000] Training Loss: 0.00000572 Testing Loss: 0.00000622 Validation loss 1: 0.00000285\n",
      "Epoch [90900/100000] Training Loss: 0.00000499 Testing Loss: 0.00000568 Validation loss 1: 0.00000273\n",
      "Epoch [91000/100000] Training Loss: 0.00000503 Testing Loss: 0.00000560 Validation loss 1: 0.00000261\n",
      "Epoch [91100/100000] Training Loss: 0.00001807 Testing Loss: 0.00001806 Validation loss 1: 0.00003754\n",
      "Epoch [91200/100000] Training Loss: 0.00000516 Testing Loss: 0.00000541 Validation loss 1: 0.00000147\n",
      "Epoch [91300/100000] Training Loss: 0.00000475 Testing Loss: 0.00000541 Validation loss 1: 0.00000230\n",
      "Epoch [91400/100000] Training Loss: 0.00002429 Testing Loss: 0.00001540 Validation loss 1: 0.00003664\n",
      "Epoch [91500/100000] Training Loss: 0.00000533 Testing Loss: 0.00000590 Validation loss 1: 0.00000434\n",
      "Epoch [91600/100000] Training Loss: 0.00000493 Testing Loss: 0.00000558 Validation loss 1: 0.00000338\n",
      "Epoch [91700/100000] Training Loss: 0.00000588 Testing Loss: 0.00000699 Validation loss 1: 0.00000711\n",
      "Epoch [91800/100000] Training Loss: 0.00000617 Testing Loss: 0.00000705 Validation loss 1: 0.00001017\n",
      "Epoch [91900/100000] Training Loss: 0.00000472 Testing Loss: 0.00000575 Validation loss 1: 0.00000287\n",
      "Epoch [92000/100000] Training Loss: 0.00000503 Testing Loss: 0.00000557 Validation loss 1: 0.00000167\n",
      "Epoch [92100/100000] Training Loss: 0.00000480 Testing Loss: 0.00000540 Validation loss 1: 0.00000147\n",
      "Epoch [92200/100000] Training Loss: 0.00000473 Testing Loss: 0.00000539 Validation loss 1: 0.00000136\n",
      "Epoch [92300/100000] Training Loss: 0.00013024 Testing Loss: 0.00012603 Validation loss 1: 0.00042026\n",
      "Epoch [92400/100000] Training Loss: 0.00000489 Testing Loss: 0.00000554 Validation loss 1: 0.00000131\n",
      "Epoch [92500/100000] Training Loss: 0.00000475 Testing Loss: 0.00000540 Validation loss 1: 0.00000230\n",
      "Epoch [92600/100000] Training Loss: 0.00000470 Testing Loss: 0.00000535 Validation loss 1: 0.00000327\n",
      "Epoch [92700/100000] Training Loss: 0.00000549 Testing Loss: 0.00000600 Validation loss 1: 0.00000317\n",
      "Epoch [92800/100000] Training Loss: 0.00000487 Testing Loss: 0.00000556 Validation loss 1: 0.00000222\n",
      "Epoch [92900/100000] Training Loss: 0.00001435 Testing Loss: 0.00001796 Validation loss 1: 0.00003361\n",
      "Epoch [93000/100000] Training Loss: 0.00000481 Testing Loss: 0.00000540 Validation loss 1: 0.00000162\n",
      "Epoch [93100/100000] Training Loss: 0.00000567 Testing Loss: 0.00000704 Validation loss 1: 0.00000508\n",
      "Epoch [93200/100000] Training Loss: 0.00000476 Testing Loss: 0.00000551 Validation loss 1: 0.00000197\n",
      "Epoch [93300/100000] Training Loss: 0.00000478 Testing Loss: 0.00000548 Validation loss 1: 0.00000182\n",
      "Epoch [93400/100000] Training Loss: 0.00000777 Testing Loss: 0.00000747 Validation loss 1: 0.00000668\n",
      "Epoch [93500/100000] Training Loss: 0.00000472 Testing Loss: 0.00000544 Validation loss 1: 0.00000183\n",
      "Epoch [93600/100000] Training Loss: 0.00000483 Testing Loss: 0.00000540 Validation loss 1: 0.00000162\n",
      "Epoch [93700/100000] Training Loss: 0.00000564 Testing Loss: 0.00000611 Validation loss 1: 0.00000382\n",
      "Epoch [93800/100000] Training Loss: 0.00000478 Testing Loss: 0.00000609 Validation loss 1: 0.00000439\n",
      "Epoch [93900/100000] Training Loss: 0.00000460 Testing Loss: 0.00000524 Validation loss 1: 0.00000148\n",
      "Epoch [94000/100000] Training Loss: 0.00000463 Testing Loss: 0.00000527 Validation loss 1: 0.00000153\n",
      "Epoch [94100/100000] Training Loss: 0.00000667 Testing Loss: 0.00000643 Validation loss 1: 0.00000465\n",
      "Epoch [94200/100000] Training Loss: 0.00000473 Testing Loss: 0.00000536 Validation loss 1: 0.00000191\n",
      "Epoch [94300/100000] Training Loss: 0.00000464 Testing Loss: 0.00000529 Validation loss 1: 0.00000163\n",
      "Epoch [94400/100000] Training Loss: 0.00000466 Testing Loss: 0.00000533 Validation loss 1: 0.00000172\n",
      "Epoch [94500/100000] Training Loss: 0.00001276 Testing Loss: 0.00001172 Validation loss 1: 0.00002310\n",
      "Epoch [94600/100000] Training Loss: 0.00000505 Testing Loss: 0.00000550 Validation loss 1: 0.00000224\n",
      "Epoch [94700/100000] Training Loss: 0.00001388 Testing Loss: 0.00001662 Validation loss 1: 0.00003504\n",
      "Epoch [94800/100000] Training Loss: 0.00000468 Testing Loss: 0.00000537 Validation loss 1: 0.00000175\n",
      "Epoch [94900/100000] Training Loss: 0.00003220 Testing Loss: 0.00003611 Validation loss 1: 0.00009624\n",
      "Epoch [95000/100000] Training Loss: 0.00000618 Testing Loss: 0.00000668 Validation loss 1: 0.00000493\n",
      "Epoch [95100/100000] Training Loss: 0.00000459 Testing Loss: 0.00000523 Validation loss 1: 0.00000154\n",
      "Epoch [95200/100000] Training Loss: 0.00000471 Testing Loss: 0.00000536 Validation loss 1: 0.00000234\n",
      "Epoch [95300/100000] Training Loss: 0.00000644 Testing Loss: 0.00000790 Validation loss 1: 0.00000755\n",
      "Epoch [95400/100000] Training Loss: 0.00002761 Testing Loss: 0.00002208 Validation loss 1: 0.00004998\n",
      "Epoch [95500/100000] Training Loss: 0.00000491 Testing Loss: 0.00000574 Validation loss 1: 0.00000530\n",
      "Epoch [95600/100000] Training Loss: 0.00000856 Testing Loss: 0.00000927 Validation loss 1: 0.00001378\n",
      "Epoch [95700/100000] Training Loss: 0.00000479 Testing Loss: 0.00000546 Validation loss 1: 0.00000416\n",
      "Epoch [95800/100000] Training Loss: 0.00000472 Testing Loss: 0.00000541 Validation loss 1: 0.00000457\n",
      "Epoch [95900/100000] Training Loss: 0.00001116 Testing Loss: 0.00001447 Validation loss 1: 0.00002516\n",
      "Epoch [96000/100000] Training Loss: 0.00000711 Testing Loss: 0.00000874 Validation loss 1: 0.00001244\n",
      "Epoch [96100/100000] Training Loss: 0.00000527 Testing Loss: 0.00000570 Validation loss 1: 0.00000240\n",
      "Epoch [96200/100000] Training Loss: 0.00000471 Testing Loss: 0.00000535 Validation loss 1: 0.00000183\n",
      "Epoch [96300/100000] Training Loss: 0.00000456 Testing Loss: 0.00000521 Validation loss 1: 0.00000143\n",
      "Epoch [96400/100000] Training Loss: 0.00000842 Testing Loss: 0.00000803 Validation loss 1: 0.00000900\n",
      "Epoch [96500/100000] Training Loss: 0.00000488 Testing Loss: 0.00000690 Validation loss 1: 0.00000694\n",
      "Epoch [96600/100000] Training Loss: 0.00000451 Testing Loss: 0.00000515 Validation loss 1: 0.00000149\n",
      "Epoch [96700/100000] Training Loss: 0.00000499 Testing Loss: 0.00000546 Validation loss 1: 0.00000226\n",
      "Epoch [96800/100000] Training Loss: 0.00000464 Testing Loss: 0.00000532 Validation loss 1: 0.00000176\n",
      "Epoch [96900/100000] Training Loss: 0.00000462 Testing Loss: 0.00000529 Validation loss 1: 0.00000184\n",
      "Epoch [97000/100000] Training Loss: 0.00000802 Testing Loss: 0.00001049 Validation loss 1: 0.00002358\n",
      "Epoch [97100/100000] Training Loss: 0.00000551 Testing Loss: 0.00000647 Validation loss 1: 0.00000535\n",
      "Epoch [97200/100000] Training Loss: 0.00000555 Testing Loss: 0.00000593 Validation loss 1: 0.00000334\n",
      "Epoch [97300/100000] Training Loss: 0.00000460 Testing Loss: 0.00000523 Validation loss 1: 0.00000167\n",
      "Epoch [97400/100000] Training Loss: 0.00000567 Testing Loss: 0.00000649 Validation loss 1: 0.00000511\n",
      "Epoch [97500/100000] Training Loss: 0.00000480 Testing Loss: 0.00000566 Validation loss 1: 0.00000259\n",
      "Epoch [97600/100000] Training Loss: 0.00000533 Testing Loss: 0.00000578 Validation loss 1: 0.00000165\n",
      "Epoch [97700/100000] Training Loss: 0.00000460 Testing Loss: 0.00000524 Validation loss 1: 0.00000137\n",
      "Epoch [97800/100000] Training Loss: 0.00000800 Testing Loss: 0.00001006 Validation loss 1: 0.00001831\n",
      "Epoch [97900/100000] Training Loss: 0.00001213 Testing Loss: 0.00001220 Validation loss 1: 0.00001991\n",
      "Epoch [98000/100000] Training Loss: 0.00000652 Testing Loss: 0.00000813 Validation loss 1: 0.00000917\n",
      "Epoch [98100/100000] Training Loss: 0.00000471 Testing Loss: 0.00000614 Validation loss 1: 0.00000399\n",
      "Epoch [98200/100000] Training Loss: 0.00000482 Testing Loss: 0.00000542 Validation loss 1: 0.00000220\n",
      "Epoch [98300/100000] Training Loss: 0.00000451 Testing Loss: 0.00000514 Validation loss 1: 0.00000156\n",
      "Epoch [98400/100000] Training Loss: 0.00001072 Testing Loss: 0.00001179 Validation loss 1: 0.00001821\n",
      "Epoch [98500/100000] Training Loss: 0.00000503 Testing Loss: 0.00000543 Validation loss 1: 0.00000201\n",
      "Epoch [98600/100000] Training Loss: 0.00000800 Testing Loss: 0.00000599 Validation loss 1: 0.00000317\n",
      "Epoch [98700/100000] Training Loss: 0.00000447 Testing Loss: 0.00000510 Validation loss 1: 0.00000261\n",
      "Epoch [98800/100000] Training Loss: 0.00000443 Testing Loss: 0.00000507 Validation loss 1: 0.00000320\n",
      "Epoch [98900/100000] Training Loss: 0.00000443 Testing Loss: 0.00000507 Validation loss 1: 0.00000419\n",
      "Epoch [99000/100000] Training Loss: 0.00001409 Testing Loss: 0.00001594 Validation loss 1: 0.00003504\n",
      "Epoch [99100/100000] Training Loss: 0.00002037 Testing Loss: 0.00002218 Validation loss 1: 0.00005070\n",
      "Epoch [99200/100000] Training Loss: 0.00000683 Testing Loss: 0.00000720 Validation loss 1: 0.00000997\n",
      "Epoch [99300/100000] Training Loss: 0.00000511 Testing Loss: 0.00000574 Validation loss 1: 0.00000971\n",
      "Epoch [99400/100000] Training Loss: 0.00000505 Testing Loss: 0.00000579 Validation loss 1: 0.00001025\n",
      "Epoch [99500/100000] Training Loss: 0.00001498 Testing Loss: 0.00001302 Validation loss 1: 0.00003228\n",
      "Epoch [99600/100000] Training Loss: 0.00000494 Testing Loss: 0.00000547 Validation loss 1: 0.00000836\n",
      "Epoch [99700/100000] Training Loss: 0.00000488 Testing Loss: 0.00000548 Validation loss 1: 0.00000310\n",
      "Epoch [99800/100000] Training Loss: 0.00000674 Testing Loss: 0.00000809 Validation loss 1: 0.00001177\n",
      "Epoch [99900/100000] Training Loss: 0.00000583 Testing Loss: 0.00001044 Validation loss 1: 0.00001150\n",
      "Epoch [100000/100000] Training Loss: 0.00000439 Testing Loss: 0.00000502 Validation loss 1: 0.00000295\n"
     ]
    }
   ],
   "source": [
    "net = GreenFun(N, N).to(device)\n",
    "# net = MyNet(N, N).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 3000, factor=0.7, verbose=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 50000, verbose=False, delta=1e-8, path='net.pth')\n",
    "num_epochs = 100000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = net(train_f, x, train_tau)\n",
    "    # outputs = net(train_f, train_tau)\n",
    "    \n",
    "    # print(outputs.shape)\n",
    "    loss = criterion(outputs, train_u)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_test = net(test_f, x, test_tau)\n",
    "        # outputs_test = net(test_f, test_tau)\n",
    "        loss_test = criterion(outputs_test, test_u)\n",
    "\n",
    "        if(epoch+1) % 100 == 0:\n",
    "            outputs_validation1 = net(f_validation1, x, tau_validation1)\n",
    "            # outputs_validation1 = net(f_validation1, tau_validation1)\n",
    "            loss_validation1 = criterion(outputs_validation1, U_validation1)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {loss.item():.8f} Testing Loss: {loss_test.item():.8f} Validation loss 1: {loss_validation1.item():.8f}\")\n",
    "\n",
    "    # 调整学习率\n",
    "    scheduler.step(loss_test)\n",
    "\n",
    "    early_stopping(loss_test, net)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau       L2 Norm        Max Norm       Relative L2 Norm    Relative Max Norm\n",
      "0.007     0.00208357     0.00078459     0.00479523          0.00586249\n",
      "0.009     0.00086308     0.00048357     0.00198634          0.00361322\n",
      "0.011     0.00063051     0.00036028     0.00145109          0.00269203\n",
      "0.013     0.00059811     0.00035051     0.00137652          0.00261904\n",
      "0.015     0.0005989      0.00033002     0.00137833          0.00246595\n",
      "0.017     0.00061908     0.00035928     0.00142479          0.00268457\n",
      "0.019     0.00060872     0.00034426     0.00140095          0.00257234\n",
      "0.021     0.00060075     0.00034051     0.0013826           0.0025443\n",
      "0.023     0.00060739     0.00035462     0.00139787          0.00264972\n",
      "0.025     0.00067375     0.00037437     0.00155061          0.0027973\n",
      "0.027     0.00059618     0.00035201     0.00137207          0.00263023\n",
      "0.029     0.0006326      0.00036373     0.0014559           0.0027178\n",
      "0.031     0.0013571      0.0006491      0.00312329          0.00485011\n",
      "--------------------------------------------------------------------------------\n",
      "0.033     0.02229726     0.00707619     0.05131616          0.05287355\n",
      "0.035     0.03351096     0.01043695     0.07712399          0.0779853\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net.pth\", map_location = device))\n",
    "\n",
    "tau_validation4 = np.array([0.007, 0.009, 0.011, 0.013, 0.015, 0.017, 0.019, 0.021, 0.023, 0.025, 0.027, 0.029, 0.031, 0.033, 0.035])\n",
    "lll = len(tau_validation4)\n",
    "f_validation4 = np.zeros((lll, N, N))\n",
    "U_validation4 = np.zeros((lll, N, N))\n",
    "\n",
    "for k in range(lll):\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            xx = 1 * j / (N - 1)\n",
    "            yy = 1 * i / (N - 1)\n",
    "\n",
    "            t = tau_validation4[k]\n",
    "        \n",
    "            U_validation4[k, i, j] = xx * (1- xx) * yy * (1- yy) * np.exp(0.6 * xx + 0.8 * yy)\n",
    "            f_validation4[k, i, j] = np.exp(0.6 * xx + 0.8 * yy) * (xx**2*yy**2 + 2.2*xx**2*yy + 1.4*xx*yy**2 + 0.4*xx**2 + 0.8*yy**2 - 4.6*xx*yy - 0.4*xx - 0.8*yy)\n",
    "            f_validation4[k, i, j] = U_validation4[k, i, j] - t * f_validation4[k, i, j]\n",
    "            \n",
    "\n",
    "U_validation4 = torch.tensor(U_validation4, dtype=torch.float32).to(device)\n",
    "f_validation4 = torch.tensor(f_validation4, dtype=torch.float32).to(device)\n",
    "tau_validation4 = torch.tensor(tau_validation4, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "outputs_validation4 = net(f_validation4, x, tau_validation4)\n",
    "error = torch.abs(U_validation4 - outputs_validation4)\n",
    "\n",
    "# 初始化存储范数的列表\n",
    "norms = []\n",
    "\n",
    "# 计算每一行的二范数和最大范数\n",
    "for i in range(error.shape[0]):\n",
    "    row = error[i, :]\n",
    "    l2_norm = torch.norm(row, p=2).item() / np.sqrt(len(row))\n",
    "    max_norm = torch.norm(row, p=float('inf')).item()\n",
    "    l2_norm_relative = torch.norm(row, p=2).item() / torch.norm(U_validation4[i, :], p=2).item()\n",
    "    max_norm_relative = torch.norm(row, p=float('inf')).item() / torch.norm(U_validation4[i, :], p=float('inf')).item()\n",
    "    norms.append([round(tau_validation4[i].cpu().item(), 4), round(l2_norm, 8), round(max_norm, 8), round(l2_norm_relative, 8), round(max_norm_relative, 8)])\n",
    "\n",
    "# 打印表格\n",
    "flag = 0\n",
    "print(f\"{'tau':<10}{'L2 Norm':<15}{'Max Norm':<15}{'Relative L2 Norm':<20}{'Relative Max Norm'}\")\n",
    "for row in norms:\n",
    "    if row[0] > 1/32 and flag == 0:\n",
    "        print(\"--\"*40)\n",
    "        flag = 1\n",
    "    print(f\"{row[0]:<10}{row[1]:<15}{row[2]:<15}{row[3]:<20}{row[4]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyc_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
