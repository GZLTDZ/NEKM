{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is used to learnig the solution operator for the boundary integral equations (from g to h) for PDEs:\n",
    "$$ \\begin{cases}\n",
    "u - \\tau \\Delta u = 0, & x \\in \\Omega, \\\\\n",
    "u = g, & x \\in \\partial \\Omega,\n",
    "\\end{cases} $$\n",
    "where $\\Omega$ is a petal-shaped domain and parameter $\\tau \\in [0.05, 0.1]$.\n",
    "\n",
    "Note that it is equivalent to solve $\\frac{1}{\\tau} u - \\Delta u = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a fixed $g$, the solution of this PDE can be written as\n",
    "$$ u(x) = \\int_{\\partial \\Omega} \\frac{\\partial G_0(x,y)}{\\partial n_y} h(y) d s_y,\\quad x \\in \\Omega, $$\n",
    "where the density function $h$ is the solution of this boundary integral equation:\n",
    "$$ \\int_{\\partial \\Omega} \\frac{\\partial G_0(x_0,y)}{\\partial n_y} h(y) d s_y + \\frac{1}{2} h(x_0) = g(x_0),\\quad x_0 \\in \\partial \\Omega, $$\n",
    "with the fundamental solution (of $\\frac{1}{\\tau} id - \\Delta$):\n",
    "$$G_0(x, y) = -\\frac{1}{2 \\pi} \\mathbb{K}_0(\\frac{|x - y|}{\\sqrt{\\tau}}), $$\n",
    "here $\\mathbb{K}_{\\gamma}(x)$ is the modified Bessel function of the second kind of order $\\gamma$.\n",
    "\n",
    "Remark that $$\\mathbb{K}_{0}(x) \\approx - \\ln(\\frac{x}{2}) - \\gamma_E + (1 - \\gamma_E)\\frac{x^2}{4}$$ and $$\\mathbb{K}'_{0}(x) \\approx - \\frac{1}{x} + (1-\\gamma_E)\\frac{x}{2}$$ near $x = 0.$\n",
    "\n",
    "Suppose that the domain boundary $\\partial \\Omega$ has a parametric representation: $\\textbf{x} = x(t) = (x_1(t), x_2(t))\\ \\text{for}\\ t \\in [0, 4)$. The boundary integral equation above now reads:\n",
    "$$\n",
    "\\frac{1}{2} h(t)+\\frac{1}{2 \\pi} \\int_0^{4} \\frac{\\partial (-\\mathbb{K}_0(\\frac{|\\textbf{x}(t) - \\textbf{x}(s)|)}{\\sqrt{\\tau}})}{\\partial \\mathbf{n}_s} h(s)\\left|\\mathbf{x}^{\\prime}(s)\\right| d s=g(t) \\quad \\text { for } t \\in[0, 4)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\frac{1}{2} h(t)+\\frac{1}{2 \\pi} \\int_0^{4} k(t, s) h(s) d s=g(t) \\quad \\text { for } t \\in[0, 4)\n",
    "$$\n",
    "with kernel\n",
    "$$\n",
    "k(t, s)=\\left\\{\\begin{array}{ll}\n",
    "- \\mathbb{K}'_0(\\frac{|\\textbf{x}(t) - \\textbf{x}(s)|}{\\sqrt{\\tau}}) \\frac{x'_2(s) (x_1(s) - x_1(t)) - x'_1(s) (x_2(s) - x_2(t))}{\\sqrt{\\tau [(x_1(t\n",
    ") - x_1(s))^2 + (x_2(t) - x_2(s))^2]}} & \\text { if } s \\neq t \\\\\n",
    "\\frac{1}{2} \\frac{x''_2(t) x'_1(t) - x''_1(t) x'_2(t)}{x'_1(t)^2 + x'_2(t)^2} & \\text { if } s=t\n",
    "\\end{array} .\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thus we can construct a network $\\mathcal{NN}$ with parameters $\\Theta$ from $g$ to $h$ which can be regarded as the solution of the corresponding boundary integral equation. We just need to sample functions $g$ from the proper space to get a dataset $\\{g_i\\}_{i=1}^{M}$, and the $\\text{Loss}$ funciton can be setted as:\n",
    "$$\\text{Loss}(\\Theta) = \\frac{1}{M_{\\text{train}}} \\sum_{i = 1}^{M_{\\text{train}}} \\sum_{j = 1}^N \\{[\\int_{\\partial \\Omega} \\frac{\\partial G_0(x_j,y)}{\\partial n_y} \\mathcal{NN}(\\Theta; y) d s_y + \\frac{1}{2} \\mathcal{NN}(\\Theta; x_j)] - g_i(x_j)\\}^2,$$\n",
    "where $\\{x_j\\}_{j=1}^N$ are sample points on the boundary $\\partial \\Omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU memory: 24210.31 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu_info.name}\")\n",
    "    print(f\"GPU memory: {gpu_info.total_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output, n_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.layin = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.hidden_layers = torch.nn.ModuleList([torch.nn.Linear(n_hidden, n_hidden) for _ in range(n_layers)])\n",
    "        self.layout = torch.nn.Linear(n_hidden, n_output)\n",
    "    def forward(self, g):\n",
    "       g = self.layin(g)\n",
    "       g = self.act(g)\n",
    "       for layer in self.hidden_layers:\n",
    "           g = layer(g)\n",
    "           g = self.act(g)\n",
    "       h = self.layout(g)\n",
    "       return h\n",
    "    \n",
    "class DeepONet(torch.nn.Module):\n",
    "    def __init__(self, n_g, n_h, n_hidden, n_layers):\n",
    "        super(DeepONet, self).__init__()\n",
    "        self.for_tau = MLP(1, n_g, n_hidden, n_layers)\n",
    "        # self.for_g_1 = torch.nn.Sequential(torch.nn.Linear(n_g, n_hidden), torch.nn.Linear(n_hidden, n_hidden), torch.nn.Linear(n_hidden, n_hidden))\n",
    "        # self.for_g_2 = torch.nn.Sequential(torch.nn.Linear(n_hidden, n_h), torch.nn.Linear(n_h, n_h), torch.nn.Linear(n_h, n_h))\n",
    "        self.for_g_1 = torch.nn.Linear(n_g, n_hidden)\n",
    "        self.for_g_2 = torch.nn.Linear(n_hidden, n_h)\n",
    "    def forward(self, tau, g):      #  tau: scalar, g: (batch_size, n_g)\n",
    "       tau = tau + torch.zeros_like(g[:, 1].view(-1, 1))\n",
    "       tau = self.for_tau(tau)\n",
    "       g = self.for_g_1(g)\n",
    "       h = self.for_g_2(tau * g)\n",
    "       return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flower_points(n, plot):\n",
    "    theta = np.linspace(0, 2 * np.pi, 8 * n, endpoint=False)  # 均匀分布的角度\n",
    "    r = 0.6 * (1 + 0.25 * np.sin(6 * theta))  # 计算极径\n",
    "    x_boundary = r * np.cos(theta)\n",
    "    y_boundary = r * np.sin(theta)\n",
    "    boundary_points = np.column_stack((x_boundary, y_boundary))\n",
    "    \n",
    "    t_boundary = np.zeros((len(boundary_points), 1))\n",
    "    for i in range(0, len(boundary_points)):\n",
    "        t_boundary[i] = theta[i]\n",
    "    \n",
    "    x_vals = np.linspace(-1, 1, n + 1)\n",
    "    y_vals = np.linspace(-1, 1, n + 1)\n",
    "    xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "    interior_candidates = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "    \n",
    "    r_candidate = np.sqrt(interior_candidates[:, 0]**2 + interior_candidates[:, 1]**2)\n",
    "    theta_candidate = np.arctan2(interior_candidates[:, 1], interior_candidates[:, 0])\n",
    "    r_max = 0.6 * (1 + 0.25 * np.sin(6 * theta_candidate))\n",
    "    interior_points = interior_candidates[r_candidate <= 0.975 * r_max]\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(interior_points[:, 0], interior_points[:, 1], color='blue', label='Interior Points', s=10)\n",
    "        plt.scatter(boundary_points[:, 0], boundary_points[:, 1], color='red', label='Boundary Points', s=1)\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Flower-Shaped Region Points\")\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.show()\n",
    "    \n",
    "    return (2 * np.pi) / (8 * n), t_boundary, boundary_points, interior_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05  , 0.0525, 0.055 , 0.0575, 0.06  , 0.0625, 0.065 , 0.0675,\n",
       "       0.07  , 0.0725, 0.075 , 0.0775, 0.08  , 0.0825, 0.085 , 0.0875,\n",
       "       0.09  , 0.0925, 0.095 , 0.0975, 0.1   ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taus = np.linspace(0.05, 0.1, 21)\n",
    "N = 64\n",
    "taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAIjCAYAAADC/VtFAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdctJREFUeJzt3Xl8TOf+B/DPyEpkMSSCRmLftyKpoGj12qpoe6MooYooutAlfmprq9JWb/WiSbUXrVatV9urqle1bhFSVdQSexSt2EIElZA8vz/GjJnJ7Nt5Zubzfr3mleTkMzPnzExmnjzne75HJYQQICIiIpJUBaVXgIiIiMgSDlaIiIhIahysEBERkdQ4WCEiIiKpcbBCREREUuNghYiIiKTGwQoRERFJjYMVIiIikhoHK0RERCQ1DlaI7khISMDw4cOVXg2POXnyJFQqFebMmaP0qjika9eu6Nq1q9KrYdHmzZuhUqmwefNmpVfFKdrXypIlS5ReFfJTHKyQz1uyZAlUKpXJS3p6utKr5xZbt25Fr169UKtWLYSGhqJ27dro27cvli1bpvSqeVxCQoLBcx4WFobExER8+umnSq+aW3Xt2tVgu9VqNdq3b49FixahrKzMY+tx48YNzJgxw+sHbKSsQKVXgMhTXnvtNdSpU8dgWfPmzRVaG/dZtWoVBg4ciNatW+O5555DlSpVkJeXh59++gkfffQRBg8erPQqelzr1q0xadIkAMDZs2fx8ccfIzU1FcXFxRg1apTb7vf+++/HX3/9heDgYLfdhyX33HMPZs+eDQC4cOECPv30U4wcORJHjhxBRkaGzbcTHx+Pv/76C0FBQXavw40bNzBz5kwAkH4mjOTFwQr5jV69eqFdu3ZKr4ZLXL9+HWFhYSZ/N2PGDDRt2hQ7duwo9yF5/vx5T6yedGrVqoUnn3xS9/Pw4cNRt25dvPfee24drFSoUAGhoaFuu31rIiMjDbZ7zJgxaNSoEebPn4/XX3/d5sGHSqVSdDuIuBuIyIITJ07g73//O9RqNSpVqoT77rsP33zzje73QghUq1YNEydO1C0rKytDVFQUAgICcOXKFd3yt956C4GBgbh27Zpu2aFDh/D4449DrVYjNDQU7dq1w9dff22wDtrdWP/73//wzDPPICYmBvfcc4/ZdT5+/Djat29v8r/5mJgYk9dZuHAh6tWrh5CQELRv3x47d+40+P1vv/2m+4APDQ1FbGwsnnrqKVy6dMkgN2PGDKhUKhw6dAgpKSmIiIhA1apV8dxzz+HmzZvl7vezzz5D27ZtUbFiRajVajzxxBM4ffq02fWrWLEiEhMTsWXLFrPbb4vo6Gg0btwYx48fN1heVlaGuXPnolmzZggNDUX16tUxZswYXL58uVxuxowZqFmzJipVqoRu3brh4MGD5eqezNWsrFq1Srfd1apVw5NPPok//vjDIDN8+HBUrlwZf/zxB/r374/KlSsjOjoaL774IkpLSx3abu1r+Pr167hw4QIA669xwHTNii3rd/LkSURHRwMAZs6cqdslNWPGDABAfn4+RowYgXvuuQchISGoUaMG+vXrh5MnTzq0feS7OLNCfqOwsBAXL140WFatWjWz+XPnziE5ORk3btzAs88+i6pVq+KTTz7BI488gtWrV2PAgAFQqVTo2LEjfvrpJ931fvvtNxQWFqJChQrYtm0b+vTpAwDYsmUL2rRpg8qVKwMADhw4gI4dO6JWrVpIT09HWFgYVq5cif79+2PNmjUYMGCAwfo888wziI6OxrRp03D9+nWz6x0fH49NmzbhzJkzFgc1WsuWLUNRURHGjBkDlUqFt99+G48++ihOnDih+89748aNOHHiBEaMGIHY2FgcOHAACxcuxIEDB7Bjxw6oVCqD20xJSUFCQgJmz56NHTt24J///CcuX75sUCcya9YsTJ06FSkpKXj66adx4cIFzJs3D/fffz92796NqKgoAMC//vUvjBkzBsnJyXj++edx4sQJPPLII1Cr1YiLi7O6fabcvn0bZ86cQZUqVQyWjxkzBkuWLMGIESPw7LPPIi8vD/Pnz8fu3buxbds23eMxefJkvP322+jbty969OiBvXv3okePHiYHZMa0t9++fXvMnj0b586dw/vvv49t27YZbDcAlJaWokePHkhKSsKcOXPw/fff491330W9evUwduxYh7b9xIkTCAgIQFRUlE2vcUusrV90dDQyMzMxduxYDBgwAI8++igAoGXLlgCAxx57DAcOHMCECROQkJCA8+fPY+PGjTh16hQSEhIc2j7yUYLIxy1evFgAMHnRFx8fL1JTU3U/P//88wKA2LJli25ZUVGRqFOnjkhISBClpaVCCCHeeecdERAQIK5evSqEEOKf//yniI+PF4mJieKVV14RQghRWloqoqKixAsvvKC7rQcffFC0aNFC3Lx5U7esrKxMJCcniwYNGpRb/06dOonbt29b3d5//etfAoAIDg4W3bp1E1OnThVbtmzRra9WXl6eACCqVq0qCgoKdMu/+uorAUD85z//0S27ceNGufv54osvBADx008/6ZZNnz5dABCPPPKIQfaZZ54RAMTevXuFEEKcPHlSBAQEiFmzZhnk9u3bJwIDA3XLS0pKRExMjGjdurUoLi7W5RYuXCgAiC5dulh9POLj48Xf/vY3ceHCBXHhwgWxb98+MXToUAFAjBs3TpfbsmWLACA+//xzg+tv2LDBYHl+fr4IDAwU/fv3N8jNmDFDADB4Df34448CgPjxxx8Ntqd58+bir7/+0uXWrVsnAIhp06bplqWmpgoA4rXXXjO4nzZt2oi2bdta3e4uXbqIxo0b67Y7NzdXPPvsswKA6Nu3rxDC9te49rWyePFiu9fvwoULAoCYPn26Qe7y5csCgHjnnXesbgsRdwOR31iwYAE2btxocLFk/fr1SExMRKdOnXTLKleujNGjR+PkyZM4ePAgAKBz584oLS1FdnY2AM0MSufOndG5c2fd7or9+/fjypUr6Ny5MwCgoKAAP/zwA1JSUlBUVISLFy/i4sWLuHTpEnr06IGjR4+W2y0watQoBAQEWN3Op556Chs2bEDXrl2xdetWvP766+jcuTMaNGigW0d9AwcONJhh0K7jiRMndMsqVqyo+/7mzZu4ePEi7rvvPgDAr7/+Wu42x40bZ/DzhAkTAGgeUwD497//jbKyMqSkpOi2/eLFi4iNjUWDBg3w448/AgB++eUXnD9/HmlpaQa7tYYPH47IyEirj4XWf//7X0RHRyM6OhotWrTA0qVLMWLECLzzzju6zKpVqxAZGYmHHnrIYJ3atm2LypUr69Zp06ZNuH37Np555hmT22iJdnueeeYZgxqQPn36oHHjxuV2vwBAWlqawc+dO3c2eG4sOXTokG67mzRpgnnz5qFPnz5YtGgRANtf45Y4un4VK1ZEcHAwNm/eXG43G5Ex7gYiv5GYmGhXge3vv/+OpKSkcsubNGmi+33z5s1x7733olKlStiyZQt69OiBLVu2YObMmYiNjcW8efNw8+ZN3aBF+6Fw7NgxCCEwdepUTJ061eT9nz9/HrVq1dL9rH8kU0lJCQoKCgzy0dHRusFMjx490KNHD9y4cQO7du3CihUrkJWVhYcffhiHDh0yqF2pXbu2we1oBy76HyAFBQWYOXMmli9fXq5It7CwsNy6N2jQwODnevXqoUKFCrpahKNHj0IIUS6npd3d8vvvv5u8vaCgINStW9fkdU1JSkrCG2+8gdLSUuzfvx9vvPEGLl++bDAAOnr0KAoLC83W9Wi3W7tO9evXN/i9Wq0ut1vJmPa6jRo1Kve7xo0bY+vWrQbLQkNDdTUfWlWqVLH5wz0hIQEfffSRrkC2QYMGBttn62vcHGfWLyQkBG+99RYmTZqE6tWr47777sPDDz+MYcOGITY21qbtI//BwQqRk4KCgpCUlISffvoJx44dQ35+Pjp37ozq1avj1q1byMnJwZYtW9C4cWPdG7u2z8WLL76IHj16mLxd4w9D/dmN7OxsdOvWzeD3eXl55fbzV6pUSTfLU61aNcycORPffvstUlNTdRlzszVCCN33KSkpyM7OxksvvYTWrVujcuXKKCsrQ8+ePW3q2WFc01JWVgaVSoVvv/3W5P1r63pcpVq1aujevTsAzUCucePGePjhh/H+++/riqPLysoQExODzz//3ORtGH8oe4ItM2mWhIWF6bbbHZxdv+effx59+/bFl19+ie+++w5Tp07F7Nmz8cMPP6BNmzYuWkvyBRysEJkRHx+Pw4cPl1t+6NAh3e+1OnfujLfeegvff/89qlWrhsaNG0OlUqFZs2bYsmULtmzZgocffliX184KBAUFOfRh0qpVq3K7saz9N6qdVTp79qxd93X58mVs2rQJM2fOxLRp03TLjx49avY6R48eNZgJOnbsGMrKynSDqXr16kEIgTp16qBhw4Zmb0f7GB89ehQPPPCAbvmtW7eQl5eHVq1a2bUtWn369EGXLl3w5ptvYsyYMQgLC0O9evXw/fffo2PHjgYDQ3PrdOzYMYNtvHTpktUZBe11Dx8+bLA92mX6rylPsOc17ijjgaqxevXqYdKkSZg0aRKOHj2K1q1b491338Vnn33m9H2T72DNCpEZvXv3xs8//4zt27frll2/fh0LFy5EQkICmjZtqlveuXNnFBcXY+7cuejUqZPuDbpz585YunQp/vzzT10tCKA5hLhr16748MMPTQ4etIeVmlOlShV0797d4KKtgdi0aZPJ62jrRUztgrBE+9+z/kwLAMydO9fsdRYsWGDw87x58wBoet0AwKOPPoqAgADMnDmz3O0KIXSHRLdr1w7R0dHIyspCSUmJLrNkyRKDw8Id8corr+DSpUv46KOPAGhmj0pLS/H666+Xy96+fVt3fw8++CACAwORmZlpkJk/f77V+2zXrh1iYmKQlZWF4uJi3fJvv/0Wubm5uiPHPMWe17ijKlWqBADlnq8bN26UO3qqXr16CA8PN3hsiADOrBCZlZ6eji+++AK9evXCs88+C7VajU8++QR5eXlYs2YNKlS4O9bv0KEDAgMDcfjwYYwePVq3/P7779d9qOkPVgDNB3qnTp3QokULjBo1CnXr1sW5c+ewfft2nDlzBnv37nVovfv164c6deqgb9++qFevHq5fv47vv/8e//nPf9C+fXv07dvXrtuLiIjA/fffj7fffhu3bt1CrVq18N///hd5eXlmr5OXl4dHHnkEPXv2xPbt2/HZZ59h8ODBupmQevXq4Y033sDkyZNx8uRJ9O/fH+Hh4cjLy8PatWsxevRovPjiiwgKCsIbb7yBMWPG4IEHHsDAgQORl5eHxYsX21WzYkqvXr3QvHlz/OMf/8C4cePQpUsXjBkzBrNnz8aePXvwt7/9DUFBQTh69ChWrVqF999/H48//jiqV6+O5557Du+++65uG/fu3Ytvv/0W1apVsziTEBQUhLfeegsjRoxAly5dMGjQIN2hywkJCXjhhRec2iZ72fMad1TFihXRtGlTrFixAg0bNoRarUbz5s1x+/ZtPPjgg0hJSUHTpk0RGBiItWvX4ty5c3jiiSdcsHXkUxQ8EonII7SH/u7cudNizvjQZSGEOH78uHj88cdFVFSUCA0NFYmJiWLdunUmr9++fXsBQOTk5OiWnTlzRgAQcXFxJq9z/PhxMWzYMBEbGyuCgoJErVq1xMMPPyxWr15t9/prffHFF+KJJ54Q9erVExUrVhShoaGiadOmYsqUKbrDq4W4eziqqUNHYXSo6ZkzZ8SAAQNEVFSUiIyMFH//+9/Fn3/+WS6nPXT54MGD4vHHHxfh4eGiSpUqYvz48QaH6mqtWbNGdOrUSYSFhYmwsDDRuHFjMW7cOHH48GGD3AcffCDq1KkjQkJCRLt27cRPP/0kunTpYvOhy3369DH5uyVLlpQ7JHfhwoWibdu2omLFiiI8PFy0aNFCvPzyy+LPP//UZW7fvi2mTp0qYmNjRcWKFcUDDzwgcnNzRdWqVUVaWpouZ3zostaKFStEmzZtREhIiFCr1WLIkCHizJkzBpnU1FQRFhZWbp21j7E1Xbp0Ec2aNbOas+U1bu7QZVvXLzs7W7Rt21YEBwfrXjMXL14U48aNE40bNxZhYWEiMjJSJCUliZUrV1pdZ/I/KiGM5mCJiBw0Y8YMzJw5ExcuXLDYcM8XXblyBVWqVMEbb7yBKVOmKL06RD6FNStERHb666+/yi3T1vDwZH1ErseaFSIiO61YsQJLlixB7969UblyZWzduhVffPEF/va3v6Fjx45Krx6Rz+FghYjITi1btkRgYCDefvttXL16VVd0+8Ybbyi9akQ+yetqVhYsWIB33nkH+fn5aNWqFebNm4fExESz+blz5yIzMxOnTp1CtWrV8Pjjj2P27Nk83TkREZGX8KqalRUrVmDixImYPn06fv31V7Rq1Qo9evQo1/5ba9myZUhPT8f06dORm5uLf/3rX1ixYgX+7//+z8NrTkRERI7yqpmVpKQktG/fXtd8qaysDHFxcZgwYQLS09PL5cePH4/c3FyDJlmTJk1CTk5OuXNwEBERkZy8pmalpKQEu3btwuTJk3XLKlSogO7duxt0X9SXnJyMzz77DD///DMSExNx4sQJrF+/HkOHDjV7P8XFxQbdE8vKylBQUICqVatabRtNRETk74QQKCoqQs2aNV3SWBDwosHKxYsXUVpaiurVqxssr169uu48FsYGDx6MixcvolOnThBC4Pbt20hLS7O4G2j27NmYOXOmS9ediIjI35w+fRr33HOPS27LawYrjti8eTPefPNNfPDBB0hKSsKxY8fw3HPP4fXXX8fUqVNNXmfy5Mm6s7ACQGFhIWrXro3Tp08jIiLCU6tORETkla5evYq4uDiEh4e77Da9ZrBSrVo1BAQE4Ny5cwbLz507Z/Zss1OnTsXQoUPx9NNPAwBatGiB69evY/To0ZgyZYrJ6amQkBCEhISUWx4REcHBChERkY1cWTrhNUcDBQcHo23btgbFsmVlZdi0aRM6dOhg8jo3btwoNyAxdwZZIiIikpPXzKwAwMSJE5Gamop27dohMTERc+fOxfXr1zFixAgAwLBhw1CrVi3Mnj0bANC3b1/84x//QJs2bXS7gaZOnYq+ffvqBi1EREQkN68arAwcOBAXLlzAtGnTkJ+fj9atW2PDhg26ottTp04ZzKS8+uqrUKlUePXVV/HHH38gOjoaffv2xaxZs5TaBCIiIrKTV/VZUcLVq1cRGRmJwsJCszUr2iONSktLPbx2RJpdm4GBgTy0noikYMvnpr28amZFRiUlJTh79ixu3Lih9KqQH6tUqRJq1KiB4OBgpVeFiMjlOFhxQllZGfLy8hAQEICaNWsiODiY/92SRwkhUFJSggsXLiAvLw8NGjRwWRMmIiJZcLDihJKSEl3L/0qVKim9OuSnKlasiKCgIPz+++8oKSnhSTqJyOfwXzAX4H+ypDS+BonIl/EdjoiIiKTGwQoRERFJjYMV8rjNmzdDpVLhypUrSq9KOUuWLEFUVJTSq0FERHo4WPFDw4cPR//+/e26jkqlwpdffumS+09OTsbZs2cRGRnpktvTN3z4cKhUKqhUKgQHB6N+/fp47bXXcPv2bZuuP3DgQBw5csSu++zatSuef/55B9aWiIhswaOByKNu3bqF4OBgsyeftFVJSYnZniI9e/bE4sWLUVxcjPXr12PcuHEICgrC5MmTrd5uxYoVUbFiRafWjYiIXIszK4SuXbvi2Wefxcsvvwy1Wo3Y2FjMmDFD9/uEhAQAwIABA6BSqXQ/A8BXX32Fe++9F6Ghoahbty5mzpxpMIuhUqmQmZmJRx55BGFhYZg1a5bJ3UBr1qxBs2bNEBISgoSEBLz77rsG65iQkIDXX38dw4YNQ0REBEaPHm12e0JCQhAbG4v4+HiMHTsW3bt3x9dffw0AuHz5MoYNG4YqVaqgUqVK6NWrF44ePaq7rvFuoBkzZqB169ZYunQpEhISEBkZiSeeeAJFRUUANDM5//vf//D+++/rZnROnjyJy5cvY8iQIYiOjkbFihXRoEEDLF682NanhIiI9HCwIomcHGDpUs1XJXzyyScICwtDTk4O3n77bbz22mvYuHEjAGDnzp0AgMWLF+Ps2bO6n7ds2YJhw4bhueeew8GDB/Hhhx9iyZIl5c69NGPGDAwYMAD79u3DU089Ve6+d+3ahZSUFDzxxBPYt28fZsyYgalTp2LJkiUGuTlz5qBVq1bYvXs3pk6davO2VaxYESUlJQA0g4tffvkFX3/9NbZv3w4hBHr37o1bt26Zvf7x48fx5ZdfYt26dVi3bh3+97//ISMjAwDw/vvvo0OHDhg1ahTOnj2Ls2fPIi4uDlOnTsXBgwfx7bffIjc3F5mZmahWrZrN60xERHoEWVRYWCgAiMLCwnK/++uvv8TBgwfFX3/95dR9vPyyEMDdy8svO3VzVqWmpop+/frpfu7SpYvo1KmTQaZ9+/bilVde0f0MQKxdu9Yg8+CDD4o333zTYNnSpUtFjRo1DK73/PPPG2R+/PFHAUBcvnxZCCHE4MGDxUMPPWSQeemll0TTpk11P8fHx4v+/fvbtW1lZWVi48aNIiQkRLz44oviyJEjAoDYtm2bLn/x4kVRsWJFsXLlSiGEEIsXLxaRkZG630+fPl1UqlRJXL161WDdkpKSdD936dJFPPfccwbr0bdvXzFixAir6+sqrnotEhE5y9LnpqM4s6KwnBzg7bcNl739tudnWFq2bGnwc40aNXD+/HmL19m7dy9ee+01VK5cWXfRzjDonyupXbt2Fm8nNzcXHTt2NFjWsWNHHD161ODkkNZuR2vdunWoXLkyQkND0atXLwwcOBAzZsxAbm4uAgMDkZSUpMtWrVoVjRo1Qm5urtnbS0hIQHh4uO5nWx6bsWPHYvny5WjdujVefvllZGdn27TuRERUHgcrCjN34ImdB6Q4LSgoyOBnlUqFsrIyi9e5du0aZs6ciT179ugu+/btw9GjRw1avoeFhblkHW29nW7dumHPnj04evQo/vrrL90uLkc58tj06tULv//+O1544QX8+eefePDBB/Hiiy86vA7kA7KygIQEzVdT3w8ebPr3RMSjgZTWsKF9y5USFBRkMMsBAPfeey8OHz6M+vXrO3XbTZo0wbZt2wyWbdu2DQ0bNkRAQIDdtxcWFmZynZo0aYLbt28jJycHycnJAIBLly7h8OHDaNq0qWMrDyA4OLjcYwMA0dHRSE1NRWpqKjp37oyXXnoJc+bMcfh+yItkZQEZGUByMpCdDaSna37+/XfNV6D892fOAKWlpn+fkaG5jbQ0z28LkQQ4s6KwpCTg5ZcNl73yima5TBISErBp0ybk5+fj8uXLAIBp06bh008/xcyZM3HgwAHk5uZi+fLlePXVV+267UmTJmHTpk14/fXXceTIEXzyySeYP3++y2ciGjRogH79+mHUqFHYunUr9u7diyeffBK1atVCv379HL7dhIQE5OTk4OTJk7h48SLKysowbdo0fPXVVzh27BgOHDiAdevWoUmTJi7cGpKadmCycuXdQUd6OhAfr/lq6vuUFNO/197WlCmcbSG/xcGKBN56C9ixA/j0U81X7T9TMnn33XexceNGxMXFoU2bNgCAHj16YN26dfjvf/+L9u3b47777sN7772H+Ph4u2773nvvxcqVK7F8+XI0b94c06ZNw2uvvYbhw4e7fDsWL16Mtm3b4uGHH0aHDh0ghMD69evL7eqxx4svvoiAgAA0bdoU0dHROHXqFIKDgzF58mS0bNkS999/PwICArB8+XIXbglJR3/XjakBSFoacPKk5qup75ctM/177W0Bdwc+3E1EfkYlhBBKr4TMrl69isjISBQWFiIiIsLgdzdv3kReXh7q1KljUKNB5Gl8LSpEu7tHfwYkPl4z0PDm+yJygqXPTUdxZoWIyB76sxr6dSj6u27cwdRsS3IyZ1jIL3BmxQrOrJA34GvRgxIS7s5qaGc5lCp+1V8XzrCQJDizQkSkBFP1KMZ1KErgDAv5Cc6sWMGZFfIGfC26mewzGLKvH/kVzqwQEXmSdkYlOdlqPYot5/dyW4YzLOTrXNa430d54txARM7ia9FN4uM1J+yKj7cYs+X8Xh7J2Li+RO7EcwMREbmbufoUM2w5v5fHMpxhIR/FwQoRkT79w5FtKKC15fxeHsto1zc727BdP5GX42CFiMjO2RR9tpzfy5MZAJxhIZ/DwQpJ5+TJk1CpVNizZ4/Sq+KQhIQEzJ07V+nVIHvYOZuiz5bze3kyA4AzLORzOFjxQ8OHD4dKpdJdqlatip49e+K3335TetUUpx0o6T82f/vb37B7926bb2Pnzp0YPXq0zfnNmzdDpVLhypUrDqwxuYST3WdtOb+XJzOu2i4iWXCw4qd69uyJs2fP4uzZs9i0aRMCAwPx8MMPK71aLlNSUuLU9b///nucPXsW3333Ha5du4ZevXrZPJiIjo5GpUqVnLp/8hDt7h/A6eZuSUnA0KGWz5juyQyAuzMsAHcHkVfjYMVPhYSEIDY2FrGxsWjdujXS09Nx+vRpXLhwQZfZt28fHnjgAVSsWBFVq1bF6NGjce3aNd3vu3btiueff97gdvv3729wtuSEhAS8+eabeOqppxAeHo7atWtj4cKFBtf5+eef0aZNG4SGhqJdu3blZjFKS0sxcuRI1KlTBxUrVkSjRo3w/vvvG2SGDx+O/v37Y9asWahZsyYaNWqE1157Dc2bNy+37a1bt8bUqVMtPj5Vq1ZFbGws2rVrhzlz5uDcuXPIuXPIxZo1a9CsWTOEhIQgISEB7777rsF1jXcDqVQqfPzxxxgwYAAqVaqEBg0a4Ouvvwagmcnp1q0bAKBKlSpQqVS6x2/16tVo0aKF7vHv3r07rl+/bnG9yU76u398mb9sJ/ksDlYI165dw2effYb69eujatWqAIDr16+jR48eqFKlCnbu3IlVq1bh+++/x/jx4+2+/XfffVc3CHnmmWcwduxYHD58WHffDz/8MJo2bYpdu3ZhxowZePHFFw2uX1ZWhnvuuQerVq3CwYMHMW3aNPzf//0fVq5caZDbtGkTDh8+jI0bN2LdunV46qmnkJubi507d+oyu3fvxm+//YYRI0bYvP4VK1YEoJmt2bVrF1JSUvDEE09g3759mDFjBqZOnYolS5ZYvI2ZM2ciJSUFv/32G3r37o0hQ4agoKAAcXFxWLNmDQDg8OHDOHv2LN5//32cPXsWgwYN0m3D5s2b8eijj0Kw4bRrpaejODYeO7qlK9PMzVOZ9HRArQaKiji7Qt7JZR1bfJQvNoVLTU0VAQEBIiwsTISFhQkAokaNGmLXrl26zMKFC0WVKlXEtWvXdMu++eYbUaFCBZGfny+EEKJLly7iueeeM7jtfv36idTUVN3P8fHx4sknn9T9XFZWJmJiYkRmZqYQQogPP/xQVK1a1eAxzMzMFADE7t27zW7DuHHjxGOPPWawTdWrVxfFxcUGuV69eomxY8fqfp4wYYLo2rWr2dvNy8szuO/Lly+LAQMGiMqVK4v8/HwxePBg8dBDDxlc56WXXhJNmzY12Ob33ntP9zMA8eqrr+p+vnbtmgAgvv32WyGEED/++KMAIC5fvqzL7Nq1SwAQJ0+eNLuu+rz1taiIzExN07TMTHmauXkiw4Zx5CFsCufL9A+d9IBu3bphz5492LNnD37++Wf06NEDvXr1wu+//w4AyM3NRatWrRAWFqa7TseOHVFWVqabFbFVy5Ytdd+rVCrExsbi/Pnzuvtp2bKlwflsOnToUO42FixYgLZt2yI6OhqVK1fGwoULcerUKYNMixYtEBwcbLBs1KhR+OKLL3Dz5k2UlJRg2bJleOqpp6yuc3JyMipXrowqVapg7969WLFiBapXr47c3Fx07NjRINuxY0ccPXoUpaWlNj0GYWFhiIiI0D0GprRq1QoPPvggWrRogb///e/46KOPcPnyZavrTTa4s0ukeGaGPM3cPJDJG8hiW/JeHKzIwsP7lMPCwlC/fn3Ur18f7du3x8cff4zr16/jo48+svk2KlSoUG63xK1bt8rlgoKCDH5WqVQoKyuz+X6WL1+OF198ESNHjsR///tf7NmzByNGjChXRKs/sNLq27cvQkJCsHbtWvznP//BrVu38Pjjj1u9zxUrVmDv3r24fPkyjh8/jt69e9u8vqbY+xgEBARg48aN+Pbbb9G0aVPMmzcPjRo1Ql5enlPr4bdM9FHZ3dP0h7Yizdw8kNnanMW25L04WJGFwocYqlQqVKhQAX/99RcAoEmTJti7d69BQee2bdtQoUIFNGrUCIDmqJezZ8/qfl9aWor9+/fbdb9NmjTBb7/9hps3b+qW7dixwyCzbds2JCcn45lnnkGbNm1Qv359HD9+3KbbDwwMRGpqKhYvXozFixfjiSee0NWgWBIXF4d69eohKiqq3Ppu27at3Po1bNgQAQEBNq2TMe1skPHMjEqlQseOHTFz5kzs3r0bwcHBWLt2rUP34fdM9FFRmTnyR6lmbh7LsNiWvBAHK7KwsxGVs4qLi5Gfn4/8/Hzk5uZiwoQJuHbtGvr27QsAGDJkCEJDQ5Gamor9+/fjxx9/xIQJEzB06FBUr14dAPDAAw/gm2++wTfffINDhw5h7NixdvcKGTx4MFQqFUaNGoWDBw9i/fr1mDNnjkGmQYMG+OWXX/Ddd9/hyJEjmDp1qkHRrDVPP/00fvjhB2zYsMGmXUCWTJo0CZs2bcLrr7+OI0eO4JNPPsH8+fPLFQXbIz4+HiqVCuvWrcOFCxdw7do15OTk4M0338Qvv/yCU6dO4d///jcuXLiAJk2aOLX+fsvEPwOyNXPzWIa9V8gbuaz6xUf5aoEtAN0lPDxctG/fXqxevdog99tvv4lu3bqJ0NBQoVarxahRo0RRUZHu9yUlJWLs2LFCrVaLmJgYMXv2bJMFtvrFpkII0apVKzF9+nTdz9u3bxetWrUSwcHBonXr1mLNmjUGRa43b94Uw4cPF5GRkSIqKkqMHTtWpKeni1atWhlsU79+/cxuc+fOnUWzZs2sPjbGBbamrF69WjRt2lQEBQWJ2rVri3feecfg96YKbNeuXWuQiYyMFIsXL9b9/Nprr4nY2FihUqlEamqqOHjwoOjRo4eIjo4WISEhomHDhmLevHlm18lbX4tup1dMa86OHUJ8+qnmq79liNzBHQW2KiF4LKQlV69eRWRkJAoLCxEREWHwu5s3byIvLw916tQxKBAluQgh0KBBAzzzzDOYOHGi0qvjFnwtmpGQoNnlER9/t16DNPUqGRma2RUPzeaS/7D0ueko7gYin3bhwgXMnz8f+fn5dvVWIR/BXR6msW6FvEyg0itA5E4xMTGoVq0aFi5ciCpVqii9OuQpd2YO8gamY+vrJ9GwIWCuM31OjuYImoYNzbev97lMevrdmRUib+CyHUo+yhdrVsj38LVo5E4DtDzEy92oTcGMEMKmmh4ie7mjZoWDFSs4WCFvwNeioRMvZ4o8xIsxyDT40NYvNt2xw/AD3Z8yOuxqS27ADraSEqxRJoXxNQiDxm9bm6ehDk7iQxgWj8rWqE2pjA5reshLsGbFCdqupDdu3LCp0RiRu9y4cQNA+U65fkWvaLThCj9t+GZjRkd7JJC20JZHBpGkOFhxQkBAAKKionTneKlUqRJUKpXCa0X+RAiBGzdu4Pz584iKinK4i65P0Csa1TZG0z9Hjrnmaf6YMWDc3ZdIQuyzYoW148WFEMjPz7e7cyuRK0VFRSE2NtY/B8sWeoZIc/SNpBkA7LlCLueOPiteN1hZsGAB3nnnHeTn56NVq1aYN28eEhMTzeavXLmCKVOm4N///jcKCgoQHx+PuXPn2nxiOlsf9NLSUpMn8SNyt6CgIP+eUWHjNyKpuGOw4lW7gVasWIGJEyciKysLSUlJmDt3Lnr06IHDhw8jJiamXL6kpAQPPfQQYmJisHr1atSqVQu///57uZPTuUJAQIB/f2AQKYU9Q5zH2RWSnFfNrCQlJaF9+/aYP38+AKCsrAxxcXGYMGEC0k28UWVlZeGdd97BoUOHHC48dMcIkYg8Q7bdLrJldDg7RS7kls9Nlx0E7WbFxcUiICCg3Anhhg0bJh555BGT1+nVq5cYMmSIGDVqlIiJiRHNmjUTs2bNErdv3zZ7Pzdv3hSFhYW6y+nTp11+vDgRuYCVhmayNWGTLWOAzeHIhfy6Kdwff/whAIjs7GyD5S+99JJITEw0eZ1GjRqJkJAQ8dRTT4lffvlFLF++XKjVajFjxgyz9zN9+nSDMxJrLxysEEnGQkMz2ZqwyZYxiQMWchE2hbNTWVkZYmJisHDhQrRt2xYDBw7ElClTkJWVZfY6kydPRmFhoe5y+vRpD64xEdnMQkMz2ZqwyZYxiSc3JIl5zWClWrVqCAgIwLlz5wyWnzt3DrGxsSavU6NGDTRs2NCg8LVJkybIz89HSUmJyeuEhIQgIiLC4EJEEtF2qgU09RUmCkJla8ImW8YkdrMliXnNYCU4OBht27bFpk2bdMvKysqwadMmdOjQweR1OnbsiGPHjqGsrEy37MiRI6hRowaCg4Pdvs5E5AY2zABoG6PpM9c8zR8zJqWlmR38ESnOZTuUPGD58uUiJCRELFmyRBw8eFCMHj1aREVFifz8fCGEEEOHDhXp6em6/KlTp0R4eLgYP368OHz4sFi3bp2IiYkRb7zxhs336Y59b0TkBDtqK3bsEOLTTy3Xa/hzxgBrVshF3PG56VWHLgPA/PnzdU3hWrdujX/+859IuvMvQ9euXZGQkIAlS5bo8tu3b8cLL7yAPXv2oFatWhg5ciReeeUVm3ui8NBlIkmwF4h78fBlchF2sFUABytEkuCHqXtxMEgu4vcdbInIjxl1qpWtwZrXZ9LSOEgheblsh5KPYs0KkcJM1FLI1mDNZzKsWyEX8OumcErhYIVIYUbN32RrsOZLmZuxho81kSPYFI78i7afRlaW+e8t5cg3GPX/kK3Bmi9ldvdkrxWSEwcrJBf9wYZ+Pw1z3wPmf2dpUEPyM9P8TbYGa76UUbHXCsnKZXM0Poq7gTxAfz+5/pS//nJz3xtf39xtCWH+tklOFs79Y1xv8cor5a/OjIMZ/m2Qk1izogAOVtxI+6aoVrtnEGFpUGP8Qcg3aPlYeU5ka7DmMxkLg0QiW3CwogAOVlzM1IBBrfb8QMH4g5CzLvLg468sPv7kJBbYkvfS1h9MmXK3pkRbODlrluf3kxvvm9cv4jSuiWGti2fx7L9EZIQdbK1gB1sX0XYfVauB8HCbu2Qq0SCr0/4s1Fmh18mTnVM9KysLxTMzsLtnOlRpafI1T/P1DF/v5CS3fG66bI7GR3E3kJO0U8qDBtk9tax4gyzjbTBV+0KulZkpCiLixRhkKv+8+2uGr29yEmtWFMDBipMcLNaToUGW2cJE1re4jbYpWR7ipXje/TlD5CjWrJD30NZ5JCc71GRKhgZZ5pZbrG8hp+zumY6TiEcGDF8vsjVP8+kMa7RIQhyskHtoP8Szsx0qnpWhQZa55QbFuUbdVflG76A7j1v1GKAOTuJDGL5eZGue5tMZDsBJRi6bo/FR3A1kJydqVIxJ0SDLXtxF5Bi9x022593vMnzdkpPc8bnJo4Gs4NFAdnLxkQSKHxlhL+1pAvR3EfGoCuv0H7e0NOmed3/OENmLRwMpgDMrNnLhjIrP4FFE1vExkROfF3ICZ1YUwJkVG7E3g3V8jMrjYyInPi/kBHd8brLAlpyXlQUUFWkavtlx1E9ODrB0qearX2T0inHzXsnCtWoJyHvFzwtxjQuU75Di+fLnjJnnhUgxLpuj8VHcDWQDB3qpSNH8SsFMHuJ1/UTWPOSnU+5mdjXI+Hz5a4bIEWwKpwAOVmxg5/5t2ZpfKZEZg0yRh3jdV3sHez7BxCBX1ufL7zKsWSEnsCkcycfoKA5bSNP8SsHMh0jT9RPJQDquVY3XPIb+1KfFxK4GWZ8vv8uw1wpJhoMVco4Db2rSNL+SJPMh0nDgm5OawZ4/fEhoB2RAuYaBSj8XzNz5hjUrJBuXzdH4KO4GsiAzUwi1WnOxc7pYiuZXMmb84XBnKzVO0jwXzBA5hIcuK4CHLlvg5OGNsjW/ki0DwPcOIc3KAqZM0Xw/a5bZXYeyPRd+l3Fg9y6Rljs+NzlYsYKDFQv4huZ++o8x4P2Pt68NvnwVnydyAvuskDw4UPEM/ZMm+kI9C2shvAOfJ5IMByvkGCsfnFI0tvK1jP4HiLcdNaRXVJuz4iSWhqXJ+zgzQyQfl1W/+CgW2JphofBTtsZWPpnRFqmq1d5RgHtnfQsi4uV5DJkxn3Gg0SORFpvCKYCDFTPMDFakamzlwxnd469Wy/+hcueosZIItRiDTGkeQ2YsZHzxKDTyGDaFI3mY2Q0kVWMrH87oallmzZJ/11BGBlBQgOKgcHyI8vVNMj/OfpvRr5UikgAHK+QYMwV4UjW28uGMjuwFuHonubww0nSxpsyPs99mZB34kv9y2RyNj+JuIBOsTBHL1tjKVzPl6D8vSk/jm9lNJdtjyIyZDGtWyAlsCqcA9lkxwYYeDFI0tvKDjFna50itBsLDPX+IuYX7l+0xZIZN4ci12BROARysmMA3Mvlpn6OiIqCg4O4uO088bzZ2qSUi38SmcERkG1MFuJ6qablTUIvwcA5UvBVrVkgynFmxgjMrJnA3kFdmOu3PQp0VGUByMpCd7Z4ZFhtmVWR9fJjR+wXb7ZMT3PK56bLqFx/FAlsT7Cywlbr5lT9m3NFQzsa+L4pvOzO2ZZQu0CavxqZwCuBgxQQLb2RSNbZixmTmxMtGAwvtoGXQIMc+oDIzhQgIsDoAkmHbmbExw8EKOYFN4UgOFmofpGpsxYxJW5sb1bMAmudz5UrN1ylTNLsBBg8uX7egrWUYPBioWlVzmTIFKC0FAgI0t2mmmZgM286MjRkZe/aQX+Nghexn4YysUjW2YsZyxrgINyXF+uBl/Pi7vyso0FwAzfXmz7dYAyPVtjNjOcOzLpNsXDZH46O4G8h+0jS2Ysa55nLa3ULa3UXaXT0BAZrfqdWaix27CmTbdmbMZLgbiJzApnAK4NFAJtjQZ0WKIxqYsTljkfb5dtFRRLJtOzM8Gohci03hFMDBigl8IyPybWz8SE5gUziSA/dnE/k2nnWZJBOo9AqQF9K+gWmPFGDjL2aY8a0MZ1ZINi6rfvGQ+fPni/j4eBESEiISExNFTk6OTdf74osvBADRr18/u+6PBbZmWDgrqzSNrZhhhhnHMjzrMjnB75vCLV++XAQHB4tFixaJAwcOiFGjRomoqChx7tw5i9fLy8sTtWrVEp07d+ZgxVXMHC0gVWMrZphhxrEMjwYiJ/h9U7h//OMfGDVqFEaMGIGmTZsiKysLlSpVwqJFi8xep7S0FEOGDMHMmTNRt25dD66tjzOzT1uqxlbMMMOMwxkimXjNYKWkpAS7du1C9+7ddcsqVKiA7t27Y/v27Wav99prryEmJgYjR4606X6Ki4tx9epVgwuZYOasrFI1tmKGGWYcy7CDLcnGZXM0bvbHH38IACI7O9tg+UsvvSQSExNNXmfLli2iVq1a4sKFC0IIIVJTU63uBpo+fboAUO7C3UBG7KhZkbr5FTPMMFM+w91A5AS/bgr3559/olatWsjOzkaHDh10y19++WX873//Q05OjkG+qKgILVu2xAcffIBevXoBAIYPH44rV67gyy+/NHs/xcXFKC4u1v189epVxMXFsc+KMStHC0hxRAMzzDDDo4HI4/y6KVxJSQkqVaqE1atXo3///rrlqampuHLlCr766iuD/J49e9CmTRsEBATolpWVlQHQ7D46fPgw6tWrZ/V+2RTOAr6hEfkmNn4kJ/h1U7jg4GC0bdsWmzZt0i0rKyvDpk2bDGZatBo3box9+/Zhz549ussjjzyCbt26Yc+ePYiLi/Pk6vsm7tcm8k3JyZqzaCcnK70mRAC8rCncxIkTkZqainbt2iExMRFz587F9evXMWLECADAsGHDUKtWLcyePRuhoaFo3ry5wfWjoqIAoNxyclB6+t2ZFSNSTGUzwwwzjmWys4HSUs1XIhm4rPrFQ+bNmydq164tgoODRWJiotih1zygS5cuIjU11ex1bSmwNcY+K/aTprEVM8ww41iGBbbkBL9vCqcEDlasMHpTk6qxFTPMMONYhoMVcoLfN4UjCRnVrcjW2IoZZphxIMN6NJIMByvkHKMzMEvV2IoZZphxLMMzq5NsXDZH46O4G8gGRlPG0jS2YoYZZhzLcDcQOcGvm8IphX1WbGCiJ4MURzQwwwwzjmXYZ4Wc4NdN4ZTCwYoN2ByOyLcMHgysXAmkpADLlim9NuRlOFhRAAcrROR3OLNCTvDrDrYkOTNnYSYiL8QOtiQZzqxYwZkVGznwn5gU++aZYYYZ1qyQS7nlc9Nlpbo+ikcD2cjOowek6dTJDDPMlM9kZgqhVmsuPCKI7MQOtgrgYMVONgxapOrUyQwzzJjMiPh4zcL4eEFkD3awJfnZ0PlSqk6dzDDDjOnlrFshiXCwQq5lQ+dLqTp1MsMMM6aX88zLJBOXzdH4KO4Gcg9pOnUywwwzpjOsWyEHsYOtAng0kANsbBInxVEPzDDDjPmM9qiggABg/nw2fSSbsCmcAjhYcQAPeyTyDVlZwPjxmt1B/HsmG7EpHHkHnrGVyDekpWlmVNRqoKiITR9JMZxZsYIzK+4j1XQ3M8wwYx53B5Ed2BROASywdYKFnivSNL9ihhlmzGZ0MjOFCAjQBAMCWHBLFrEpnAI4WHGCmaZSsjW/YoYZZspnyuGAhWzEpnDkXczUrsjW/IoZZpixvBzA3fqVgABNwe2UKZrdQ4MH8ySm5HaBSq8A+TDtfm1tN9s7P8vW/IoZZpixvFxH/2+6qEhTx3LmjGbwMn488NNPmiZy2n9QbGhhQGQTl83R+CjuBnKSmV1BUjW/YoYZZkxmLNLWpA0aZLh7SPv3rv3bV6vv5rQ1bPr1bPZ+r3/fzvzO2s92npyV7mJTOAXwaCAnWWgQJ9tRD8www4yDtH/nycnlZ1aKioCCgru7j+LjNb/T9mKy9/uTJw17OTn6O2s/G1/X+L3MxuaX/ohHAymAMysuwv9SiPyT/gyMN8+sGM8SG/9s7bb9CGdWFMCZFRdhV1si8mbWZlYszdKkp/vVLAzb7SuAgxUXMTFlKtt0NzPMMMOMo5lO+7NQZ4WZwUxGhuVdSj6Gu4EUwN1A7iFb8ytmmGGGGXdkhBCWdyn54O4iNoVTAAcrLpaZKW7GxosxyDT4IwfkbpDFDDPMMON0Yz0t/QGKtdoXL8SmcOT9MjIQkv870pFR7lcyN8hihhlmmLE3Y1ZammZ3UFpa+eaZ2l1GGRma3UVsuAeAZ10mT0tPR3FsPDJQ/ozMMjfIYoYZZpixN2MT/YELYDh40R+4+DuXzdH4KO4Gco81D2WKPNzdHaR08ytmmGGGGXdknOKlTep46LICeDSQm9w5rO9a1Xgc+OakVFX9zDDDDDOuzLiEFx0KzUOXFcDBipv4+KF7REQuZelQaMm443OTNSukDG1hmbaIjIiIzDNXlOsnRbicWbGCMytuZENXW9mmc5lhhhlmpMpI2B2cTeEUwAJbN7JSMCZbcydmmGGGGekyEhbhsimcAjhY8QATf2CyNXdihhlmmJE5I4Qo32BOIWwKR77JRC8B2Zo7McMMM8zInAFwt5YlOdnn6lg4WCHlGXdwhHzNnZhhhhlmZM4AuFuEm53te83kXDZH46O4G8iDjHYHydbciRlmmGFG5oyO9r100CBFaljYFE4BPBrIg0xUtSteac8MM8ww40UZAwodKcSmcArgYMWD2CiOiMh1tO+pycmaXUMeem9lUzjybdr9rYDPFYcREXmcD9WwcGbFCs6sKMDOqUvZpmGZYYYZZqTKeHiGhU3hFMACWwXY0dhIiqZMzDDDDDPekPFQHxY2hVMABysKsjJoka0pEzPMMMOMzBmRmSmEWq25uPEIITaFI/9iolmcPtmaMjHDDDPMyJxBWhoQHg4UFHhd/YrXDVYWLFiAhIQEhIaGIikpCT///LPZ7EcffYTOnTujSpUqqFKlCrp3724xT5Ix0SxOn2xNmZhhhhlmZM4AsPq+Ki2XzdF4wPLly0VwcLBYtGiROHDggBg1apSIiooS586dM5kfPHiwWLBggdi9e7fIzc0Vw4cPF5GRkeLMmTM23yd3AynMzpMdKt2UiRlmmGFG5oyOG0966PdN4ZKSktC+fXvMnz8fAFBWVoa4uDhMmDAB6TaMEktLS1GlShXMnz8fw4YNs+k+eTSQwmw4MkjxSntmmGGGGS/KAHBrwzi/bgpXUlKCSpUqYfXq1ejfv79ueWpqKq5cuYKvvvrK6m0UFRUhJiYGq1atwsMPP2wyU1xcjOLiYt3PV69eRVxcHAcrSmGjOCIi1xs8GFi5EkhJAZYtc+lN+3VTuIsXL6K0tBTVq1c3WF69enXk5+fbdBuvvPIKatasie7du5vNzJ49G5GRkbpLXFycU+tNTmKjOCIi18vOBkpLNV+9QKDSK+ApGRkZWL58OTZv3ozQ0FCzucmTJ2PixIm6n7UzK6Qw/SODjGZYZJtiZYYZZpiROQNAM1utnbX2Bi6rfnGz4uJiERAQINauXWuwfNiwYeKRRx6xeN133nlHREZGip07d9p9vyywlYSZ/gBSNVxihhlmmJE8Y8BNRbZ+3xQuMTFRjB8/XvdzaWmpqFWrlpg9e7bZ67z11lsiIiJCbN++3aH75GBFIkbdF2VruMQMM8wwI3OmHDd1tPX7pnATJ07ERx99hE8++QS5ubkYO3Ysrl+/jhEjRgAAhg0bhsmTJ+vyb731FqZOnYpFixYhISEB+fn5yM/Px7Vr15TaBHKGUX8A2RouMcMMM8zInCknPR1Qq4GiIunrAb1qsDJw4EDMmTMH06ZNQ+vWrbFnzx5s2LBBV3R76tQpnD17VpfPzMxESUkJHn/8cdSoUUN3mTNnjlKbQM4wKrbttN/0H5fMTZmYYYYZZpTKlONNHW1dNkfjo7gbSEJ6U5eyNVxihhlmmJE5U44b6lb8vimcEtgUTkJZWcCUKZrvZ81CTps0qSrtmWGGGWZkzhhwQy8rv24KpxQOViTlxu6LRER+ww3vpX7dFI7IgBcVhhERSctLTmzoN03hyMekpVlsFGdMtmlYZphhhhmlMl7JZdUvPooFthIz0yjOmGxNmZhhhhlmlMqU44ZeK37fFE4JHKxIzsofmmxNmZhhhhlmpGkKJ4QQgwYJERCg+eoift8UjqgcK7UrsjVlYoYZZphRKmOSl5zQkIMV8m5WmhrJ1pSJGWaYYUapjEleUmALl83R+CjuBvICVqYxZWvKxAwzzDCjVMaAF53IkH1WrGCfFS9gQ58A2arxmWGGGWaUyui4qV8Vm8IpgIMVL2DU0dZVXRiJiHyaG7rXAmwKR2SaN52Mi4hIBm4aqLgLm8KRb0hOBs6c0Xw1QbZpWGaYYYYZpTIA7GqqKQWXVb/4KBbYegm1WlNRplaX+5VsTZmYYYYZZpTK6LipuFYINoVTBAcrXsLMYEW2pkzMMMMMM4o3hXPjQEUINoUjMq9HDyAgQPNVj2xNmZhhhhlmlMro6O8C8hIcrJBvMNOFUbamTMwwwwwzSmV0vKURnD6XzdH4KO4G8gJWTmgoW1MmZphhhhmlMu7eBSQEm8Ipgn1WvEDVqprDltVq4NIlkxHZqvGZYYYZZhTJuKkRnD42hVMABytewIbBChGRX9P2VUlO1uwud2N/FXd8brLPCnk37ZmW1WpN91oiIipPW1QLuG1GxZ04WCHvlZUFjB+vKayNj7f4X4JU07DMMMMMM27KmJWefrdjrTdyWfWLj2KBraQyMzVnWgY0Xy0Ui8nWlIkZZphhxh0ZszxQVKuPTeEUwMGKpOLjbRqoyNaUiRlmmGHGrQ3fTNG+X8bH23gF57ApHFFWlqaaPTlZs+tn/nyLu39ka8rEDDPMMOOOjEXe2FfFCGtWyHvo16gANhWJydaUiRlmmGHGHRmT9M+s7IVFtQZcNkfjo7gbSALa/a3a8/9Y2fVjTLamTMwwwwwz7siU4+HdP1psCqcA9lmRgLaJkVoNhIc71B9Atop9Zphhhhl3ZAzoz6y4qaeKKWwKpwAOVhTkwSZGREQ+Q6FBihYHKwrgYEUB2j+0oiJNZ1o3toUmIvI5Hmipbwk72JJ/0HZaVKtRHBuP3d3SocqRY4qVGWaYYUbKjP5sirc3gDPFZdUvPooFth6i37TozvdrHso0KChTuuESM8www4y0GYWKaU1hUzgFcLDiZsZH+tz5Q5Ot4RIzzDDDjMwZT3eptYRN4cj36J9cS69pkWwNl5hhhhlmpMxoG2UCmvoUHz0IgYMV8jztH1dW1t3OirNmGfyhydZwiRlmmGFGyoz2H76MDNMhX+GyORofxd1ALqI/RWnjvlXZGi4xwwwzzEiXkWj3jxabwimAhy67iP6hdPqV6lamLKWptGeGGWaYkSTTaX8W6qxQro+KNeyzogAOVpzExm5ERK6lcB8Va9zxucmaFXI9/ZoU7f7U7GyfLv4iInIrU7V+vtRHxQrOrFjBmRUH3Bn1X6sajwsj081OV8o6xcoMM8wwI11G8tkUfW753HRZ9YuPYoGtjfSKvNY8lCnyEC/GQNPUTfpmSswwwwwzkmdkLKQ1h03hFMDBio3uHOFzMzbe4I9Oe5G5mRIzzDDDjJQZLxqg6GNTOJKH/v5TQLcPdXdP0/tQpWymxAwzzDAjc8ZfeqjYgIMVcozxH1FaGnDyJFRmCmilbKbEDDPMMCNZZgyykIcEdNrvn4W0ZrlsjsZHcTeQHhMnGzQ1PSlFoyRmmGGGGS/M5CFe840EJyR0FJvCKYBHA+mxoxpdqip6ZphhhhmZM1lZKJ6Zgd0901E9BlI3fLMFm8IpwO8HK9peKdppSBs7zxIRkY286LBkW7ApHIAFCxYgISEBoaGhSEpKws8//2wxv2rVKjRu3BihoaFo0aIF1q9f76E19RH6tSl36lI4UCEiciHWplhl92AlNTUVP/30kzvWxaoVK1Zg4sSJmD59On799Ve0atUKPXr0wPnz503ms7OzMWjQIIwcORK7d+9G//790b9/f+zfv9/Da+5lrHRKzMkBli7VfDWHGWaYYYYZC/TfZ/mPoHX2Frn069dPBAUFifr164tZs2aJM2fOuKyAxprExEQxbtw43c+lpaWiZs2aYvbs2SbzKSkpok+fPgbLkpKSxJgxY2y+T78ssLVwVmTZGiUxwwwzzMicMcvGs897I2mawp0/f168++67omXLliIwMFD07NlTrFq1SpSUlLhsxYwVFxeLgIAAsXbtWoPlw4YNE4888ojJ68TFxYn33nvPYNm0adNEy5Ytzd7PzZs3RWFhoe5y+vRp/xis2HCkjzSNkphhhhlmvCBTjo1HVHo7aZrCRUdHY+LEidi7dy9ycnJQv359DB06FDVr1sQLL7yAo0ePunLyBwBw8eJFlJaWonr16gbLq1evjvz8fJPXyc/PtysPALNnz0ZkZKTuEhcX5/zKewMbalOkaZTEDDPMMOMFmXJYA+gwpwpsz549i40bN2Ljxo0ICAhA7969sW/fPjRt2hTvvfeeq9bRoyZPnozCwkLd5fTp00qvkvvYeRZP2ZspMcMMM8zIlAHg92dLdhl7p2JKSkrE6tWrRZ8+fURQUJBo27atyMzMNJju+fe//y2ioqJcNv0jhOd2Axnz6ZoVB/aZytxMiRlmmGFGtowv16aYI0VTuGrVqqGsrAyDBg3CqFGj0Lp163KZK1euoE2bNsjLy3PNiOqOpKQkJCYmYt68eQCAsrIy1K5dG+PHj0e6iZHqwIEDcePGDfznP//RLUtOTkbLli2RpT2njRU+1WdFv2dKWlr5n20kVTMlZphhhhnZMn7en0qKpnBLly7F3//+d4SGhrpkBeyxYsUKpKam4sMPP0RiYiLmzp2LlStX4tChQ6hevTqGDRuGWrVqYfbs2QA0hy536dIFGRkZ6NOnD5YvX44333wTv/76K5o3b27TffrUYMXHGg8REUnJz99rpWgKN3ToUEUGKoBmpmTOnDmYNm0aWrdujT179mDDhg26ItpTp07h7NmzunxycjKWLVuGhQsXolWrVli9ejW+/PJLmwcqPoH7S4mI3MvMWej5Xus6bLdvhdfPrOiN8HNWnJR/+pQZZphhxosyAPx+JsWYWz43XVb94qO8vsD2zrH8ax7KNCgEU7pREjPMMMOMV2f8pGeKI6RpCudPvHKwYvSHI1ujJGaYYYYZb8yceFnvvdUPj/KxlTRN4Uhy+o2HIF+jJGaYYYYZb8xE/0vvvZV1KR7FwYqvsFBIK1ujJGaYYYYZb8mMQRbykIAxyMKFkXrvrexA61kum6PxUV6zG8jKlKRsjZKYYYYZZrwhk4d4IQBREBFfPkQmSdEUzt94zdFANjR4k62KnhlmmGFGysyd99O8gek4dx5osyEDIdP9p6mbs6RoCudvpB+sONiFloiI9Oi/l2rr/ngoskOkaApHkjEqpiUiIgdksHhWZoFKrwA5QP8/gPR0FM/MwO5u6VDlWG5cJM0UKzPMMMOMwhkd7ftpcrLmZ+0sNWeq5eKy6hcfJWWBrV4xrS3NjYSQrJkSM8www4yCGYNeVOyX4nJsCqcAKQcrd/7ATrxs2JVWe9FvbiSEHM2UmGGGGWZkyAghDAco7D7rcmwK5++0vVQA4ORJbG1ueprSuKGRDM2UmGGGGWYUzZjrRcV+KV6BNSveRL8ALC3NpuZGpn42tZwZZphhxhczY5CFlJczgJIioKBA8/7JwYn3cdkcjY+SajeQielKW5ob2ZpjhhlmmPG1TEFEvOYbtZq7ezyETeEUoHifFRc1e7M1xwwzzDDjCxmRlaVp5tYtGcjOZi8qD2JTOAUoPlhJSGBzIiIiW2n/wSu6s9uH750ex6Zw/kRbDJaczOZERETWaN8zp0zR/IMH8L3Th7DAVlbaYlrA6n8F3A3EDDPM+EvGLO17plpteKQP+QaXVb/4KMUKbG089p9N4Zhhhhl/yZiVmakpoFWrWUArATaFU4DHByt2NCiytQGSbE2ZmGGGGWYcauZmTPt+qVZrrsAutFJgUzh/YMeJCW1pkmRrjhlmmGFG5kw5WVnA+PGsT/ETHKzIxo6zfdrSJMnWHDPMMMOMzJlyMjKA0lIgIACYNYuN3nydy+ZofJTHdgM5eH4KNoVjhhlm/CWjwxoVqbEpnAI81mfFiX4qPBqIGWaY8ZeMbvdPaSl7qEiKTeEU4LHBig2daomI/J72H7uAAGD+fL5fSohN4XwVBypERLZJTtYMVFJS+H7pRzizYoVHZlYs7AJyZSMl2aZzmWGGGWbsynAXkFdwy+emy6pffJRHCmzNFNe6spGSbM2dmGGGGWbszYj4eM0vAwJYWCsxNoVTgNsHK2YGKq5spCRbcydmmGGGGYeawg0apBmoDBokSF5sCueLzDSBc2UjJdmaOzHDDDPMOPJehuxszS6g7GzTVyCfxcGK0sw0gXNlIyXZmjsxwwwzzDjyXqYrrk1ONn0F8l0um6PxUW7dDWSlEZwrGynJ1tyJGWaYYcbejO4cQGq1iV+SLNgUTgFuPRrIhkZwPBqIGWaYYeaOqlWBggJArQYuXTJ9I6Q4NoVTgNsGK1lZwJQpmu9nzWK/ACIia/i+6RU4WFGA2wYrTrTXJyLyW3zvlJ47PjcDXXIrZL/09Ltday3gbiBmmGGGGT3JycCZMyyy9Tcuq37xUW4psLXxDMtsCscMM8wwY0TbGC4+3kyAlMamcApwy2DFhj82NoVjhhlmmBHlZWZqjgZSq9nFVlJsCucrzPRW0efKRkqyNXdihhlmmHHkvQyApqg2PFxzVJBRM03yXRysSMqVjZRka+7EDDPMMOPIe5lOerrm8OWiIs0RQuT7XDZH46OU2g0kBJvCMcMMM8yYxdoVabEpnALccuhyVtbdI4Gs9Ang0UDMMMMMMybY8T5KnsU+KwpQerBCRERm8L1UShysKMAtgxU2NSIich7fS6XEpnC+wo6mRtwNxAwzzDBjJpOermm/ry205eyK73JZ9YuPkqnAlk3hmGGGGWaMMiy0lQ6bwilAqQ62bArHDDPMMGM9c+LlO++ngwbZ1Bmc3M+vm8IVFBRgyJAhiIiIQFRUFEaOHIlr165ZzE+YMAGNGjVCxYoVUbt2bTz77LMoLCz04FqbkZZ299xAZnoEuLKRkmzNnZhhhhlmXJXZ2jxNU6+Sna2pX2GjOJ/kNYOVIUOG4MCBA9i4cSPWrVuHn376CaNHjzab//PPP/Hnn39izpw52L9/P5YsWYINGzZg5MiRHlxrCzIyLP5hubKRkmzNnZhhhhlmXJ6xoTM4eTGXzdG40cGDBwUAsXPnTt2yb7/9VqhUKvHHH3/YfDsrV64UwcHB4tatW2YzN2/eFIWFhbrL6dOnXT6dJYTQTFkGBGi+msGmcMwwwwwzdmRsPEksuZffNoVbtGgRJk2ahMuXL+uW3b59G6GhoVi1ahUGDBhg0+18/PHHmDx5Mi5cuGA2M2PGDMycObPccpceugzYfMgdjwZihhlmmLExw0OZpeC3fVbefPNNfPLJJzh8+LDB8piYGMycORNjx461ehsXL15E27Zt8eSTT2LWrFlmc8XFxSguLtb9fPXqVcTFxbl+sMJmRkRErqV9X01O1tSw8P1VEe4YrChas5Keng6VSmXxcujQIafv5+rVq+jTpw+aNm2KGTNmWMyGhIQgIiLC4OIWNhTZEhGRHdJYbOurFG0KN2nSJAwfPtxipm7duoiNjcX58+cNlt++fRsFBQWIjY21eP2ioiL07NkT4eHhWLt2LYKCgpxdbdfRL7I1M/rnbiBmmGGGGfsyun8EWWzrO1xW/eJG2gLbX375Rbfsu+++s1pgW1hYKO677z7RpUsXcf36dYfu2y19VrS0xWBm+gOwKRwzzDDDjH0ZHRbbKsavm8L17NlTtGnTRuTk5IitW7eKBg0aiEF6R9KcOXNGNGrUSOTk5AghNA9WUlKSaNGihTh27Jg4e/as7nL79m2b79etgxUtEx0Y2RSOGWaYYca+jAF2tlWMXzeF+/zzz9G4cWM8+OCD6N27Nzp16oSFCxfqfn/r1i0cPnwYN27cAAD8+uuvyMnJwb59+1C/fn3UqFFDdzl9+rRSm2Gaif4Armqk5MrbYoYZZpiROWOAfVd8itecyFCtVmPZsmVmf5+QkAAhhO7nrl27GvwsNW29irYYLC3NZY2UbM0xwwwzzHh7xkBamuaSlaU5pJlHBnk3l83R+CiP7AYSwuSUJZvCMcMMM8zYlymHu4M8zm+bwinJHceLm2SmPwCPBmKGGWaYsS9jgD2tPM5vm8IpyWODFS12YCQicj0OWjzG55rCkQnaorDkZM3AhQ3jiIicZ+XksSQ3zqxY4fGZFS07Zli4G4gZZphhxkqGMyse45bPTZdVv/gojxXYGrPSME6LTeGYYYYZZuzIsFmc2/l1UzilKDZY0bJQyc6mcMwwwwwz9mV4dJD7+XVTOL9loYbF1iZJsjVuYoYZZphRKsNmcd6JgxXZWTiLqK1NkmRr3MQMM8wwo1SGZ7z3Ui6bo/FRiu8G0tLfz6r3PZvCMcMMM8zYl+GuIPdiUzgFKHY0kCX6Rwqlp6N4ZgZ290yHKi2NRwMxwwwzzFjL8Mggt2JTOAVIOVjR/0PT9g5gEzkiIvtw0OIWHKwoQMrBij79PzaAf3hERLZix3C3cMfnptecdZnM0J5ZFLj7h6ctwnXTwEWq6VxmmGGGGQczols62mzIQAiPDJKfy6pffJQ0Bba20C/C1S8gc2ETJKmaOzHDDDPMuClDjmNTOAV41WBFnxsGLrI1d2KGGWaYcUVmDDLFzdh4drV1ETaFI9tp+7NoewpomyAZn8wrK8vmEybK1tyJGWaYYcYVmXRkICSfJzmUGQcr/sDcwAWwa/AiW3MnZphhhhlXZDKQjuLYeHa1lZnL5mh8lNfuBrKV8W4hK7uMZGvuxAwzzDDjsgxPcugSbAqnAOkPXXY1Sz1c7vwub2A6tjZPQ6f9WaizwvQRRzJW/jPDDDPMWMzwUGaXYJ8VBfjdYEWfccMk4z9k458t9Xxh8yUikh3fp1zCHZ+brFkh8/RrXYDy9S6W6l+Ma2Gs1cbo/2zpd/Zk7fl+8GDzy/R/R0S+iyc5lBZnVqzw65kVe9kzs2JplgawPINja1b/d9a+DwgASktNL9N+VauB8HAgOVlzFmwn/vtSfLqbGWaYMZ3hriCnueVz02XVLz7K5wtslWJcyGbmrNJOZe35ftAg88u0X9VqTTVeQIDmq1rtUDGebM2vmGGGGb1fssjWaWwKpwAOVkjHeBCjP3ix8Y1N6eZXzDDDjPUMOYdN4YiUpK3hWbZM83XWrLu7iKZMsamuRenmV8www4z1DAC7GmaS+3GwQuSotDRg/nzDOpfx4y2+uSnd/IoZZpixngFQ/qAAUpbL5mh8FHcDkU0yM+/WsljZLSRF8ytmmGHGYoa1K45jUzgF8GggsllWlmZmRXsE0fz5Zo8WkuKoB2aYYcZiBgB7rziATeEUwMEK2UV/wMJDH4m8Hw9lthubwhHJTlvHolYDRUUsziPydsbNL0kRnFmxgjMr5JCqVYGCAs2g5dKlcr+WbbqbGWaYIVdhUzgFsMCWHKLtwaJWl/uVNM2vmGGGGbMZAyy2tQubwimAgxVySGamZqCiVhu8wcnW/IoZZpgpnyknPl4Tio+3ECItNoUj8hZpaZrzCBUUGPRpkK35FTPMMGN5OQDWrUggUOkVIPJZycnAmTOar3fI1vyKGWaYsbwcgOafDx62rCyXzdH4KO4GIoeZmTqWrfkVM8wwUz5TDutWbMamcArg0UDksKwszTmDAM15hPT+M5PtqAdmmGHGCvZbsRmbwimAgxVyCt/giHwDO9najE3hiLwNC/OIfIP2rOsAz8asABbYEilAtuluZphhxsaM/tmYOcPiOS6rfvFRLLAlp5gospWt+RUzzDBjR4aFtlaxKZwCOFghpwwaJERAgOarkK/5FTPMMONYhsxjUzgib5OdrTkDc3Y2APmaXzHDDDMOZLKyWLfiYRysELmTUYGtbM2vmGGGGQcy+nUr5Bkum6PxUdwNRE4xsX9btuZXzDDDjJ0Z1q1Y5NdN4QoKCjBhwgT85z//QYUKFfDYY4/h/fffR+XKla1eVwiB3r17Y8OGDVi7di369+9v8/2yzwo5xUyfFcWPaGCGGWacy7Dvill+3RSuV69eOHv2LD788EPcunULI0aMQPv27bFs2TKr133vvfewceNGfPvttxyskGfxDY3IN7Hho1l+2xQuNzcXGzZswMcff4ykpCR06tQJ8+bNw/Lly/Hnn39avO6ePXvw7rvvYtGiRR5aWyIi8nls+OhRXtEUbvv27YiKikK7du10y7p3744KFSogJycHAwYMMHm9GzduYPDgwViwYAFiY2Ntuq/i4mIUFxfrfr569apzK0/+zUwDKWmmsplhhhnHMjwTs2e5rPrFjWbNmiUaNmxYbnl0dLT44IMPzF5v9OjRYuTIkbqfAYi1a9davK/p06cLAOUuLLAlh9hQYCtt8ytmmGHGfIZFtmb5XFO4V155xeTAQP+Sm5vr0GDlq6++EvXr1xdFRUW6ZbYMVm7evCkKCwt1l9OnT3OwQo4zekOTrbEVM8ww41jGVHdq0vC5pnCTJk1Cbm6uxUvdunURGxuL8+fPG1z39u3bKCgoMLt754cffsDx48cRFRWFwMBABAZq9ng99thj6Nq1q9l1CgkJQUREhMGFyGFG/RikamzFDDPMOJxhzYpnKVqzEh0djejoaKu5Dh064MqVK9i1axfatm0LQDMYKSsrQ5KZnY3p6el4+umnDZa1aNEC7733Hvr27ev8yhPZIj397tFAkKyxFTPMMONwRlevom0Mx/oV93LZHI2b9ezZU7Rp00bk5OSIrVu3igYNGohBd863IoQQZ86cEY0aNRI5OTlmbwOwvhvIGJvCkVPYFI4ZZnw2w11Bpvl9U7jx48cbNIX75z//qWsKd/LkSdSpUwc//vij2d08KpWKfVbIs9gUjhlmfDfDPkom+XVTOKVwsEJO4ZsZEfkZv20KR0REJB2efdljOLNiBWdWyCncDeQTGU+SbduZsZBhy32T3PK56bLqFx/FAltyCpvCeX3GIu3zO2iQEGq15jJoUPllNjYOk23bmbHy2mBjOJN8rimcN+BghZzCpnBenbEoM1OIgADNlbRf9b/XX6ZWW/1Qk23bmbHxtcEBSzk+1xSOyOexKZxXZ0zS1ilMmQKUlgIBAUBKCqBWay4pKZrdAvrLAM3rYMoUszUOsm07M5YzOkZ/4+QeHKwQuZNRl0vZGlsxYzlTTlYWMH685sMJ0Dy38+cDy5YBly5pLsuWaeoX9JfNmqXJAmYHLbJtOzOWMzrsZOsZLpuj8VHcDUSuJltjK2YsZ4QQd6f61eq7u3gcmfY3vh3t7oM7X2XbdmbseG1wN5COXzeFUwqPBiKnmOmzItURDcxYb/w1frxml49aDYSHO983R/91od2NcOe28wamY2vzNDm2nRnrR4rxiKBy2BROARyskFP4Rub9tM9hQIBml4+rm/tpBy5FRUBBgesGROQZbPxYDpvCEXkb7s/2bllZmkGEWu2egQqguc2TJ8vXtbBgk0iHMytWcGaF3EG2qWxmzGSUmBnT/08dcPi/dmkeQx/PcPa0PDaFUwALbMkpbArnnRn9Zm9KFk/qn9XXjkJOKR5DP8gIIVhgawKbwimAgxVyitEp5GVrbMWMmYzR86YY/Q9CG9dJmsfQxzPlnh/SYVM4Im9jVLMiW2MrZsoT+nUqStcaaetZ0tLuvpaSky2ePE+Gx9AfMgDYEM6DOFgh8iDZGlsxU16bDRmao3LCw+U6ukM7cMnOtvgBKcNj6A8ZACyg9ySXzdH4KO4GIqeYmLqXrbEVM5rLGGSKgoh45etUrNHf9WBmN4TMj7MvZbgbyDQ2hVMAjwYip7ApnNdkUl5OQEi+lx3VYeFIFFkfZ5/K8Eggk9gUTgEcrBD5gawszfl6AE2/E5l2/1jiosOcyUFsCGcSm8IReRvtGXrNFEOSJDIkrVOxRr8AV7/Yk6879+NAxaM4s2IFZ1bIKWamiaWayvbzDADf+OAxdb4hK7snZHsuZMtYxF1AZrEpnAJYYEtOYVM4qTNCCN8sknSgCFfp50K2jFW++LpxETaFUwAHK+RKsjW28ueMjiwN4NzFxPbJ9lzIlrGKAxWL2BSOyNsY1Q7I1tjKnzO65yY52bd7ZRj3AsnKQrM+CRiD8vUsMj9fHm/4ZgmbwXkcBytE7mT0piZbYyt/zuiem+zsu0Wqvki/CBcAMjJQ+dLvSEf5D1qZny+PN3wzR6YOx/7EZXM0Poq7gcgpNtSsKN3Yym8z/jqVf2e71zyUKcYgU+QhXoxBpvzPl4czZvn6bkMXYFM4BfBoIHIKm8LJmfGFo39coLiGphFecWw8Qs6eNJmR4vlSKGMSXztWsSmcAjhYIafw8EY58XnRYFM523GQYjM2hSPyNjzRmZz4vGiYayoHsLGcMRbVKoqDFSJ3Mi5uvCMnB1i6VPPVHGbckNF+AAO+XVTrCOMBnIs/nGV7bdiFRbXKc1n1i49igS05hU3h5MqwONJ2NjSWs5Xiz7uz+LqxC5vCKYCDFXKK0ZucbM2v/C1z4mU/PQLIWcYf1nYMXmR43u1q+KZPu52DBvF1Ywc2hSPyNkZT67I1v/KXzBhkIQ8JOHce3P3jCCd2Ecn22rCLv/Ti8QIcrBC5k1HNimzNr/wlk44MJOB3tNnA4kiHGNde6Q9erBTiyvbasIm/dDf2Ji6bo/FR3A1ETjOaMpet+ZU/ZMYgUxRExHMa3x1s2EUk22vDLO26q9WsUXECm8IpgH1WyGkmenrI1vzKlzMiKwttNmQgZDr7Y7iFcf8R/dd7errudzlt0qR6bZikXXe1GggPZ08VB7EpnAI4WCGnsZmUstgAzrP0X+/amg/ZH3vtOicna+pT+LfqFDaFI/JGZnqtkIewAZxn6b/eTZzxWZpGc/rrwkJa6XGwQuRuJt6gZWuQ5ZMZNoBTnokzPuuOIjL+u3DnQEZ724MHlx+gaGeBOKCVm8uqX3wUC2zJaUYFiLI1yPLVDBt5SUi/+Nb4+dH/2bhI19a+LuYa2WlvOyDA/H2Qy7ApnAI4WCGn6b0pytYgy1czxo87ScjSgMTWgYzxbejnTF2Hzd08gk3hiLyR3lS4bA2yfDLD3T/ewXgXkaVaF/2f9XffGDen08/pf6+97WXL+JrwUoFKrwCRX7izj7zTwHQA5d8oZWme5hMZ/Q8wfih5p7Q0w+fO+GdtnYnx96auR77BZXM0Poq7gcgl9KakZWuQ5XMZ7v4hUhSbwimAfVbIJYx6rcjYPM0XM0TkeWwKpwAOVsgl2BjO/fgYE0mBgxUFcLBCLsEuqu7Hx5hICuxgS+StHGg6JVWDNQkz5bCxF5Hvcln1i5tdunRJDB48WISHh4vIyEjx1FNPiaKiIqvXy87OFt26dROVKlUS4eHhonPnzuLGjRs23y8LbEkJsjVYky1jgAW1RFLx66ZwPXv2FK1atRI7duwQW7ZsEfXr1xeDBg2yeJ3s7GwREREhZs+eLfbv3y8OHTokVqxYIW7evGnz/XKwQi5j44eqbA3WZMuUw061RFLx26Zwubm52LBhAz7++GMkJSWhU6dOmDdvHpYvX44///zT7PVeeOEFPPvss0hPT0ezZs3QqFEjpKSkICQkxINrT3SHcQMrM6RqsCZhphzu/iHyeV4xWNm+fTuioqLQrl073bLu3bujQoUKyDGzU/v8+fPIyclBTEwMkpOTUb16dXTp0gVbt261eF/FxcW4evWqwYXIJWz8UJWqwZqEGR12qiXyHy6bo3GjWbNmiYYNG5ZbHh0dLT744AOT19m+fbsAINRqtVi0aJH49ddfxfPPPy+Cg4PFkSNHzN7X9OnTBYByF+4GIk+SpsGapBkhBHf/EEnK52pWXnnlFZMDA/1Lbm6uQ4OVbdu2CQBi8uTJBstbtGgh0tPTza7TzZs3RWFhoe5y+vRpDlbIdewoBt2xQ4hPP7Vcr+HPGRbWEsnJ5zrYXrhwAZcuXbKYqVu3Lj777DNMmjQJly9f1i2/ffs2QkNDsWrVKgwYMKDc9fLy8lC3bl0sXboUTz75pG75wIEDERgYiM8//9ymdWSfFXIp9gJxHpu/EUnNHZ+bip7IMDo6GtHR0VZzHTp0wJUrV7Br1y60bdsWAPDDDz+grKwMSWb6bCckJKBmzZo4fPiwwfIjR46gV69ezq88kSO0Z41lMajjeKJCIr/jFWddbtKkCXr27IlRo0YhKysLt27dwvjx4/HEE0+gZs2aAIA//vgDDz74ID799FMkJiZCpVLhpZdewvTp09GqVSu0bt0an3zyCQ4dOoTVq1crvEXkt7Qfrtojgix82Mp2nh1pMhzwEfkfl+1QcrNLly6JQYMGicqVK4uIiAgxYsQIg6ZweXl5AoD48ccfDa43e/Zscc8994hKlSqJDh06iC1btth1v+yzQi5nQ2GobE3YFM+wPoXIa/hcga034GCFXM7KB69sTdhkyNyMjeeRP0Rewm+bwhH5lLQ0i31BZGvCJkNmd082fiPyZxysEClB29AsK6vcr2RrwqZkZgyykIcEVI8BG78R+TOXzdH4KO4GIrewUrciWxM2pTJ5iOfuHyIv43N9VrwB+6yQW9jQK0Sao28UzHTan4U6K9hThcibuONzk4MVKzhYIbdigzPT+LgQeS0OVhTAwQq5FTvamsbHhchrueNzkwW2REqycCbmnBxg6VLNV3N8NWPrGaqJyE+4rPrFR7HAltzORN8VxZuwKZFh4zcin8ACWwVwNxC5ndEuj5wc4L77ysd27LhbkOqTGe76IfIJ3A1E5IuMdnnI0IRNkQx3/RCRGV5xIkMin2Z0csOGbUwf/SJLozZXZzrtzwIS7hz5wxkVIjKBMytEMsjI0OwCychAUhLw8suGv37lFcOeJL6UqbPi7rYTEZnCmhUrWLNCHmGir4isjdpcnmFPFSKfwj4rCuBghTzKXz64/WU7ifwQBysK4GCFPMpfjojxl+0k8kM8GojI12mPiElONntWZn3SNXOzRnu26eRkHvlDRLZzWccWH8WmcKQIK2dlFkKiZm4u3i4i8m7u+NzkzAqRjKz0HMnJAd5+23DZ228bznp4MmORdjYlK4u9VIjIIRysEMkoLe1uLYeJ3UFSNXOzRu+wbN12saiWiOzAwQqRzPQ/6PXI0MzN3HIAnE0hIpfiYIVIZmYKbmVo5maujwoAzqYQkUvx0GUreOgyScHMob5SNXzT750CsI8KkZ9inxUFcLBCUtAOBJKTgexsOQcB7J1CRGCfFSL/pd2Vkp0t33l02DuFiNyMgxUib2Jn0ziP0NanZGezNoWI3IKDFSJvYjzDMmWKMoMWHu1DRB7EwQqRN9IOEIC7u4X0BxDuoH/7PNqHiDyIgxUib6QdIMyadXdWQzuAcOVsi7kBCmdTiMiDOFgh8mb6sxrOzrZos4MHWx+gcDaFiDyIhy5bwUOXyevo9zvRDjbUaiA8vHwPFP3vtdmAAKC01HDGRsZDpYlISuyzogAOVsiraQcuRUVAQYHhzIvx99qBicy9XIhIehysKICDFfIJ5rrL6n/PgQkRuQAHKwrgYIWIiMh27GBLREREfoeDFSIiIpIaBytEREQkNQ5WiIiISGocrBAREZHUOFghIiIiqXGwQkRERFLjYIWIiIikxsEKERERSY2DFSIiIpIaBytEREQkNQ5WiIiISGocrBAREZHUOFghIiIiqXnNYKWgoABDhgxBREQEoqKiMHLkSFy7ds3idfLz8zF06FDExsYiLCwM9957L9asWeOhNSYiIiJX8JrBypAhQ3DgwAFs3LgR69atw08//YTRo0dbvM6wYcNw+PBhfP3119i3bx8effRRpKSkYPfu3R5aayIiInKWSgghlF4Ja3Jzc9G0aVPs3LkT7dq1AwBs2LABvXv3xpkzZ1CzZk2T16tcuTIyMzMxdOhQ3bKqVavirbfewtNPP23TfV+9ehWRkZEoLCxERESE8xtDRETkw9zxuRnokltxs+3btyMqKko3UAGA7t27o0KFCsjJycGAAQNMXi85ORkrVqxAnz59EBUVhZUrV+LmzZvo2rWr2fsqLi5GcXGx7ufCwkIAmgefiIiILNN+XrpyLsQrBiv5+fmIiYkxWBYYGAi1Wo38/Hyz11u5ciUGDhyIqlWrIjAwEJUqVcLatWtRv359s9eZPXs2Zs6cWW55XFyc4xtARETkZy5duoTIyEiX3Jaig5X09HS89dZbFjO5ubkO3/7UqVNx5coVfP/996hWrRq+/PJLpKSkYMuWLWjRooXJ60yePBkTJ07U/XzlyhXEx8fj1KlTLnvQZXT16lXExcXh9OnTPru7yx+2EeB2+hp/2E5/2EbAf7azsLAQtWvXhlqtdtltKjpYmTRpEoYPH24xU7duXcTGxuL8+fMGy2/fvo2CggLExsaavN7x48cxf/587N+/H82aNQMAtGrVClu2bMGCBQuQlZVl8nohISEICQkptzwyMtKnX1xaERERPr+d/rCNALfT1/jDdvrDNgL+s50VKrjuGB5FByvR0dGIjo62muvQoQOuXLmCXbt2oW3btgCAH374AWVlZUhKSjJ5nRs3bgAo/2AFBASgrKzMyTUnIiIiT/GKQ5ebNGmCnj17YtSoUfj555+xbds2jB8/Hk888YTuSKA//vgDjRs3xs8//wwAaNy4MerXr48xY8bg559/xvHjx/Huu+9i48aN6N+/v4JbQ0RERPbwisEKAHz++edo3LgxHnzwQfTu3RudOnXCwoULdb+/desWDh8+rJtRCQoKwvr16xEdHY2+ffuiZcuW+PTTT/HJJ5+gd+/eNt9vSEgIpk+fbnLXkC/xh+30h20EuJ2+xh+20x+2EeB2OsMr+qwQERGR//KamRUiIiLyTxysEBERkdQ4WCEiIiKpcbBCREREUuNgxYSCggIMGTIEERERiIqKwsiRI3Ht2jWL18nPz8fQoUMRGxuLsLAw3HvvvVizZo2H1th+jmwjoDlP0wMPPICwsDBERETg/vvvx19//eWBNXaMo9sJaM5r0atXL6hUKnz55ZfuXVEn2budBQUFmDBhAho1aoSKFSuidu3aePbZZ3XnwpLFggULkJCQgNDQUCQlJelaE5izatUqNG7cGKGhoWjRogXWr1/voTV1jj3b+dFHH6Fz586oUqUKqlSpgu7du1t9XGRg73OptXz5cqhUKq9pOWHvdl65cgXjxo1DjRo1EBISgoYNG3rF69be7Zw7d67u/SYuLg4vvPACbt68afsdCiqnZ8+eolWrVmLHjh1iy5Yton79+mLQoEEWr/PQQw+J9u3bi5ycHHH8+HHx+uuviwoVKohff/3VQ2ttH0e2MTs7W0RERIjZs2eL/fv3i0OHDokVK1aImzdvemit7efIdmr94x//EL169RIAxNq1a927ok6ydzv37dsnHn30UfH111+LY8eOiU2bNokGDRqIxx57zINrbdny5ctFcHCwWLRokThw4IAYNWqUiIqKEufOnTOZ37ZtmwgICBBvv/22OHjwoHj11VdFUFCQ2Ldvn4fX3D72bufgwYPFggULxO7du0Vubq4YPny4iIyMFGfOnPHwmtvO3m3UysvLE7Vq1RKdO3cW/fr188zKOsHe7SwuLhbt2rUTvXv3Flu3bhV5eXli8+bNYs+ePR5ec/vYu52ff/65CAkJEZ9//rnIy8sT3333nahRo4Z44YUXbL5PDlaMHDx4UAAQO3fu1C379ttvhUqlEn/88YfZ64WFhYlPP/3UYJlarRYfffSR29bVUY5uY1JSknj11Vc9sYou4eh2CiHE7t27Ra1atcTZs2elH6w4s536Vq5cKYKDg8WtW7fcsZp2S0xMFOPGjdP9XFpaKmrWrClmz55tMp+SkiL69OljsCwpKUmMGTPGrevpLHu309jt27dFeHi4+OSTT9y1ik5zZBtv374tkpOTxccffyxSU1O9YrBi73ZmZmaKunXripKSEk+tokvYu53jxo0TDzzwgMGyiRMnio4dO9p8n9wNZGT79u2IiopCu3btdMu6d++OChUqICcnx+z1kpOTsWLFChQUFKCsrAzLly/HzZs30bVrVw+stX0c2cbz588jJycHMTExSE5ORvXq1dGlSxds3brVU6ttN0efyxs3bmDw4MFYsGCB2XNPycTR7TRWWFiIiIgIBAYqfzL2kpIS7Nq1C927d9ctq1ChArp3747t27ebvM727dsN8gDQo0cPs3kZOLKdxm7cuIFbt2659KRxruToNr722muIiYnByJEjPbGaTnNkO7/++mt06NAB48aNQ/Xq1dG8eXO8+eabKC0t9dRq282R7UxOTsauXbt0u4pOnDiB9evX29WgVfl3Jcnk5+cjJibGYFlgYCDUajXy8/PNXm/lypUYOHAgqlatisDAQFSqVAlr165F/fr13b3KdnNkG0+cOAEAmDFjBubMmYPWrVvj008/xYMPPoj9+/ejQYMGbl9vezn6XL7wwgtITk5Gv3793L2KLuHoduq7ePEiXn/9dYwePdodq2i3ixcvorS0FNWrVzdYXr16dRw6dMjkdfLz803mbX0MlODIdhp75ZVXULNmzXIDNVk4so1bt27Fv/71L+zZs8cDa+gajmzniRMn8MMPP2DIkCFYv349jh07hmeeeQa3bt3C9OnTPbHadnNkOwcPHoyLFy+iU6dOEELg9u3bSEtLw//93//ZfL9+M7OSnp4OlUpl8WLrm4MpU6dOxZUrV/D999/jl19+wcSJE5GSkoJ9+/a5cCssc+c2ak/+OGbMGIwYMQJt2rTBe++9h0aNGmHRokWu3Ayr3LmdX3/9NX744QfMnTvXtSvtAHe/ZrWuXr2KPn36oGnTppgxY4bzK04ek5GRgeXLl2Pt2rUIDQ1VenVcoqioCEOHDsVHH32EatWqKb06blVWVoaYmBgsXLgQbdu2xcCBAzFlyhRkZWUpvWoutXnzZrz55pv44IMP8Ouvv+Lf//43vvnmG7z++us234bfzKxMmjQJw4cPt5ipW7cuYmNjcf78eYPlt2/fRkFBgdldAsePH8f8+fOxf/9+NGvWDADQqlUrbNmyBQsWLPDYC8+d21ijRg0AQNOmTQ2WN2nSBKdOnXJ8pR3gzu384YcfcPz4cURFRRksf+yxx9C5c2ds3rzZiTW3jzu3U6uoqAg9e/ZEeHg41q5di6CgIGdX2yWqVauGgIAAnDt3zmD5uXPnzG5TbGysXXkZOLKdWnPmzEFGRga+//57tGzZ0p2r6RR7t/H48eM4efIk+vbtq1um/WcpMDAQhw8fRr169dy70g5w5LmsUaMGgoKCEBAQoFvWpEkT5Ofno6SkBMHBwW5dZ0c4sp1Tp07F0KFD8fTTTwMAWrRogevXr2P06NGYMmUKKlSwPm/iN4OV6OhoREdHW8116NABV65cwa5du9C2bVsAmg+wsrIyJCUlmbyO9uSJxg94QECA7o/ME9y5jQkJCahZsyYOHz5ssPzIkSPo1auX8ytvB3duZ3p6uu4PSqtFixZ47733DN48PcGd2wloZlR69OiBkJAQfP3111L9Zx4cHIy2bdti06ZNukNWy8rKsGnTJowfP97kdTp06IBNmzbh+eef1y3buHEjOnTo4IE1dowj2wkAb7/9NmbNmoXvvvvOoFZJRvZuY+PGjcvNSL/66qsoKirC+++/j7i4OE+stt0ceS47duyIZcuWoaysTPf5ceTIEdSoUUPKgQrg2HbeuHHD5OcjoGkRYRNHKoF9Xc+ePUWbNm1ETk6O2Lp1q2jQoIHBYaBnzpwRjRo1Ejk5OUIIIUpKSkT9+vVF586dRU5Ojjh27JiYM2eOUKlU4ptvvlFqMyyydxuFEOK9994TERERYtWqVeLo0aPi1VdfFaGhoeLYsWNKbIJNHNlOY5D8aCAh7N/OwsJCkZSUJFq0aCGOHTsmzp49q7vcvn1bqc0wsHz5chESEiKWLFkiDh48KEaPHi2ioqJEfn6+EEKIoUOHivT0dF1+27ZtIjAwUMyZM0fk5uaK6dOne82hy/ZsZ0ZGhggODharV682eN6KioqU2gSr7N1GY95yNJC923nq1CkRHh4uxo8fLw4fPizWrVsnYmJixBtvvKHUJtjE3u2cPn26CA8PF1988YU4ceKE+O9//yvq1asnUlJSbL5PDlZMuHTpkhg0aJCoXLmyiIiIECNGjDB4I8jLyxMAxI8//qhbduTIEfHoo4+KmJgYUalSJdGyZctyhzLLxJFtFEKI2bNni3vuuUdUqlRJdOjQQWzZssXDa24fR7dTnzcMVuzdzh9//FEAMHnJy8tTZiNMmDdvnqhdu7YIDg4WiYmJYseOHbrfdenSRaSmphrkV65cKRo2bCiCg4NFs2bNpP1nwZg92xkfH2/yeZs+fbrnV9wO9j6X+rxlsCKE/duZnZ0tkpKSREhIiKhbt66YNWuWNP8wWGLPdt66dUvMmDFD1KtXT4SGhoq4uDjxzDPPiMuXL9t8fyohbJ2DISIiIvI8vzkaiIiIiLwTBytEREQkNQ5WiIiISGocrBAREZHUOFghIiIiqXGwQkRERFLjYIWIiIikxsEKERERSY2DFSIiIpIaBytEREQkNQ5WiIiISGocrBCR17hw4QJiY2Px5ptv6pZlZ2cjODgYmzZtUnDNiMideCJDIvIq69evR//+/ZGdnY1GjRqhdevW6NevH/7xj38ovWpE5CYcrBCR1xk3bhy+//57tGvXDvv27cPOnTsREhKi9GoRkZtwsEJEXuevv/5C8+bNcfr0aezatQstWrRQepWIyI1Ys0JEXuf48eP4888/UVZWhpMnTyq9OkTkZpxZISKvUlJSgsTERLRu3RqNGjXC3LlzsW/fPsTExCi9akTkJhysEJFXeemll7B69Wrs3bsXlStXRpcuXRAZGYl169YpvWpE5CbcDUREXmPz5s2YO3culi5dioiICFSoUAFLly7Fli1bkJmZqfTqEZGbcGaFiIiIpMaZFSIiIpIaBytEREQkNQ5WiIiISGocrBAREZHUOFghIiIiqXGwQkRERFLjYIWIiIikxsEKERERSY2DFSIiIpIaBytEREQkNQ5WiIiISGr/D+Om09YMTI4GAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h, s_for_integral, x_bd, x_interior = generate_flower_points(N, True)\n",
    "s_for_intergal_pt = torch.from_numpy(s_for_integral).clone().float().to(device).view(-1, 1)\n",
    "x_bd_pt = torch.from_numpy(x_bd).clone().float().to(device)\n",
    "x_interior_pt = torch.from_numpy(x_interior).clone().float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point3(theta):\n",
    "    temp1 = 0.6 * (1 + 0.25 * np.sin(6 * theta))\n",
    "\n",
    "    x = temp1 * np.cos(theta)\n",
    "    y = temp1 * np.sin(theta)\n",
    "\n",
    "    temp2 = 0.6 + 0.15 * np.sin(6 * theta)\n",
    "\n",
    "    dx = -temp2 * np.sin(theta) + 0.9 * np.cos(theta) * np.cos(6 * theta)\n",
    "    dy = temp2 * np.cos(theta) + 0.9 * np.sin(theta) * np.cos(6 * theta)\n",
    "\n",
    "    temp3 = -5.55 * np.sin(6 * theta) - 0.6\n",
    "    ddx = temp3 * np.cos(theta) - 1.8 * np.sin(theta) * np.cos(6 * theta)\n",
    "    ddy = temp3 * np.sin(theta) + 1.8 * np.cos(theta) * np.cos(6 * theta)\n",
    "\n",
    "    return x, y, dx, dy, ddx, ddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import kv, kvp\n",
    "\n",
    "def kernel_bd(tau, t, s):\n",
    "    x1_s, x2_s, dx1_s, dx2_s, ddx1_s, ddx2_s = get_point3(s)\n",
    "    x1_t, x2_t, dx1_t, dx2_t, ddx1_t, ddx2_t = get_point3(t)\n",
    "\n",
    "    if (s != t):\n",
    "        r = np.sqrt((x1_s - x1_t) ** 2 + (x2_s - x2_t) ** 2)\n",
    "        result = - kvp(0, r / np.sqrt(tau), n=1) * (dx2_s * (x1_s - x1_t) - dx1_s * (x2_s - x2_t)) / (np.sqrt(tau) * r)\n",
    "        return result\n",
    "    else:\n",
    "        return 0.5 * (ddx2_t * dx1_t - ddx1_t * dx2_t) / (dx1_t * dx1_t + dx2_t * dx2_t);\n",
    "    \n",
    "kernels_for_bd = torch.zeros(len(taus), len(x_bd), len(x_bd))\n",
    "for k in range(len(taus)):\n",
    "    for i in range(len(x_bd)):\n",
    "        for j in range(len(x_bd)):\n",
    "            kernels_for_bd[k, i, j] = h * kernel_bd(taus[k], s_for_integral[i, 0], s_for_integral[j, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_u_bd(net, tau, g):      # tau: scalar, g: (batch_size, n_g)\n",
    "    flag = 0\n",
    "    for i in range(len(taus)):\n",
    "        if np.abs(tau - taus[i]) < 1e-6:\n",
    "            kernel = kernels_for_bd[i, :, :]\n",
    "            flag = 1\n",
    "            # print(taus[i])\n",
    "            \n",
    "    if flag == 0 and tau > 0:\n",
    "        kernel = torch.zeros(len(x_bd), len(x_bd))\n",
    "        for i in range(len(x_bd)):\n",
    "            for j in range(len(x_bd)):\n",
    "                kernel[i, j] = h * kernel_bd(tau, s_for_integral[i, 0], s_for_integral[j, 0])\n",
    "    else:\n",
    "        exit(\"The value of tau is illegal!\")\n",
    "\n",
    "    kernel = kernel.to(device)\n",
    "    result = 0.5 * net(tau, g).T + 0.5 / (np.pi) * kernel @ net(tau, g).T \n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kernel_int(tau, x, s):\n",
    "#     x1, x2 = x[0], x[1]\n",
    "    \n",
    "#     x1_s, x2_s, dx1_s, dx2_s, ddx1_s, ddx2_s = get_point3(s)\n",
    "\n",
    "#     r = np.sqrt((x1_s - x1) ** 2 + (x2_s - x2) ** 2)\n",
    "#     result = - kvp(0, r / np.sqrt(tau), n=1) * (dx2_s * (x1_s - x1) - dx1_s * (x2_s - x2)) / (np.sqrt(tau) * r)\n",
    "#     return result\n",
    "\n",
    "# kernels_for_int = torch.zeros(len(taus), len(x_interior), len(x_bd))\n",
    "# for k in range(len(taus)):\n",
    "#     for i in range(len(x_interior)):\n",
    "#         for j in range(len(x_bd)):\n",
    "#             kernels_for_int[k, i, j] = h * kernel_int(taus[k], x_interior[i, :], s_for_integral[j, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_u_int(net, tau, g):      # tau: scalar, g: (batch_size, 8 * N)\n",
    "#     flag = 0\n",
    "#     for i in range(len(taus)):\n",
    "#         if np.abs(tau - taus[i]) < 1e-6:\n",
    "#             kernel = kernels_for_int[i, :, :]\n",
    "#             flag = 1\n",
    "#             # print(taus[i])\n",
    "    \n",
    "#     if flag == 0 and tau > 0:\n",
    "#         kernel = torch.zeros(len(x_interior), len(x_bd))\n",
    "#         for i in range(len(x_interior)):\n",
    "#             for j in range(len(x_bd)):\n",
    "#                 kernel[i, j] = h * kernel_int(tau, x_interior[i, :], s_for_integral[j, 0])\n",
    "#     else:\n",
    "#         exit(\"The value of tau is illegal!\")\n",
    "\n",
    "#     kernel = kernel.to(device)\n",
    "\n",
    "#     result = 0.5 / (np.pi) * kernel @ net(tau, g).T\n",
    "#     return result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def generate_exact_solution_1(num_samples, n, sigma):\n",
    "    g_list = []\n",
    "    for _ in range(num_samples):\n",
    "        g = np.random.randn(8 * n)\n",
    "        g = gaussian_filter(g, sigma=sigma)\n",
    "        g_list.append(g)\n",
    "    return g_list\n",
    "\n",
    "def generate_exact_solution_2(num_samples):\n",
    "    g_list = []\n",
    "    x = x_bd[:, 0].reshape(-1)\n",
    "    y = x_bd[:, 1].reshape(-1)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        g = a * np.sin(b * np.pi * x) * np.sin(c * np.pi * y)\n",
    "        g_list.append(g)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        g = a * np.cos(b * np.pi * x) * np.cos(c * np.pi * y)\n",
    "        g_list.append(g)\n",
    "\n",
    "    for _ in range(num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        g = a * np.sin(b * np.pi * x) * np.cos(c * np.pi * y)\n",
    "        g_list.append(g)\n",
    "\n",
    "    for _ in range(num_samples - num_samples // 4 - num_samples // 4 - num_samples // 4):\n",
    "        a = np.random.uniform(0.5, 4.5)\n",
    "        b = np.random.uniform(0.5, 4.5)\n",
    "        c = np.random.uniform(0.5, 4.5)\n",
    "        g = a * np.cos(b * np.pi * x) * np.sin(c * np.pi * y)\n",
    "        g_list.append(g)\n",
    "\n",
    "    return g_list\n",
    "\n",
    "def generate_g(num_of_samples):\n",
    "\n",
    "    g_1 = generate_exact_solution_1(num_of_samples // 10, N, 1)\n",
    "    g_2 = generate_exact_solution_1(num_of_samples // 10, N, 2)\n",
    "    g_3 = generate_exact_solution_1(num_of_samples // 10, N, 3)\n",
    "    g_4 = generate_exact_solution_1(num_of_samples // 10, N, 4)\n",
    "    g_5 = generate_exact_solution_1(num_of_samples // 10, N, 5)\n",
    "    g_6 = generate_exact_solution_2(num_of_samples - 5 * (num_of_samples // 10))\n",
    "    \n",
    "    g_list = g_1 + g_2 + g_3 + g_4 + g_5 + g_6\n",
    "        \n",
    "    return np.array(g_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_np = generate_g(10000)\n",
    "# g_np = np.random.rand(10000, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(g_np, tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "g_pt = torch.tensor(g_np, dtype=torch.float32).to(device)\n",
    "dataset = TensorDataset(g_pt)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_g = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "test_g = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DeepONet(N * 8, N * 8, N * 8, 2).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "parameters = net.parameters()\n",
    "optimizer = torch.optim.Adam(parameters, lr = 1e-3)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', patience = 1000, factor = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_loss(net, g):\n",
    "#     loss = criterion(predict_u_bd(net, taus[0], g), g)\n",
    "#     for i in range(1, len(taus)):\n",
    "#         loss += criterion(predict_u_bd(net, taus[i], g), g)\n",
    "#     return loss / len(taus)\n",
    "\n",
    "def compute_loss(net, g):\n",
    "    predictions = torch.cat([predict_u_bd(net, tau, g).unsqueeze(0) for tau in taus], dim=0)\n",
    "    targets = g.unsqueeze(0).repeat(len(taus), 1, 1)\n",
    "    # print(predictions.shape)\n",
    "    # print(targets.shape)\n",
    "    loss = criterion(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100000] Training Loss: 0.04381882 Testing Loss: 0.04285311 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [100/100000] Training Loss: 0.02709929 Testing Loss: 0.02695389 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [150/100000] Training Loss: 0.01826629 Testing Loss: 0.01870794 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [200/100000] Training Loss: 0.01197476 Testing Loss: 0.01287911 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [250/100000] Training Loss: 0.00810322 Testing Loss: 0.00921836 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [300/100000] Training Loss: 0.00579885 Testing Loss: 0.00693710 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [350/100000] Training Loss: 0.00436469 Testing Loss: 0.00544276 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [400/100000] Training Loss: 0.00342455 Testing Loss: 0.00442041 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [450/100000] Training Loss: 0.00277717 Testing Loss: 0.00369117 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [500/100000] Training Loss: 0.00231832 Testing Loss: 0.00315662 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [550/100000] Training Loss: 0.00198452 Testing Loss: 0.00275642 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [600/100000] Training Loss: 0.00171863 Testing Loss: 0.00242355 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [650/100000] Training Loss: 0.00153581 Testing Loss: 0.00218533 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [700/100000] Training Loss: 0.00140479 Testing Loss: 0.00200965 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [750/100000] Training Loss: 0.00125191 Testing Loss: 0.00180403 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [800/100000] Training Loss: 0.00116901 Testing Loss: 0.00167667 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [850/100000] Training Loss: 0.00129459 Testing Loss: 0.00176940 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [900/100000] Training Loss: 0.00098799 Testing Loss: 0.00142795 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [950/100000] Training Loss: 0.00093325 Testing Loss: 0.00134524 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1000/100000] Training Loss: 0.00088153 Testing Loss: 0.00127262 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1050/100000] Training Loss: 0.00102049 Testing Loss: 0.00142777 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1100/100000] Training Loss: 0.00081029 Testing Loss: 0.00115165 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1150/100000] Training Loss: 0.00104050 Testing Loss: 0.00112824 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1200/100000] Training Loss: 0.00073900 Testing Loss: 0.00104874 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1250/100000] Training Loss: 0.00077749 Testing Loss: 0.00108931 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1300/100000] Training Loss: 0.00096717 Testing Loss: 0.00165612 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1350/100000] Training Loss: 0.00067830 Testing Loss: 0.00094716 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1400/100000] Training Loss: 0.00087208 Testing Loss: 0.00115781 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1450/100000] Training Loss: 0.00279445 Testing Loss: 0.00306801 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1500/100000] Training Loss: 0.00063264 Testing Loss: 0.00087027 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1550/100000] Training Loss: 0.00061244 Testing Loss: 0.00084271 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1600/100000] Training Loss: 0.00063335 Testing Loss: 0.00088194 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1650/100000] Training Loss: 0.00059330 Testing Loss: 0.00080890 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1700/100000] Training Loss: 0.00061672 Testing Loss: 0.00083833 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1750/100000] Training Loss: 0.00062467 Testing Loss: 0.00083294 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1800/100000] Training Loss: 0.00087845 Testing Loss: 0.00134662 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1850/100000] Training Loss: 0.00055566 Testing Loss: 0.00074548 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1900/100000] Training Loss: 0.00054493 Testing Loss: 0.00072844 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [1950/100000] Training Loss: 0.00084867 Testing Loss: 0.00157144 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2000/100000] Training Loss: 0.00053653 Testing Loss: 0.00071056 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2050/100000] Training Loss: 0.00052413 Testing Loss: 0.00069292 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2100/100000] Training Loss: 0.00064927 Testing Loss: 0.00080510 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2150/100000] Training Loss: 0.00056439 Testing Loss: 0.00071669 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2200/100000] Training Loss: 0.00050517 Testing Loss: 0.00066130 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2250/100000] Training Loss: 0.00056680 Testing Loss: 0.00073151 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2300/100000] Training Loss: 0.00053181 Testing Loss: 0.00067545 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2350/100000] Training Loss: 0.00048454 Testing Loss: 0.00062966 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2400/100000] Training Loss: 0.00074738 Testing Loss: 0.00086829 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2450/100000] Training Loss: 0.00051768 Testing Loss: 0.00067871 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2500/100000] Training Loss: 0.00045242 Testing Loss: 0.00058744 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2550/100000] Training Loss: 0.00045306 Testing Loss: 0.00058541 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2600/100000] Training Loss: 0.00052237 Testing Loss: 0.00061546 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2650/100000] Training Loss: 0.00050157 Testing Loss: 0.00071477 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2700/100000] Training Loss: 0.00039227 Testing Loss: 0.00052439 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2750/100000] Training Loss: 0.00120462 Testing Loss: 0.00158578 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2800/100000] Training Loss: 0.00030298 Testing Loss: 0.00041929 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2850/100000] Training Loss: 0.00068528 Testing Loss: 0.00077757 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2900/100000] Training Loss: 0.00030746 Testing Loss: 0.00042312 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [2950/100000] Training Loss: 0.00023913 Testing Loss: 0.00035974 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3000/100000] Training Loss: 0.00025490 Testing Loss: 0.00039866 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3050/100000] Training Loss: 0.00029661 Testing Loss: 0.00041707 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3100/100000] Training Loss: 0.00016027 Testing Loss: 0.00026315 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3150/100000] Training Loss: 0.00017774 Testing Loss: 0.00031817 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3200/100000] Training Loss: 0.00115480 Testing Loss: 0.00147886 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3250/100000] Training Loss: 0.00013593 Testing Loss: 0.00023372 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3300/100000] Training Loss: 0.00056927 Testing Loss: 0.00084266 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3350/100000] Training Loss: 0.00015212 Testing Loss: 0.00024663 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3400/100000] Training Loss: 0.00011819 Testing Loss: 0.00020762 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3450/100000] Training Loss: 0.00058485 Testing Loss: 0.00083953 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3500/100000] Training Loss: 0.00012188 Testing Loss: 0.00021110 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3550/100000] Training Loss: 0.00188323 Testing Loss: 0.00236153 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3600/100000] Training Loss: 0.00010214 Testing Loss: 0.00018861 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3650/100000] Training Loss: 0.00010815 Testing Loss: 0.00019291 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3700/100000] Training Loss: 0.00068911 Testing Loss: 0.00088504 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3750/100000] Training Loss: 0.00009289 Testing Loss: 0.00017463 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3800/100000] Training Loss: 0.00031236 Testing Loss: 0.00044340 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3850/100000] Training Loss: 0.00010097 Testing Loss: 0.00020724 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3900/100000] Training Loss: 0.00008852 Testing Loss: 0.00016605 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [3950/100000] Training Loss: 0.00059191 Testing Loss: 0.00085876 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4000/100000] Training Loss: 0.00009060 Testing Loss: 0.00016426 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4050/100000] Training Loss: 0.00007943 Testing Loss: 0.00015421 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4100/100000] Training Loss: 0.00012781 Testing Loss: 0.00019385 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4150/100000] Training Loss: 0.00007523 Testing Loss: 0.00014749 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4200/100000] Training Loss: 0.00022483 Testing Loss: 0.00031974 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4250/100000] Training Loss: 0.00007482 Testing Loss: 0.00014590 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4300/100000] Training Loss: 0.00125329 Testing Loss: 0.00202104 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4350/100000] Training Loss: 0.00007699 Testing Loss: 0.00014437 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4400/100000] Training Loss: 0.00013073 Testing Loss: 0.00018744 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4450/100000] Training Loss: 0.00010138 Testing Loss: 0.00018314 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4500/100000] Training Loss: 0.00008835 Testing Loss: 0.00015450 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4550/100000] Training Loss: 0.00006510 Testing Loss: 0.00013048 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4600/100000] Training Loss: 0.00016707 Testing Loss: 0.00023728 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4650/100000] Training Loss: 0.00075759 Testing Loss: 0.00093516 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4700/100000] Training Loss: 0.00006665 Testing Loss: 0.00012935 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4750/100000] Training Loss: 0.00025489 Testing Loss: 0.00024025 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4800/100000] Training Loss: 0.00006148 Testing Loss: 0.00012290 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4850/100000] Training Loss: 0.00009609 Testing Loss: 0.00016377 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4900/100000] Training Loss: 0.00006675 Testing Loss: 0.00012105 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [4950/100000] Training Loss: 0.00016271 Testing Loss: 0.00021885 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5000/100000] Training Loss: 0.00057048 Testing Loss: 0.00041444 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5050/100000] Training Loss: 0.00005703 Testing Loss: 0.00011485 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5100/100000] Training Loss: 0.00007081 Testing Loss: 0.00012379 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5150/100000] Training Loss: 0.00014473 Testing Loss: 0.00028494 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5200/100000] Training Loss: 0.00005478 Testing Loss: 0.00011070 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5250/100000] Training Loss: 0.00020375 Testing Loss: 0.00025261 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5300/100000] Training Loss: 0.00008573 Testing Loss: 0.00014448 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5350/100000] Training Loss: 0.00005083 Testing Loss: 0.00010508 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5400/100000] Training Loss: 0.00064486 Testing Loss: 0.00065556 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5450/100000] Training Loss: 0.00005642 Testing Loss: 0.00011226 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5500/100000] Training Loss: 0.00004976 Testing Loss: 0.00010271 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5550/100000] Training Loss: 0.00010034 Testing Loss: 0.00014634 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5600/100000] Training Loss: 0.00004824 Testing Loss: 0.00009989 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5650/100000] Training Loss: 0.00014395 Testing Loss: 0.00021138 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5700/100000] Training Loss: 0.00005718 Testing Loss: 0.00010641 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5750/100000] Training Loss: 0.00013668 Testing Loss: 0.00017579 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5800/100000] Training Loss: 0.00004995 Testing Loss: 0.00010093 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5850/100000] Training Loss: 0.00006631 Testing Loss: 0.00012154 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5900/100000] Training Loss: 0.00017396 Testing Loss: 0.00018231 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [5950/100000] Training Loss: 0.00009490 Testing Loss: 0.00016004 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6000/100000] Training Loss: 0.00006548 Testing Loss: 0.00011713 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6050/100000] Training Loss: 0.00004600 Testing Loss: 0.00009433 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6100/100000] Training Loss: 0.00008933 Testing Loss: 0.00013823 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6150/100000] Training Loss: 0.00005330 Testing Loss: 0.00010439 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6200/100000] Training Loss: 0.00006043 Testing Loss: 0.00011307 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6250/100000] Training Loss: 0.00026287 Testing Loss: 0.00039770 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6300/100000] Training Loss: 0.00096259 Testing Loss: 0.00093218 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6350/100000] Training Loss: 0.00004542 Testing Loss: 0.00008959 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6400/100000] Training Loss: 0.00026903 Testing Loss: 0.00042632 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6450/100000] Training Loss: 0.00004945 Testing Loss: 0.00009436 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6500/100000] Training Loss: 0.00004037 Testing Loss: 0.00008332 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6550/100000] Training Loss: 0.00163961 Testing Loss: 0.00139419 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6600/100000] Training Loss: 0.00004310 Testing Loss: 0.00008658 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6650/100000] Training Loss: 0.00004315 Testing Loss: 0.00008485 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6700/100000] Training Loss: 0.00081773 Testing Loss: 0.00139037 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6750/100000] Training Loss: 0.00004442 Testing Loss: 0.00008590 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6800/100000] Training Loss: 0.00008476 Testing Loss: 0.00015022 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6850/100000] Training Loss: 0.00005991 Testing Loss: 0.00010747 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6900/100000] Training Loss: 0.00005708 Testing Loss: 0.00009720 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [6950/100000] Training Loss: 0.00005601 Testing Loss: 0.00011144 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7000/100000] Training Loss: 0.00015395 Testing Loss: 0.00022369 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7050/100000] Training Loss: 0.00003484 Testing Loss: 0.00007465 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7100/100000] Training Loss: 0.00015291 Testing Loss: 0.00021449 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7150/100000] Training Loss: 0.00005684 Testing Loss: 0.00010204 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7200/100000] Training Loss: 0.00003370 Testing Loss: 0.00007261 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7250/100000] Training Loss: 0.00056358 Testing Loss: 0.00075387 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7300/100000] Training Loss: 0.00003741 Testing Loss: 0.00007629 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7350/100000] Training Loss: 0.00010945 Testing Loss: 0.00016924 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7400/100000] Training Loss: 0.00011974 Testing Loss: 0.00014896 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7450/100000] Training Loss: 0.00003279 Testing Loss: 0.00007018 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7500/100000] Training Loss: 0.00012003 Testing Loss: 0.00019806 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7550/100000] Training Loss: 0.00004670 Testing Loss: 0.00008559 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7600/100000] Training Loss: 0.00003115 Testing Loss: 0.00006782 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7650/100000] Training Loss: 0.00225823 Testing Loss: 0.00196615 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7700/100000] Training Loss: 0.00003570 Testing Loss: 0.00007135 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7750/100000] Training Loss: 0.00016875 Testing Loss: 0.00014521 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7800/100000] Training Loss: 0.00108174 Testing Loss: 0.00116450 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7850/100000] Training Loss: 0.00003347 Testing Loss: 0.00006861 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7900/100000] Training Loss: 0.00010100 Testing Loss: 0.00009981 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [7950/100000] Training Loss: 0.00012928 Testing Loss: 0.00015096 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8000/100000] Training Loss: 0.00002985 Testing Loss: 0.00006477 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8050/100000] Training Loss: 0.00004378 Testing Loss: 0.00008721 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8100/100000] Training Loss: 0.00075834 Testing Loss: 0.00104299 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8150/100000] Training Loss: 0.00003164 Testing Loss: 0.00006522 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8200/100000] Training Loss: 0.00009538 Testing Loss: 0.00013357 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8250/100000] Training Loss: 0.00005201 Testing Loss: 0.00008324 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8300/100000] Training Loss: 0.00002771 Testing Loss: 0.00006107 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8350/100000] Training Loss: 0.00119959 Testing Loss: 0.00069682 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8400/100000] Training Loss: 0.00003053 Testing Loss: 0.00006371 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8450/100000] Training Loss: 0.00002693 Testing Loss: 0.00005966 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8500/100000] Training Loss: 0.00212010 Testing Loss: 0.00200640 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8550/100000] Training Loss: 0.00003078 Testing Loss: 0.00006378 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8600/100000] Training Loss: 0.00005899 Testing Loss: 0.00010468 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8650/100000] Training Loss: 0.00004392 Testing Loss: 0.00007831 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8700/100000] Training Loss: 0.00004466 Testing Loss: 0.00007464 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8750/100000] Training Loss: 0.00013282 Testing Loss: 0.00014186 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8800/100000] Training Loss: 0.00016208 Testing Loss: 0.00021549 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8850/100000] Training Loss: 0.00006537 Testing Loss: 0.00010772 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8900/100000] Training Loss: 0.00139527 Testing Loss: 0.00096149 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [8950/100000] Training Loss: 0.00002912 Testing Loss: 0.00005994 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9000/100000] Training Loss: 0.00006686 Testing Loss: 0.00008730 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9050/100000] Training Loss: 0.00005567 Testing Loss: 0.00009989 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9100/100000] Training Loss: 0.00003473 Testing Loss: 0.00006584 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9150/100000] Training Loss: 0.00004545 Testing Loss: 0.00007588 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9200/100000] Training Loss: 0.00006480 Testing Loss: 0.00009794 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9250/100000] Training Loss: 0.00003326 Testing Loss: 0.00006593 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9300/100000] Training Loss: 0.00005324 Testing Loss: 0.00008407 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9350/100000] Training Loss: 0.00008810 Testing Loss: 0.00014303 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9400/100000] Training Loss: 0.00003172 Testing Loss: 0.00006072 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9450/100000] Training Loss: 0.00019219 Testing Loss: 0.00028792 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9500/100000] Training Loss: 0.00003069 Testing Loss: 0.00005918 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9550/100000] Training Loss: 0.00002662 Testing Loss: 0.00005734 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9600/100000] Training Loss: 0.00006097 Testing Loss: 0.00008535 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9650/100000] Training Loss: 0.00003644 Testing Loss: 0.00006932 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9700/100000] Training Loss: 0.00003761 Testing Loss: 0.00006521 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9750/100000] Training Loss: 0.00003488 Testing Loss: 0.00006343 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9800/100000] Training Loss: 0.00004450 Testing Loss: 0.00007199 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9850/100000] Training Loss: 0.00003805 Testing Loss: 0.00005909 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9900/100000] Training Loss: 0.00008858 Testing Loss: 0.00010420 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [9950/100000] Training Loss: 0.00012267 Testing Loss: 0.00015215 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10000/100000] Training Loss: 0.00102426 Testing Loss: 0.00117325 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10050/100000] Training Loss: 0.00002607 Testing Loss: 0.00005359 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10100/100000] Training Loss: 0.00008582 Testing Loss: 0.00011474 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10150/100000] Training Loss: 0.00070424 Testing Loss: 0.00069843 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10200/100000] Training Loss: 0.00002338 Testing Loss: 0.00005047 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10250/100000] Training Loss: 0.00013058 Testing Loss: 0.00020235 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10300/100000] Training Loss: 0.00005948 Testing Loss: 0.00009959 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10350/100000] Training Loss: 0.00002935 Testing Loss: 0.00005509 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10400/100000] Training Loss: 0.00003480 Testing Loss: 0.00006618 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10450/100000] Training Loss: 0.00007997 Testing Loss: 0.00009671 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10500/100000] Training Loss: 0.00002079 Testing Loss: 0.00004699 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10550/100000] Training Loss: 0.00010889 Testing Loss: 0.00009815 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10600/100000] Training Loss: 0.00003735 Testing Loss: 0.00006344 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10650/100000] Training Loss: 0.00002195 Testing Loss: 0.00004810 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10700/100000] Training Loss: 0.00005180 Testing Loss: 0.00007440 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10750/100000] Training Loss: 0.00008177 Testing Loss: 0.00011880 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10800/100000] Training Loss: 0.00002757 Testing Loss: 0.00005321 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10850/100000] Training Loss: 0.00013614 Testing Loss: 0.00016465 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10900/100000] Training Loss: 0.00003043 Testing Loss: 0.00005530 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [10950/100000] Training Loss: 0.00003716 Testing Loss: 0.00005940 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11000/100000] Training Loss: 0.00003062 Testing Loss: 0.00005544 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11050/100000] Training Loss: 0.00004691 Testing Loss: 0.00007161 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11100/100000] Training Loss: 0.00003605 Testing Loss: 0.00005565 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11150/100000] Training Loss: 0.00009728 Testing Loss: 0.00010164 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11200/100000] Training Loss: 0.00001987 Testing Loss: 0.00004449 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11250/100000] Training Loss: 0.00007621 Testing Loss: 0.00010326 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11300/100000] Training Loss: 0.00005356 Testing Loss: 0.00009038 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11350/100000] Training Loss: 0.00002703 Testing Loss: 0.00005129 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11400/100000] Training Loss: 0.00048424 Testing Loss: 0.00054832 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11450/100000] Training Loss: 0.00002151 Testing Loss: 0.00004549 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11500/100000] Training Loss: 0.00010177 Testing Loss: 0.00010293 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11550/100000] Training Loss: 0.00003178 Testing Loss: 0.00005925 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11600/100000] Training Loss: 0.00002966 Testing Loss: 0.00005255 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11650/100000] Training Loss: 0.00011686 Testing Loss: 0.00017238 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11700/100000] Training Loss: 0.00002845 Testing Loss: 0.00005218 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11750/100000] Training Loss: 0.00001784 Testing Loss: 0.00004125 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11800/100000] Training Loss: 0.00010564 Testing Loss: 0.00013557 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11850/100000] Training Loss: 0.00001825 Testing Loss: 0.00004147 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11900/100000] Training Loss: 0.00012516 Testing Loss: 0.00019358 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [11950/100000] Training Loss: 0.00002479 Testing Loss: 0.00004737 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12000/100000] Training Loss: 0.00001919 Testing Loss: 0.00004278 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12050/100000] Training Loss: 0.00003463 Testing Loss: 0.00005743 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12100/100000] Training Loss: 0.00026835 Testing Loss: 0.00040642 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12150/100000] Training Loss: 0.00002044 Testing Loss: 0.00004300 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12200/100000] Training Loss: 0.00013842 Testing Loss: 0.00020183 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12250/100000] Training Loss: 0.00002469 Testing Loss: 0.00004660 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12300/100000] Training Loss: 0.00002235 Testing Loss: 0.00004761 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12350/100000] Training Loss: 0.00002989 Testing Loss: 0.00005242 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12400/100000] Training Loss: 0.00001670 Testing Loss: 0.00003884 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12450/100000] Training Loss: 0.00005000 Testing Loss: 0.00008041 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12500/100000] Training Loss: 0.00003072 Testing Loss: 0.00005259 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12550/100000] Training Loss: 0.00001657 Testing Loss: 0.00003861 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12600/100000] Training Loss: 0.00002014 Testing Loss: 0.00004441 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12650/100000] Training Loss: 0.00002912 Testing Loss: 0.00005017 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12700/100000] Training Loss: 0.00005179 Testing Loss: 0.00007718 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12750/100000] Training Loss: 0.00049273 Testing Loss: 0.00057334 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12800/100000] Training Loss: 0.00002338 Testing Loss: 0.00004626 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12850/100000] Training Loss: 0.00006191 Testing Loss: 0.00007067 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12900/100000] Training Loss: 0.00002376 Testing Loss: 0.00004752 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [12950/100000] Training Loss: 0.00002953 Testing Loss: 0.00004967 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13000/100000] Training Loss: 0.00003416 Testing Loss: 0.00005420 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13050/100000] Training Loss: 0.00003299 Testing Loss: 0.00005370 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13100/100000] Training Loss: 0.00002898 Testing Loss: 0.00004437 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13150/100000] Training Loss: 0.00010907 Testing Loss: 0.00013290 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13200/100000] Training Loss: 0.00001605 Testing Loss: 0.00003690 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13250/100000] Training Loss: 0.00021737 Testing Loss: 0.00029200 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13300/100000] Training Loss: 0.00001697 Testing Loss: 0.00003774 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13350/100000] Training Loss: 0.00015070 Testing Loss: 0.00013913 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13400/100000] Training Loss: 0.00003664 Testing Loss: 0.00005820 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13450/100000] Training Loss: 0.00104726 Testing Loss: 0.00109206 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13500/100000] Training Loss: 0.00001819 Testing Loss: 0.00003839 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13550/100000] Training Loss: 0.00003390 Testing Loss: 0.00006574 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13600/100000] Training Loss: 0.00004706 Testing Loss: 0.00006840 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13650/100000] Training Loss: 0.00001534 Testing Loss: 0.00003554 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13700/100000] Training Loss: 0.00006920 Testing Loss: 0.00008280 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13750/100000] Training Loss: 0.00001645 Testing Loss: 0.00003688 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13800/100000] Training Loss: 0.00024115 Testing Loss: 0.00034997 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13850/100000] Training Loss: 0.00001762 Testing Loss: 0.00003722 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13900/100000] Training Loss: 0.00023915 Testing Loss: 0.00036054 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [13950/100000] Training Loss: 0.00001844 Testing Loss: 0.00003807 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14000/100000] Training Loss: 0.00002629 Testing Loss: 0.00004953 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14050/100000] Training Loss: 0.00003221 Testing Loss: 0.00005215 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14100/100000] Training Loss: 0.00002155 Testing Loss: 0.00004435 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14150/100000] Training Loss: 0.00002206 Testing Loss: 0.00004378 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14200/100000] Training Loss: 0.00005919 Testing Loss: 0.00007776 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14250/100000] Training Loss: 0.00004201 Testing Loss: 0.00006425 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14300/100000] Training Loss: 0.00002799 Testing Loss: 0.00004685 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14350/100000] Training Loss: 0.00001448 Testing Loss: 0.00003401 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14400/100000] Training Loss: 0.00008076 Testing Loss: 0.00007731 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14450/100000] Training Loss: 0.00006172 Testing Loss: 0.00009702 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14500/100000] Training Loss: 0.00002074 Testing Loss: 0.00003946 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14550/100000] Training Loss: 0.00042544 Testing Loss: 0.00047452 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14600/100000] Training Loss: 0.00001732 Testing Loss: 0.00003636 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14650/100000] Training Loss: 0.00004417 Testing Loss: 0.00006357 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14700/100000] Training Loss: 0.00002480 Testing Loss: 0.00004546 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14750/100000] Training Loss: 0.00002788 Testing Loss: 0.00004581 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14800/100000] Training Loss: 0.00003414 Testing Loss: 0.00006029 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14850/100000] Training Loss: 0.00001994 Testing Loss: 0.00003781 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14900/100000] Training Loss: 0.00065173 Testing Loss: 0.00062002 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [14950/100000] Training Loss: 0.00001551 Testing Loss: 0.00003386 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15000/100000] Training Loss: 0.00019782 Testing Loss: 0.00027618 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15050/100000] Training Loss: 0.00001794 Testing Loss: 0.00003590 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15100/100000] Training Loss: 0.00051444 Testing Loss: 0.00061168 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15150/100000] Training Loss: 0.00001464 Testing Loss: 0.00003277 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15200/100000] Training Loss: 0.00013549 Testing Loss: 0.00013540 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15250/100000] Training Loss: 0.00001570 Testing Loss: 0.00003376 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15300/100000] Training Loss: 0.00005222 Testing Loss: 0.00006457 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15350/100000] Training Loss: 0.00002683 Testing Loss: 0.00004524 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15400/100000] Training Loss: 0.00004958 Testing Loss: 0.00006382 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15450/100000] Training Loss: 0.00001304 Testing Loss: 0.00003101 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15500/100000] Training Loss: 0.00015678 Testing Loss: 0.00020765 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15550/100000] Training Loss: 0.00001796 Testing Loss: 0.00003569 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15600/100000] Training Loss: 0.00038377 Testing Loss: 0.00048981 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15650/100000] Training Loss: 0.00001542 Testing Loss: 0.00003285 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15700/100000] Training Loss: 0.00004573 Testing Loss: 0.00006437 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15750/100000] Training Loss: 0.00002602 Testing Loss: 0.00004316 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15800/100000] Training Loss: 0.00007793 Testing Loss: 0.00009091 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15850/100000] Training Loss: 0.00002678 Testing Loss: 0.00004275 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15900/100000] Training Loss: 0.00001315 Testing Loss: 0.00003067 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [15950/100000] Training Loss: 0.00004549 Testing Loss: 0.00006214 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16000/100000] Training Loss: 0.00001267 Testing Loss: 0.00003003 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16050/100000] Training Loss: 0.00035782 Testing Loss: 0.00046387 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16100/100000] Training Loss: 0.00001505 Testing Loss: 0.00003210 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16150/100000] Training Loss: 0.00006620 Testing Loss: 0.00008762 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16200/100000] Training Loss: 0.00002604 Testing Loss: 0.00004185 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16250/100000] Training Loss: 0.00001718 Testing Loss: 0.00003715 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16300/100000] Training Loss: 0.00019452 Testing Loss: 0.00019174 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16350/100000] Training Loss: 0.00001288 Testing Loss: 0.00002980 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16400/100000] Training Loss: 0.00003316 Testing Loss: 0.00005044 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16450/100000] Training Loss: 0.00009629 Testing Loss: 0.00009589 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16500/100000] Training Loss: 0.00001244 Testing Loss: 0.00002934 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16550/100000] Training Loss: 0.00003261 Testing Loss: 0.00004340 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16600/100000] Training Loss: 0.00004334 Testing Loss: 0.00005752 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16650/100000] Training Loss: 0.00001255 Testing Loss: 0.00002924 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16700/100000] Training Loss: 0.00012569 Testing Loss: 0.00012449 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16750/100000] Training Loss: 0.00001240 Testing Loss: 0.00002901 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16800/100000] Training Loss: 0.00058517 Testing Loss: 0.00076471 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16850/100000] Training Loss: 0.00001530 Testing Loss: 0.00003181 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16900/100000] Training Loss: 0.00001205 Testing Loss: 0.00002853 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [16950/100000] Training Loss: 0.00181744 Testing Loss: 0.00166078 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17000/100000] Training Loss: 0.00001597 Testing Loss: 0.00003260 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17050/100000] Training Loss: 0.00001155 Testing Loss: 0.00002796 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17100/100000] Training Loss: 0.00001127 Testing Loss: 0.00002748 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17150/100000] Training Loss: 0.00013167 Testing Loss: 0.00015113 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17200/100000] Training Loss: 0.00001221 Testing Loss: 0.00002846 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17250/100000] Training Loss: 0.00066037 Testing Loss: 0.00062981 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17300/100000] Training Loss: 0.00001293 Testing Loss: 0.00002903 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17350/100000] Training Loss: 0.00002383 Testing Loss: 0.00003878 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17400/100000] Training Loss: 0.00017482 Testing Loss: 0.00016879 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17450/100000] Training Loss: 0.00001198 Testing Loss: 0.00002804 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17500/100000] Training Loss: 0.00005726 Testing Loss: 0.00008329 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17550/100000] Training Loss: 0.00005161 Testing Loss: 0.00007618 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17600/100000] Training Loss: 0.00002008 Testing Loss: 0.00003527 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17650/100000] Training Loss: 0.00006838 Testing Loss: 0.00008525 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17700/100000] Training Loss: 0.00002384 Testing Loss: 0.00004370 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17750/100000] Training Loss: 0.00002061 Testing Loss: 0.00003532 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17800/100000] Training Loss: 0.00004240 Testing Loss: 0.00005530 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17850/100000] Training Loss: 0.00002190 Testing Loss: 0.00003598 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17900/100000] Training Loss: 0.00005892 Testing Loss: 0.00008875 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [17950/100000] Training Loss: 0.00001676 Testing Loss: 0.00003206 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18000/100000] Training Loss: 0.00001529 Testing Loss: 0.00003211 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18050/100000] Training Loss: 0.00004045 Testing Loss: 0.00005399 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18100/100000] Training Loss: 0.00001095 Testing Loss: 0.00002651 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18150/100000] Training Loss: 0.00002278 Testing Loss: 0.00003622 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18200/100000] Training Loss: 0.00003689 Testing Loss: 0.00005137 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18250/100000] Training Loss: 0.00004130 Testing Loss: 0.00006328 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18300/100000] Training Loss: 0.00002976 Testing Loss: 0.00004356 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18350/100000] Training Loss: 0.00016566 Testing Loss: 0.00025114 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18400/100000] Training Loss: 0.00001418 Testing Loss: 0.00002890 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18450/100000] Training Loss: 0.00004106 Testing Loss: 0.00005807 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18500/100000] Training Loss: 0.00001742 Testing Loss: 0.00003241 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18550/100000] Training Loss: 0.00008747 Testing Loss: 0.00011675 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18600/100000] Training Loss: 0.00001363 Testing Loss: 0.00002833 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18650/100000] Training Loss: 0.00014571 Testing Loss: 0.00013319 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18700/100000] Training Loss: 0.00001138 Testing Loss: 0.00002635 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18750/100000] Training Loss: 0.00009429 Testing Loss: 0.00011385 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18800/100000] Training Loss: 0.00002212 Testing Loss: 0.00003771 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18850/100000] Training Loss: 0.00001022 Testing Loss: 0.00002507 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18900/100000] Training Loss: 0.00040312 Testing Loss: 0.00059072 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [18950/100000] Training Loss: 0.00001349 Testing Loss: 0.00002816 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19000/100000] Training Loss: 0.00005875 Testing Loss: 0.00008589 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19050/100000] Training Loss: 0.00005415 Testing Loss: 0.00006124 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19100/100000] Training Loss: 0.00001182 Testing Loss: 0.00002690 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19150/100000] Training Loss: 0.00002578 Testing Loss: 0.00003859 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19200/100000] Training Loss: 0.00001444 Testing Loss: 0.00003027 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19250/100000] Training Loss: 0.00002578 Testing Loss: 0.00003933 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19300/100000] Training Loss: 0.00003443 Testing Loss: 0.00005915 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19350/100000] Training Loss: 0.00001608 Testing Loss: 0.00003046 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19400/100000] Training Loss: 0.00003369 Testing Loss: 0.00005187 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19450/100000] Training Loss: 0.00004234 Testing Loss: 0.00005639 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19500/100000] Training Loss: 0.00001965 Testing Loss: 0.00003717 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19550/100000] Training Loss: 0.00004940 Testing Loss: 0.00005991 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19600/100000] Training Loss: 0.00001742 Testing Loss: 0.00003388 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19650/100000] Training Loss: 0.00003648 Testing Loss: 0.00004820 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19700/100000] Training Loss: 0.00000985 Testing Loss: 0.00002411 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19750/100000] Training Loss: 0.00009507 Testing Loss: 0.00010284 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19800/100000] Training Loss: 0.00000999 Testing Loss: 0.00002414 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19850/100000] Training Loss: 0.00014792 Testing Loss: 0.00018049 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19900/100000] Training Loss: 0.00001436 Testing Loss: 0.00002840 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [19950/100000] Training Loss: 0.00003708 Testing Loss: 0.00004800 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20000/100000] Training Loss: 0.00006352 Testing Loss: 0.00008984 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20050/100000] Training Loss: 0.00001519 Testing Loss: 0.00002937 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20100/100000] Training Loss: 0.00014245 Testing Loss: 0.00013578 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20150/100000] Training Loss: 0.00001029 Testing Loss: 0.00002414 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20200/100000] Training Loss: 0.00014452 Testing Loss: 0.00015730 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20250/100000] Training Loss: 0.00002066 Testing Loss: 0.00003813 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20300/100000] Training Loss: 0.00042385 Testing Loss: 0.00040921 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20350/100000] Training Loss: 0.00001116 Testing Loss: 0.00002484 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20400/100000] Training Loss: 0.00004557 Testing Loss: 0.00005824 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20450/100000] Training Loss: 0.00002430 Testing Loss: 0.00003839 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20500/100000] Training Loss: 0.00000944 Testing Loss: 0.00002322 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20550/100000] Training Loss: 0.00005803 Testing Loss: 0.00009780 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20600/100000] Training Loss: 0.00003162 Testing Loss: 0.00004301 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20650/100000] Training Loss: 0.00000989 Testing Loss: 0.00002358 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20700/100000] Training Loss: 0.00004202 Testing Loss: 0.00005547 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20750/100000] Training Loss: 0.00000930 Testing Loss: 0.00002287 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20800/100000] Training Loss: 0.00004791 Testing Loss: 0.00006451 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20850/100000] Training Loss: 0.00001463 Testing Loss: 0.00002771 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20900/100000] Training Loss: 0.00028280 Testing Loss: 0.00020374 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [20950/100000] Training Loss: 0.00001096 Testing Loss: 0.00002427 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21000/100000] Training Loss: 0.00029411 Testing Loss: 0.00041836 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21050/100000] Training Loss: 0.00001250 Testing Loss: 0.00002599 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21100/100000] Training Loss: 0.00001349 Testing Loss: 0.00002729 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21150/100000] Training Loss: 0.00005568 Testing Loss: 0.00006559 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21200/100000] Training Loss: 0.00000960 Testing Loss: 0.00002313 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21250/100000] Training Loss: 0.00001186 Testing Loss: 0.00002565 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21300/100000] Training Loss: 0.00002012 Testing Loss: 0.00003223 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21350/100000] Training Loss: 0.00002040 Testing Loss: 0.00003408 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21400/100000] Training Loss: 0.00042091 Testing Loss: 0.00026168 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21450/100000] Training Loss: 0.00002116 Testing Loss: 0.00003528 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21500/100000] Training Loss: 0.00004693 Testing Loss: 0.00007144 lr: [0.001]\n",
      "**************************************************\n",
      "Epoch [21550/100000] Training Loss: 0.00001063 Testing Loss: 0.00002568 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [21600/100000] Training Loss: 0.00000861 Testing Loss: 0.00002162 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [21650/100000] Training Loss: 0.00000851 Testing Loss: 0.00002145 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [21700/100000] Training Loss: 0.00000846 Testing Loss: 0.00002133 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [21750/100000] Training Loss: 0.00000840 Testing Loss: 0.00002123 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [21800/100000] Training Loss: 0.00000836 Testing Loss: 0.00002114 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [21850/100000] Training Loss: 0.00000831 Testing Loss: 0.00002105 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [21900/100000] Training Loss: 0.00000827 Testing Loss: 0.00002096 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [21950/100000] Training Loss: 0.00000823 Testing Loss: 0.00002088 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22000/100000] Training Loss: 0.00000819 Testing Loss: 0.00002080 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22050/100000] Training Loss: 0.00000815 Testing Loss: 0.00002072 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22100/100000] Training Loss: 0.00000812 Testing Loss: 0.00002064 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22150/100000] Training Loss: 0.00000808 Testing Loss: 0.00002056 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22200/100000] Training Loss: 0.00000805 Testing Loss: 0.00002049 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22250/100000] Training Loss: 0.00000801 Testing Loss: 0.00002041 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22300/100000] Training Loss: 0.00000797 Testing Loss: 0.00002034 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22350/100000] Training Loss: 0.00000794 Testing Loss: 0.00002026 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22400/100000] Training Loss: 0.00000790 Testing Loss: 0.00002019 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22450/100000] Training Loss: 0.00000787 Testing Loss: 0.00002012 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22500/100000] Training Loss: 0.00003393 Testing Loss: 0.00003366 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22550/100000] Training Loss: 0.00000926 Testing Loss: 0.00002135 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22600/100000] Training Loss: 0.00011637 Testing Loss: 0.00012264 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22650/100000] Training Loss: 0.00000832 Testing Loss: 0.00002050 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22700/100000] Training Loss: 0.00003688 Testing Loss: 0.00004829 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22750/100000] Training Loss: 0.00000840 Testing Loss: 0.00002070 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22800/100000] Training Loss: 0.00001682 Testing Loss: 0.00003130 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22850/100000] Training Loss: 0.00000869 Testing Loss: 0.00002108 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22900/100000] Training Loss: 0.00003373 Testing Loss: 0.00003774 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [22950/100000] Training Loss: 0.00000887 Testing Loss: 0.00002100 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23000/100000] Training Loss: 0.00002000 Testing Loss: 0.00003090 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23050/100000] Training Loss: 0.00001019 Testing Loss: 0.00002276 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23100/100000] Training Loss: 0.00002870 Testing Loss: 0.00003430 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23150/100000] Training Loss: 0.00002506 Testing Loss: 0.00003349 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23200/100000] Training Loss: 0.00004001 Testing Loss: 0.00004045 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23250/100000] Training Loss: 0.00000793 Testing Loss: 0.00001981 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23300/100000] Training Loss: 0.00015633 Testing Loss: 0.00019819 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23350/100000] Training Loss: 0.00000820 Testing Loss: 0.00002001 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23400/100000] Training Loss: 0.00008940 Testing Loss: 0.00010410 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23450/100000] Training Loss: 0.00001087 Testing Loss: 0.00002313 lr: [0.0006]\n",
      "**************************************************\n",
      "Epoch [23500/100000] Training Loss: 0.00000889 Testing Loss: 0.00002028 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23550/100000] Training Loss: 0.00000746 Testing Loss: 0.00001920 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23600/100000] Training Loss: 0.00000742 Testing Loss: 0.00001913 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23650/100000] Training Loss: 0.00000740 Testing Loss: 0.00001908 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23700/100000] Training Loss: 0.00000737 Testing Loss: 0.00001902 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23750/100000] Training Loss: 0.00000735 Testing Loss: 0.00001897 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23800/100000] Training Loss: 0.00000733 Testing Loss: 0.00001892 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23850/100000] Training Loss: 0.00000731 Testing Loss: 0.00001887 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23900/100000] Training Loss: 0.00000728 Testing Loss: 0.00001883 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [23950/100000] Training Loss: 0.00000726 Testing Loss: 0.00001878 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24000/100000] Training Loss: 0.00000724 Testing Loss: 0.00001873 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24050/100000] Training Loss: 0.00000722 Testing Loss: 0.00001868 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24100/100000] Training Loss: 0.00000719 Testing Loss: 0.00001863 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24150/100000] Training Loss: 0.00000717 Testing Loss: 0.00001858 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24200/100000] Training Loss: 0.00000715 Testing Loss: 0.00001853 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24250/100000] Training Loss: 0.00000712 Testing Loss: 0.00001848 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24300/100000] Training Loss: 0.00000710 Testing Loss: 0.00001843 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24350/100000] Training Loss: 0.00000708 Testing Loss: 0.00001838 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24400/100000] Training Loss: 0.00000705 Testing Loss: 0.00001833 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24450/100000] Training Loss: 0.00000946 Testing Loss: 0.00002203 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24500/100000] Training Loss: 0.00000779 Testing Loss: 0.00001914 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24550/100000] Training Loss: 0.00001105 Testing Loss: 0.00002224 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24600/100000] Training Loss: 0.00000844 Testing Loss: 0.00001966 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24650/100000] Training Loss: 0.00001332 Testing Loss: 0.00002482 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24700/100000] Training Loss: 0.00000946 Testing Loss: 0.00001987 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24750/100000] Training Loss: 0.00000713 Testing Loss: 0.00001833 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24800/100000] Training Loss: 0.00000947 Testing Loss: 0.00002077 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24850/100000] Training Loss: 0.00001024 Testing Loss: 0.00002144 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24900/100000] Training Loss: 0.00005499 Testing Loss: 0.00006366 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [24950/100000] Training Loss: 0.00000716 Testing Loss: 0.00001822 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25000/100000] Training Loss: 0.00005626 Testing Loss: 0.00006008 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25050/100000] Training Loss: 0.00000740 Testing Loss: 0.00001844 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25100/100000] Training Loss: 0.00002852 Testing Loss: 0.00004670 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25150/100000] Training Loss: 0.00000731 Testing Loss: 0.00001819 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25200/100000] Training Loss: 0.00000994 Testing Loss: 0.00002094 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25250/100000] Training Loss: 0.00002765 Testing Loss: 0.00002680 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25300/100000] Training Loss: 0.00000694 Testing Loss: 0.00001785 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25350/100000] Training Loss: 0.00000932 Testing Loss: 0.00002037 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25400/100000] Training Loss: 0.00001079 Testing Loss: 0.00002201 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25450/100000] Training Loss: 0.00000811 Testing Loss: 0.00001909 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25500/100000] Training Loss: 0.00000672 Testing Loss: 0.00001753 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25550/100000] Training Loss: 0.00004429 Testing Loss: 0.00006722 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25600/100000] Training Loss: 0.00000703 Testing Loss: 0.00001778 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25650/100000] Training Loss: 0.00001457 Testing Loss: 0.00002659 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25700/100000] Training Loss: 0.00000750 Testing Loss: 0.00001817 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25750/100000] Training Loss: 0.00003928 Testing Loss: 0.00005251 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25800/100000] Training Loss: 0.00000692 Testing Loss: 0.00001763 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25850/100000] Training Loss: 0.00001046 Testing Loss: 0.00002207 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25900/100000] Training Loss: 0.00001063 Testing Loss: 0.00002121 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [25950/100000] Training Loss: 0.00002639 Testing Loss: 0.00004449 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26000/100000] Training Loss: 0.00000717 Testing Loss: 0.00001774 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26050/100000] Training Loss: 0.00001053 Testing Loss: 0.00002111 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26100/100000] Training Loss: 0.00001822 Testing Loss: 0.00002385 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26150/100000] Training Loss: 0.00001176 Testing Loss: 0.00002372 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26200/100000] Training Loss: 0.00001004 Testing Loss: 0.00002073 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26250/100000] Training Loss: 0.00000840 Testing Loss: 0.00001874 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26300/100000] Training Loss: 0.00000656 Testing Loss: 0.00001710 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26350/100000] Training Loss: 0.00001166 Testing Loss: 0.00002321 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26400/100000] Training Loss: 0.00000918 Testing Loss: 0.00001999 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26450/100000] Training Loss: 0.00000695 Testing Loss: 0.00001759 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26500/100000] Training Loss: 0.00006724 Testing Loss: 0.00007096 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26550/100000] Training Loss: 0.00000657 Testing Loss: 0.00001697 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26600/100000] Training Loss: 0.00002364 Testing Loss: 0.00003700 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26650/100000] Training Loss: 0.00000681 Testing Loss: 0.00001718 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26700/100000] Training Loss: 0.00001572 Testing Loss: 0.00002517 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26750/100000] Training Loss: 0.00000654 Testing Loss: 0.00001690 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26800/100000] Training Loss: 0.00001341 Testing Loss: 0.00002642 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26850/100000] Training Loss: 0.00000812 Testing Loss: 0.00001857 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26900/100000] Training Loss: 0.00008506 Testing Loss: 0.00011899 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [26950/100000] Training Loss: 0.00000670 Testing Loss: 0.00001688 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27000/100000] Training Loss: 0.00000921 Testing Loss: 0.00001912 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27050/100000] Training Loss: 0.00001357 Testing Loss: 0.00002542 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27100/100000] Training Loss: 0.00000698 Testing Loss: 0.00001718 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27150/100000] Training Loss: 0.00001620 Testing Loss: 0.00002895 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27200/100000] Training Loss: 0.00000676 Testing Loss: 0.00001691 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27250/100000] Training Loss: 0.00000862 Testing Loss: 0.00001888 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27300/100000] Training Loss: 0.00001388 Testing Loss: 0.00002293 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27350/100000] Training Loss: 0.00000619 Testing Loss: 0.00001631 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27400/100000] Training Loss: 0.00000929 Testing Loss: 0.00001894 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27450/100000] Training Loss: 0.00001421 Testing Loss: 0.00002442 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27500/100000] Training Loss: 0.00000772 Testing Loss: 0.00001804 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27550/100000] Training Loss: 0.00005314 Testing Loss: 0.00007656 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27600/100000] Training Loss: 0.00000650 Testing Loss: 0.00001650 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27650/100000] Training Loss: 0.00001580 Testing Loss: 0.00002446 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27700/100000] Training Loss: 0.00001961 Testing Loss: 0.00003311 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27750/100000] Training Loss: 0.00000666 Testing Loss: 0.00001682 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27800/100000] Training Loss: 0.00004622 Testing Loss: 0.00006014 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27850/100000] Training Loss: 0.00000658 Testing Loss: 0.00001657 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27900/100000] Training Loss: 0.00008137 Testing Loss: 0.00009290 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [27950/100000] Training Loss: 0.00000633 Testing Loss: 0.00001623 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28000/100000] Training Loss: 0.00000878 Testing Loss: 0.00001872 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28050/100000] Training Loss: 0.00000942 Testing Loss: 0.00001907 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28100/100000] Training Loss: 0.00000609 Testing Loss: 0.00001597 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28150/100000] Training Loss: 0.00007005 Testing Loss: 0.00008230 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28200/100000] Training Loss: 0.00000667 Testing Loss: 0.00001663 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28250/100000] Training Loss: 0.00000923 Testing Loss: 0.00001912 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28300/100000] Training Loss: 0.00000923 Testing Loss: 0.00001956 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28350/100000] Training Loss: 0.00000596 Testing Loss: 0.00001576 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28400/100000] Training Loss: 0.00000876 Testing Loss: 0.00001872 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28450/100000] Training Loss: 0.00000725 Testing Loss: 0.00001677 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28500/100000] Training Loss: 0.00001521 Testing Loss: 0.00002711 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28550/100000] Training Loss: 0.00000651 Testing Loss: 0.00001623 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28600/100000] Training Loss: 0.00001080 Testing Loss: 0.00001872 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28650/100000] Training Loss: 0.00001680 Testing Loss: 0.00002355 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28700/100000] Training Loss: 0.00000603 Testing Loss: 0.00001570 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28750/100000] Training Loss: 0.00000976 Testing Loss: 0.00001895 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28800/100000] Training Loss: 0.00001109 Testing Loss: 0.00002084 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28850/100000] Training Loss: 0.00000715 Testing Loss: 0.00001670 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28900/100000] Training Loss: 0.00001060 Testing Loss: 0.00002113 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [28950/100000] Training Loss: 0.00000685 Testing Loss: 0.00001637 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29000/100000] Training Loss: 0.00000970 Testing Loss: 0.00002027 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29050/100000] Training Loss: 0.00001261 Testing Loss: 0.00002356 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29100/100000] Training Loss: 0.00000681 Testing Loss: 0.00001651 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29150/100000] Training Loss: 0.00001422 Testing Loss: 0.00002641 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29200/100000] Training Loss: 0.00000634 Testing Loss: 0.00001585 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29250/100000] Training Loss: 0.00001080 Testing Loss: 0.00002008 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29300/100000] Training Loss: 0.00000684 Testing Loss: 0.00001646 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29350/100000] Training Loss: 0.00001256 Testing Loss: 0.00002510 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29400/100000] Training Loss: 0.00001120 Testing Loss: 0.00002167 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29450/100000] Training Loss: 0.00000764 Testing Loss: 0.00001709 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29500/100000] Training Loss: 0.00001164 Testing Loss: 0.00001982 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29550/100000] Training Loss: 0.00000590 Testing Loss: 0.00001535 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29600/100000] Training Loss: 0.00006149 Testing Loss: 0.00008105 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29650/100000] Training Loss: 0.00000599 Testing Loss: 0.00001536 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29700/100000] Training Loss: 0.00000713 Testing Loss: 0.00001644 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29750/100000] Training Loss: 0.00001207 Testing Loss: 0.00002085 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29800/100000] Training Loss: 0.00000580 Testing Loss: 0.00001521 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29850/100000] Training Loss: 0.00001558 Testing Loss: 0.00002893 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29900/100000] Training Loss: 0.00000631 Testing Loss: 0.00001552 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [29950/100000] Training Loss: 0.00001815 Testing Loss: 0.00002635 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30000/100000] Training Loss: 0.00000907 Testing Loss: 0.00001843 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30050/100000] Training Loss: 0.00001115 Testing Loss: 0.00002286 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30100/100000] Training Loss: 0.00000559 Testing Loss: 0.00001485 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30150/100000] Training Loss: 0.00000774 Testing Loss: 0.00001666 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30200/100000] Training Loss: 0.00001030 Testing Loss: 0.00002052 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30250/100000] Training Loss: 0.00000621 Testing Loss: 0.00001542 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30300/100000] Training Loss: 0.00001044 Testing Loss: 0.00002140 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30350/100000] Training Loss: 0.00000676 Testing Loss: 0.00001609 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30400/100000] Training Loss: 0.00001289 Testing Loss: 0.00002319 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30450/100000] Training Loss: 0.00000602 Testing Loss: 0.00001529 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30500/100000] Training Loss: 0.00001211 Testing Loss: 0.00002099 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30550/100000] Training Loss: 0.00000762 Testing Loss: 0.00001684 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30600/100000] Training Loss: 0.00002134 Testing Loss: 0.00003368 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30650/100000] Training Loss: 0.00000596 Testing Loss: 0.00001506 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30700/100000] Training Loss: 0.00002150 Testing Loss: 0.00003506 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30750/100000] Training Loss: 0.00000587 Testing Loss: 0.00001489 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30800/100000] Training Loss: 0.00001287 Testing Loss: 0.00002168 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30850/100000] Training Loss: 0.00001149 Testing Loss: 0.00001733 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30900/100000] Training Loss: 0.00000666 Testing Loss: 0.00001611 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [30950/100000] Training Loss: 0.00000813 Testing Loss: 0.00001742 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31000/100000] Training Loss: 0.00000790 Testing Loss: 0.00001661 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31050/100000] Training Loss: 0.00001211 Testing Loss: 0.00002263 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31100/100000] Training Loss: 0.00001197 Testing Loss: 0.00001915 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31150/100000] Training Loss: 0.00000551 Testing Loss: 0.00001451 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31200/100000] Training Loss: 0.00003620 Testing Loss: 0.00003527 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31250/100000] Training Loss: 0.00000570 Testing Loss: 0.00001470 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31300/100000] Training Loss: 0.00005063 Testing Loss: 0.00005611 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31350/100000] Training Loss: 0.00000962 Testing Loss: 0.00001963 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31400/100000] Training Loss: 0.00000907 Testing Loss: 0.00001855 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31450/100000] Training Loss: 0.00000675 Testing Loss: 0.00001576 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31500/100000] Training Loss: 0.00000574 Testing Loss: 0.00001484 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31550/100000] Training Loss: 0.00000723 Testing Loss: 0.00001626 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31600/100000] Training Loss: 0.00000700 Testing Loss: 0.00001596 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31650/100000] Training Loss: 0.00000726 Testing Loss: 0.00001638 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31700/100000] Training Loss: 0.00000693 Testing Loss: 0.00001610 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31750/100000] Training Loss: 0.00000993 Testing Loss: 0.00001902 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31800/100000] Training Loss: 0.00000755 Testing Loss: 0.00001647 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31850/100000] Training Loss: 0.00000867 Testing Loss: 0.00001777 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31900/100000] Training Loss: 0.00001932 Testing Loss: 0.00002792 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [31950/100000] Training Loss: 0.00002420 Testing Loss: 0.00003233 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32000/100000] Training Loss: 0.00001046 Testing Loss: 0.00002017 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32050/100000] Training Loss: 0.00000596 Testing Loss: 0.00001473 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32100/100000] Training Loss: 0.00001660 Testing Loss: 0.00002947 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32150/100000] Training Loss: 0.00000566 Testing Loss: 0.00001438 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32200/100000] Training Loss: 0.00001169 Testing Loss: 0.00001895 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32250/100000] Training Loss: 0.00008093 Testing Loss: 0.00008291 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32300/100000] Training Loss: 0.00000532 Testing Loss: 0.00001402 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32350/100000] Training Loss: 0.00001035 Testing Loss: 0.00001949 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32400/100000] Training Loss: 0.00000887 Testing Loss: 0.00001854 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32450/100000] Training Loss: 0.00000921 Testing Loss: 0.00001812 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32500/100000] Training Loss: 0.00001019 Testing Loss: 0.00001895 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32550/100000] Training Loss: 0.00001238 Testing Loss: 0.00002158 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32600/100000] Training Loss: 0.00000604 Testing Loss: 0.00001457 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32650/100000] Training Loss: 0.00001535 Testing Loss: 0.00002550 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32700/100000] Training Loss: 0.00000553 Testing Loss: 0.00001416 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32750/100000] Training Loss: 0.00005541 Testing Loss: 0.00006721 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32800/100000] Training Loss: 0.00000555 Testing Loss: 0.00001420 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32850/100000] Training Loss: 0.00000990 Testing Loss: 0.00001955 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32900/100000] Training Loss: 0.00000597 Testing Loss: 0.00001452 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [32950/100000] Training Loss: 0.00001174 Testing Loss: 0.00002073 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33000/100000] Training Loss: 0.00001151 Testing Loss: 0.00002749 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33050/100000] Training Loss: 0.00000516 Testing Loss: 0.00001370 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33100/100000] Training Loss: 0.00000705 Testing Loss: 0.00001554 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33150/100000] Training Loss: 0.00001872 Testing Loss: 0.00002874 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33200/100000] Training Loss: 0.00000522 Testing Loss: 0.00001379 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33250/100000] Training Loss: 0.00000742 Testing Loss: 0.00001590 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33300/100000] Training Loss: 0.00001527 Testing Loss: 0.00002276 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33350/100000] Training Loss: 0.00000817 Testing Loss: 0.00001664 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33400/100000] Training Loss: 0.00000744 Testing Loss: 0.00001511 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33450/100000] Training Loss: 0.00001453 Testing Loss: 0.00002328 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33500/100000] Training Loss: 0.00000646 Testing Loss: 0.00001486 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33550/100000] Training Loss: 0.00001323 Testing Loss: 0.00002157 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33600/100000] Training Loss: 0.00000833 Testing Loss: 0.00001613 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33650/100000] Training Loss: 0.00000671 Testing Loss: 0.00001559 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33700/100000] Training Loss: 0.00001391 Testing Loss: 0.00001818 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33750/100000] Training Loss: 0.00000501 Testing Loss: 0.00001340 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33800/100000] Training Loss: 0.00000686 Testing Loss: 0.00001521 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33850/100000] Training Loss: 0.00001034 Testing Loss: 0.00001891 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33900/100000] Training Loss: 0.00000496 Testing Loss: 0.00001333 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [33950/100000] Training Loss: 0.00000612 Testing Loss: 0.00001435 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34000/100000] Training Loss: 0.00001168 Testing Loss: 0.00002052 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34050/100000] Training Loss: 0.00000494 Testing Loss: 0.00001327 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34100/100000] Training Loss: 0.00000879 Testing Loss: 0.00001764 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34150/100000] Training Loss: 0.00000607 Testing Loss: 0.00001415 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34200/100000] Training Loss: 0.00000996 Testing Loss: 0.00001652 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34250/100000] Training Loss: 0.00000818 Testing Loss: 0.00001683 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34300/100000] Training Loss: 0.00000633 Testing Loss: 0.00001424 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34350/100000] Training Loss: 0.00000984 Testing Loss: 0.00001973 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34400/100000] Training Loss: 0.00000745 Testing Loss: 0.00001582 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34450/100000] Training Loss: 0.00000649 Testing Loss: 0.00001459 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34500/100000] Training Loss: 0.00001108 Testing Loss: 0.00001932 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34550/100000] Training Loss: 0.00000790 Testing Loss: 0.00001779 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34600/100000] Training Loss: 0.00000841 Testing Loss: 0.00001513 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34650/100000] Training Loss: 0.00000767 Testing Loss: 0.00001594 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34700/100000] Training Loss: 0.00000619 Testing Loss: 0.00001436 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34750/100000] Training Loss: 0.00000608 Testing Loss: 0.00001471 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34800/100000] Training Loss: 0.00001525 Testing Loss: 0.00002420 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34850/100000] Training Loss: 0.00000627 Testing Loss: 0.00001429 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34900/100000] Training Loss: 0.00001016 Testing Loss: 0.00001844 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [34950/100000] Training Loss: 0.00001564 Testing Loss: 0.00002339 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35000/100000] Training Loss: 0.00000481 Testing Loss: 0.00001294 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35050/100000] Training Loss: 0.00000765 Testing Loss: 0.00001572 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35100/100000] Training Loss: 0.00000635 Testing Loss: 0.00001439 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35150/100000] Training Loss: 0.00000866 Testing Loss: 0.00001717 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35200/100000] Training Loss: 0.00001395 Testing Loss: 0.00002055 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35250/100000] Training Loss: 0.00001145 Testing Loss: 0.00002047 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35300/100000] Training Loss: 0.00000769 Testing Loss: 0.00001541 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35350/100000] Training Loss: 0.00004392 Testing Loss: 0.00005181 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35400/100000] Training Loss: 0.00000512 Testing Loss: 0.00001321 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35450/100000] Training Loss: 0.00001731 Testing Loss: 0.00002691 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35500/100000] Training Loss: 0.00000492 Testing Loss: 0.00001301 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35550/100000] Training Loss: 0.00005717 Testing Loss: 0.00008066 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35600/100000] Training Loss: 0.00000498 Testing Loss: 0.00001294 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35650/100000] Training Loss: 0.00000893 Testing Loss: 0.00001673 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35700/100000] Training Loss: 0.00001673 Testing Loss: 0.00002126 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35750/100000] Training Loss: 0.00000486 Testing Loss: 0.00001290 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35800/100000] Training Loss: 0.00000690 Testing Loss: 0.00001487 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35850/100000] Training Loss: 0.00000878 Testing Loss: 0.00001643 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35900/100000] Training Loss: 0.00003900 Testing Loss: 0.00005248 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [35950/100000] Training Loss: 0.00000870 Testing Loss: 0.00001742 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36000/100000] Training Loss: 0.00005390 Testing Loss: 0.00006950 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36050/100000] Training Loss: 0.00000533 Testing Loss: 0.00001343 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36100/100000] Training Loss: 0.00000855 Testing Loss: 0.00001697 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36150/100000] Training Loss: 0.00000541 Testing Loss: 0.00001339 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36200/100000] Training Loss: 0.00001299 Testing Loss: 0.00002219 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36250/100000] Training Loss: 0.00001283 Testing Loss: 0.00001863 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36300/100000] Training Loss: 0.00000991 Testing Loss: 0.00001760 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36350/100000] Training Loss: 0.00000764 Testing Loss: 0.00001557 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36400/100000] Training Loss: 0.00001398 Testing Loss: 0.00002202 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36450/100000] Training Loss: 0.00000545 Testing Loss: 0.00001331 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36500/100000] Training Loss: 0.00000948 Testing Loss: 0.00001742 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36550/100000] Training Loss: 0.00000705 Testing Loss: 0.00001548 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36600/100000] Training Loss: 0.00000570 Testing Loss: 0.00001364 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36650/100000] Training Loss: 0.00000824 Testing Loss: 0.00001602 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36700/100000] Training Loss: 0.00000571 Testing Loss: 0.00001339 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36750/100000] Training Loss: 0.00000747 Testing Loss: 0.00001569 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36800/100000] Training Loss: 0.00000577 Testing Loss: 0.00001334 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36850/100000] Training Loss: 0.00001362 Testing Loss: 0.00002152 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36900/100000] Training Loss: 0.00000665 Testing Loss: 0.00001458 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [36950/100000] Training Loss: 0.00001719 Testing Loss: 0.00002035 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37000/100000] Training Loss: 0.00000454 Testing Loss: 0.00001231 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37050/100000] Training Loss: 0.00000646 Testing Loss: 0.00001415 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37100/100000] Training Loss: 0.00000875 Testing Loss: 0.00001657 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37150/100000] Training Loss: 0.00000920 Testing Loss: 0.00001659 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37200/100000] Training Loss: 0.00002038 Testing Loss: 0.00003118 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37250/100000] Training Loss: 0.00000483 Testing Loss: 0.00001252 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37300/100000] Training Loss: 0.00008635 Testing Loss: 0.00008320 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37350/100000] Training Loss: 0.00000466 Testing Loss: 0.00001237 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37400/100000] Training Loss: 0.00001536 Testing Loss: 0.00002432 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37450/100000] Training Loss: 0.00000655 Testing Loss: 0.00001373 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37500/100000] Training Loss: 0.00000813 Testing Loss: 0.00001559 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37550/100000] Training Loss: 0.00000595 Testing Loss: 0.00001396 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37600/100000] Training Loss: 0.00000683 Testing Loss: 0.00001549 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37650/100000] Training Loss: 0.00000645 Testing Loss: 0.00001420 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37700/100000] Training Loss: 0.00001909 Testing Loss: 0.00001910 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37750/100000] Training Loss: 0.00000458 Testing Loss: 0.00001222 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37800/100000] Training Loss: 0.00001047 Testing Loss: 0.00001799 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37850/100000] Training Loss: 0.00002869 Testing Loss: 0.00003201 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37900/100000] Training Loss: 0.00000713 Testing Loss: 0.00001477 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [37950/100000] Training Loss: 0.00000570 Testing Loss: 0.00001344 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38000/100000] Training Loss: 0.00001499 Testing Loss: 0.00002442 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38050/100000] Training Loss: 0.00000497 Testing Loss: 0.00001258 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38100/100000] Training Loss: 0.00000713 Testing Loss: 0.00001449 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38150/100000] Training Loss: 0.00000894 Testing Loss: 0.00001548 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38200/100000] Training Loss: 0.00000569 Testing Loss: 0.00001381 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38250/100000] Training Loss: 0.00001505 Testing Loss: 0.00002182 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38300/100000] Training Loss: 0.00000754 Testing Loss: 0.00001604 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38350/100000] Training Loss: 0.00001297 Testing Loss: 0.00002006 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38400/100000] Training Loss: 0.00000824 Testing Loss: 0.00001612 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38450/100000] Training Loss: 0.00000811 Testing Loss: 0.00001691 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38500/100000] Training Loss: 0.00000440 Testing Loss: 0.00001191 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38550/100000] Training Loss: 0.00000783 Testing Loss: 0.00001534 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38600/100000] Training Loss: 0.00003122 Testing Loss: 0.00003818 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38650/100000] Training Loss: 0.00000566 Testing Loss: 0.00001343 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38700/100000] Training Loss: 0.00000594 Testing Loss: 0.00001344 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38750/100000] Training Loss: 0.00002619 Testing Loss: 0.00004072 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38800/100000] Training Loss: 0.00000462 Testing Loss: 0.00001210 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38850/100000] Training Loss: 0.00000834 Testing Loss: 0.00001594 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38900/100000] Training Loss: 0.00000629 Testing Loss: 0.00001388 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [38950/100000] Training Loss: 0.00000906 Testing Loss: 0.00001814 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39000/100000] Training Loss: 0.00001029 Testing Loss: 0.00001902 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39050/100000] Training Loss: 0.00000495 Testing Loss: 0.00001236 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39100/100000] Training Loss: 0.00000858 Testing Loss: 0.00001587 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39150/100000] Training Loss: 0.00000601 Testing Loss: 0.00001343 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39200/100000] Training Loss: 0.00000661 Testing Loss: 0.00001420 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39250/100000] Training Loss: 0.00000810 Testing Loss: 0.00001566 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39300/100000] Training Loss: 0.00001291 Testing Loss: 0.00001809 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39350/100000] Training Loss: 0.00000430 Testing Loss: 0.00001169 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39400/100000] Training Loss: 0.00000994 Testing Loss: 0.00001921 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39450/100000] Training Loss: 0.00000785 Testing Loss: 0.00001573 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39500/100000] Training Loss: 0.00000649 Testing Loss: 0.00001414 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39550/100000] Training Loss: 0.00001181 Testing Loss: 0.00002056 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39600/100000] Training Loss: 0.00000471 Testing Loss: 0.00001207 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39650/100000] Training Loss: 0.00000799 Testing Loss: 0.00001399 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39700/100000] Training Loss: 0.00000645 Testing Loss: 0.00001324 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39750/100000] Training Loss: 0.00000561 Testing Loss: 0.00001336 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39800/100000] Training Loss: 0.00003733 Testing Loss: 0.00005446 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39850/100000] Training Loss: 0.00000464 Testing Loss: 0.00001197 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39900/100000] Training Loss: 0.00000739 Testing Loss: 0.00001518 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [39950/100000] Training Loss: 0.00000493 Testing Loss: 0.00001215 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40000/100000] Training Loss: 0.00000829 Testing Loss: 0.00001637 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40050/100000] Training Loss: 0.00000570 Testing Loss: 0.00001264 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40100/100000] Training Loss: 0.00001224 Testing Loss: 0.00002026 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40150/100000] Training Loss: 0.00000529 Testing Loss: 0.00001228 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40200/100000] Training Loss: 0.00000778 Testing Loss: 0.00001584 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40250/100000] Training Loss: 0.00002929 Testing Loss: 0.00003287 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40300/100000] Training Loss: 0.00000423 Testing Loss: 0.00001147 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40350/100000] Training Loss: 0.00000780 Testing Loss: 0.00001521 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40400/100000] Training Loss: 0.00000570 Testing Loss: 0.00001301 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40450/100000] Training Loss: 0.00005837 Testing Loss: 0.00006412 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40500/100000] Training Loss: 0.00000431 Testing Loss: 0.00001153 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40550/100000] Training Loss: 0.00001822 Testing Loss: 0.00002538 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40600/100000] Training Loss: 0.00002989 Testing Loss: 0.00003399 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40650/100000] Training Loss: 0.00000640 Testing Loss: 0.00001389 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40700/100000] Training Loss: 0.00000851 Testing Loss: 0.00001559 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40750/100000] Training Loss: 0.00000505 Testing Loss: 0.00001265 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40800/100000] Training Loss: 0.00000584 Testing Loss: 0.00001301 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40850/100000] Training Loss: 0.00003154 Testing Loss: 0.00005259 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40900/100000] Training Loss: 0.00000422 Testing Loss: 0.00001137 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [40950/100000] Training Loss: 0.00000957 Testing Loss: 0.00001653 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41000/100000] Training Loss: 0.00000658 Testing Loss: 0.00001387 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41050/100000] Training Loss: 0.00000522 Testing Loss: 0.00001207 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41100/100000] Training Loss: 0.00000809 Testing Loss: 0.00001522 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41150/100000] Training Loss: 0.00004969 Testing Loss: 0.00007571 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41200/100000] Training Loss: 0.00000453 Testing Loss: 0.00001158 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41250/100000] Training Loss: 0.00001048 Testing Loss: 0.00001726 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41300/100000] Training Loss: 0.00000974 Testing Loss: 0.00001714 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41350/100000] Training Loss: 0.00000517 Testing Loss: 0.00001215 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41400/100000] Training Loss: 0.00001904 Testing Loss: 0.00002374 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41450/100000] Training Loss: 0.00000608 Testing Loss: 0.00001337 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41500/100000] Training Loss: 0.00000690 Testing Loss: 0.00001337 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41550/100000] Training Loss: 0.00000677 Testing Loss: 0.00001449 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41600/100000] Training Loss: 0.00001122 Testing Loss: 0.00002032 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41650/100000] Training Loss: 0.00000466 Testing Loss: 0.00001162 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41700/100000] Training Loss: 0.00000894 Testing Loss: 0.00001632 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41750/100000] Training Loss: 0.00000855 Testing Loss: 0.00001483 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41800/100000] Training Loss: 0.00001104 Testing Loss: 0.00001841 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41850/100000] Training Loss: 0.00000696 Testing Loss: 0.00001359 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41900/100000] Training Loss: 0.00001163 Testing Loss: 0.00001989 lr: [0.00035999999999999997]\n",
      "**************************************************\n",
      "Epoch [41950/100000] Training Loss: 0.00000404 Testing Loss: 0.00001107 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42000/100000] Training Loss: 0.00000397 Testing Loss: 0.00001098 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42050/100000] Training Loss: 0.00000396 Testing Loss: 0.00001096 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42100/100000] Training Loss: 0.00000396 Testing Loss: 0.00001095 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42150/100000] Training Loss: 0.00000395 Testing Loss: 0.00001094 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42200/100000] Training Loss: 0.00000395 Testing Loss: 0.00001093 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42250/100000] Training Loss: 0.00000394 Testing Loss: 0.00001092 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42300/100000] Training Loss: 0.00000394 Testing Loss: 0.00001091 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42350/100000] Training Loss: 0.00000393 Testing Loss: 0.00001089 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42400/100000] Training Loss: 0.00000393 Testing Loss: 0.00001088 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42450/100000] Training Loss: 0.00000392 Testing Loss: 0.00001087 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42500/100000] Training Loss: 0.00000392 Testing Loss: 0.00001086 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42550/100000] Training Loss: 0.00000391 Testing Loss: 0.00001084 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42600/100000] Training Loss: 0.00000391 Testing Loss: 0.00001083 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42650/100000] Training Loss: 0.00000390 Testing Loss: 0.00001082 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42700/100000] Training Loss: 0.00000390 Testing Loss: 0.00001081 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42750/100000] Training Loss: 0.00000389 Testing Loss: 0.00001079 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42800/100000] Training Loss: 0.00000388 Testing Loss: 0.00001078 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42850/100000] Training Loss: 0.00000388 Testing Loss: 0.00001076 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42900/100000] Training Loss: 0.00000387 Testing Loss: 0.00001075 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [42950/100000] Training Loss: 0.00000424 Testing Loss: 0.00001113 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43000/100000] Training Loss: 0.00000722 Testing Loss: 0.00001347 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43050/100000] Training Loss: 0.00000391 Testing Loss: 0.00001078 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43100/100000] Training Loss: 0.00000481 Testing Loss: 0.00001172 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43150/100000] Training Loss: 0.00000613 Testing Loss: 0.00001235 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43200/100000] Training Loss: 0.00000409 Testing Loss: 0.00001102 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43250/100000] Training Loss: 0.00000824 Testing Loss: 0.00001625 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43300/100000] Training Loss: 0.00000407 Testing Loss: 0.00001089 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43350/100000] Training Loss: 0.00000467 Testing Loss: 0.00001161 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43400/100000] Training Loss: 0.00000408 Testing Loss: 0.00001107 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43450/100000] Training Loss: 0.00000628 Testing Loss: 0.00001303 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43500/100000] Training Loss: 0.00000496 Testing Loss: 0.00001174 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43550/100000] Training Loss: 0.00000449 Testing Loss: 0.00001129 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43600/100000] Training Loss: 0.00000529 Testing Loss: 0.00001218 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43650/100000] Training Loss: 0.00000450 Testing Loss: 0.00001118 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43700/100000] Training Loss: 0.00000521 Testing Loss: 0.00001164 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43750/100000] Training Loss: 0.00000992 Testing Loss: 0.00001777 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43800/100000] Training Loss: 0.00000397 Testing Loss: 0.00001074 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43850/100000] Training Loss: 0.00001241 Testing Loss: 0.00001628 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43900/100000] Training Loss: 0.00000423 Testing Loss: 0.00001107 lr: [0.00021599999999999996]\n",
      "**************************************************\n",
      "Epoch [43950/100000] Training Loss: 0.00000379 Testing Loss: 0.00001053 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44000/100000] Training Loss: 0.00000378 Testing Loss: 0.00001052 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44050/100000] Training Loss: 0.00000377 Testing Loss: 0.00001051 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44100/100000] Training Loss: 0.00000377 Testing Loss: 0.00001050 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44150/100000] Training Loss: 0.00000377 Testing Loss: 0.00001049 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44200/100000] Training Loss: 0.00000376 Testing Loss: 0.00001048 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44250/100000] Training Loss: 0.00000376 Testing Loss: 0.00001047 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44300/100000] Training Loss: 0.00000376 Testing Loss: 0.00001046 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44350/100000] Training Loss: 0.00000375 Testing Loss: 0.00001045 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44400/100000] Training Loss: 0.00000375 Testing Loss: 0.00001044 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44450/100000] Training Loss: 0.00000374 Testing Loss: 0.00001043 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44500/100000] Training Loss: 0.00000374 Testing Loss: 0.00001042 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44550/100000] Training Loss: 0.00000373 Testing Loss: 0.00001041 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44600/100000] Training Loss: 0.00000373 Testing Loss: 0.00001040 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44650/100000] Training Loss: 0.00000373 Testing Loss: 0.00001039 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44700/100000] Training Loss: 0.00000372 Testing Loss: 0.00001038 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44750/100000] Training Loss: 0.00000372 Testing Loss: 0.00001037 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44800/100000] Training Loss: 0.00000371 Testing Loss: 0.00001036 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44850/100000] Training Loss: 0.00000371 Testing Loss: 0.00001035 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44900/100000] Training Loss: 0.00000371 Testing Loss: 0.00001034 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [44950/100000] Training Loss: 0.00000392 Testing Loss: 0.00001055 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45000/100000] Training Loss: 0.00000385 Testing Loss: 0.00001058 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45050/100000] Training Loss: 0.00000424 Testing Loss: 0.00001093 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45100/100000] Training Loss: 0.00000533 Testing Loss: 0.00001245 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45150/100000] Training Loss: 0.00000374 Testing Loss: 0.00001033 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45200/100000] Training Loss: 0.00000446 Testing Loss: 0.00001098 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45250/100000] Training Loss: 0.00000613 Testing Loss: 0.00001175 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45300/100000] Training Loss: 0.00000400 Testing Loss: 0.00001071 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45350/100000] Training Loss: 0.00000416 Testing Loss: 0.00001084 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45400/100000] Training Loss: 0.00000390 Testing Loss: 0.00001050 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45450/100000] Training Loss: 0.00000430 Testing Loss: 0.00001089 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45500/100000] Training Loss: 0.00000403 Testing Loss: 0.00001058 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45550/100000] Training Loss: 0.00000404 Testing Loss: 0.00001066 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45600/100000] Training Loss: 0.00000402 Testing Loss: 0.00001064 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45650/100000] Training Loss: 0.00000373 Testing Loss: 0.00001031 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45700/100000] Training Loss: 0.00000415 Testing Loss: 0.00001073 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45750/100000] Training Loss: 0.00000414 Testing Loss: 0.00001074 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45800/100000] Training Loss: 0.00000380 Testing Loss: 0.00001032 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45850/100000] Training Loss: 0.00001063 Testing Loss: 0.00001775 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45900/100000] Training Loss: 0.00000442 Testing Loss: 0.00001092 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [45950/100000] Training Loss: 0.00000392 Testing Loss: 0.00001044 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46000/100000] Training Loss: 0.00000382 Testing Loss: 0.00001045 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46050/100000] Training Loss: 0.00000467 Testing Loss: 0.00001124 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46100/100000] Training Loss: 0.00000579 Testing Loss: 0.00001294 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46150/100000] Training Loss: 0.00000365 Testing Loss: 0.00001013 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46200/100000] Training Loss: 0.00000407 Testing Loss: 0.00001055 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46250/100000] Training Loss: 0.00000460 Testing Loss: 0.00001142 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46300/100000] Training Loss: 0.00000371 Testing Loss: 0.00001014 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46350/100000] Training Loss: 0.00000371 Testing Loss: 0.00001021 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46400/100000] Training Loss: 0.00000393 Testing Loss: 0.00001027 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46450/100000] Training Loss: 0.00000441 Testing Loss: 0.00001092 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46500/100000] Training Loss: 0.00000367 Testing Loss: 0.00001009 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46550/100000] Training Loss: 0.00000435 Testing Loss: 0.00001068 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46600/100000] Training Loss: 0.00000491 Testing Loss: 0.00001092 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46650/100000] Training Loss: 0.00000392 Testing Loss: 0.00001042 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46700/100000] Training Loss: 0.00000392 Testing Loss: 0.00001035 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46750/100000] Training Loss: 0.00000455 Testing Loss: 0.00001060 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46800/100000] Training Loss: 0.00000357 Testing Loss: 0.00000998 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46850/100000] Training Loss: 0.00000382 Testing Loss: 0.00001023 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46900/100000] Training Loss: 0.00000392 Testing Loss: 0.00001024 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [46950/100000] Training Loss: 0.00000480 Testing Loss: 0.00001115 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47000/100000] Training Loss: 0.00000390 Testing Loss: 0.00001012 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47050/100000] Training Loss: 0.00000489 Testing Loss: 0.00001120 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47100/100000] Training Loss: 0.00000827 Testing Loss: 0.00001531 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47150/100000] Training Loss: 0.00000357 Testing Loss: 0.00000995 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47200/100000] Training Loss: 0.00000700 Testing Loss: 0.00001211 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47250/100000] Training Loss: 0.00000399 Testing Loss: 0.00001035 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47300/100000] Training Loss: 0.00000463 Testing Loss: 0.00001127 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47350/100000] Training Loss: 0.00000360 Testing Loss: 0.00000996 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47400/100000] Training Loss: 0.00000404 Testing Loss: 0.00001046 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47450/100000] Training Loss: 0.00000362 Testing Loss: 0.00000993 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47500/100000] Training Loss: 0.00000405 Testing Loss: 0.00001045 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47550/100000] Training Loss: 0.00000358 Testing Loss: 0.00000991 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47600/100000] Training Loss: 0.00000452 Testing Loss: 0.00001068 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47650/100000] Training Loss: 0.00000399 Testing Loss: 0.00001031 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47700/100000] Training Loss: 0.00000450 Testing Loss: 0.00001098 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47750/100000] Training Loss: 0.00000406 Testing Loss: 0.00001056 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47800/100000] Training Loss: 0.00000396 Testing Loss: 0.00001029 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47850/100000] Training Loss: 0.00000364 Testing Loss: 0.00000992 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47900/100000] Training Loss: 0.00000385 Testing Loss: 0.00001029 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [47950/100000] Training Loss: 0.00000445 Testing Loss: 0.00001084 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48000/100000] Training Loss: 0.00000399 Testing Loss: 0.00001027 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48050/100000] Training Loss: 0.00000374 Testing Loss: 0.00001002 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48100/100000] Training Loss: 0.00000553 Testing Loss: 0.00001222 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48150/100000] Training Loss: 0.00000389 Testing Loss: 0.00001017 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48200/100000] Training Loss: 0.00000399 Testing Loss: 0.00001030 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48250/100000] Training Loss: 0.00000388 Testing Loss: 0.00001016 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48300/100000] Training Loss: 0.00000544 Testing Loss: 0.00001152 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48350/100000] Training Loss: 0.00000356 Testing Loss: 0.00000983 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48400/100000] Training Loss: 0.00000455 Testing Loss: 0.00001058 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48450/100000] Training Loss: 0.00000593 Testing Loss: 0.00001248 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48500/100000] Training Loss: 0.00000373 Testing Loss: 0.00001001 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48550/100000] Training Loss: 0.00000364 Testing Loss: 0.00000978 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48600/100000] Training Loss: 0.00000381 Testing Loss: 0.00001002 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48650/100000] Training Loss: 0.00000417 Testing Loss: 0.00001085 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48700/100000] Training Loss: 0.00000429 Testing Loss: 0.00001040 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48750/100000] Training Loss: 0.00000655 Testing Loss: 0.00001318 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48800/100000] Training Loss: 0.00000376 Testing Loss: 0.00001004 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48850/100000] Training Loss: 0.00000653 Testing Loss: 0.00001283 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48900/100000] Training Loss: 0.00000405 Testing Loss: 0.00001036 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [48950/100000] Training Loss: 0.00000379 Testing Loss: 0.00000994 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49000/100000] Training Loss: 0.00000392 Testing Loss: 0.00001002 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49050/100000] Training Loss: 0.00000401 Testing Loss: 0.00001024 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49100/100000] Training Loss: 0.00000373 Testing Loss: 0.00000989 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49150/100000] Training Loss: 0.00000404 Testing Loss: 0.00001026 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49200/100000] Training Loss: 0.00000487 Testing Loss: 0.00001130 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49250/100000] Training Loss: 0.00000355 Testing Loss: 0.00000972 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49300/100000] Training Loss: 0.00000369 Testing Loss: 0.00000988 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49350/100000] Training Loss: 0.00000404 Testing Loss: 0.00001013 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49400/100000] Training Loss: 0.00000367 Testing Loss: 0.00000967 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49450/100000] Training Loss: 0.00000384 Testing Loss: 0.00001016 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49500/100000] Training Loss: 0.00001004 Testing Loss: 0.00001677 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49550/100000] Training Loss: 0.00000360 Testing Loss: 0.00000975 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49600/100000] Training Loss: 0.00000382 Testing Loss: 0.00001001 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49650/100000] Training Loss: 0.00000346 Testing Loss: 0.00000955 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49700/100000] Training Loss: 0.00000375 Testing Loss: 0.00000983 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49750/100000] Training Loss: 0.00000525 Testing Loss: 0.00001179 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49800/100000] Training Loss: 0.00000337 Testing Loss: 0.00000948 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49850/100000] Training Loss: 0.00000384 Testing Loss: 0.00001005 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49900/100000] Training Loss: 0.00000344 Testing Loss: 0.00000954 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [49950/100000] Training Loss: 0.00000367 Testing Loss: 0.00000981 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50000/100000] Training Loss: 0.00000401 Testing Loss: 0.00000974 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50050/100000] Training Loss: 0.00000393 Testing Loss: 0.00000994 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50100/100000] Training Loss: 0.00000416 Testing Loss: 0.00001035 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50150/100000] Training Loss: 0.00000385 Testing Loss: 0.00000982 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50200/100000] Training Loss: 0.00000377 Testing Loss: 0.00000981 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50250/100000] Training Loss: 0.00000494 Testing Loss: 0.00001185 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50300/100000] Training Loss: 0.00000340 Testing Loss: 0.00000948 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50350/100000] Training Loss: 0.00000475 Testing Loss: 0.00001080 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50400/100000] Training Loss: 0.00000779 Testing Loss: 0.00001415 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50450/100000] Training Loss: 0.00000356 Testing Loss: 0.00000964 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50500/100000] Training Loss: 0.00000445 Testing Loss: 0.00001060 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50550/100000] Training Loss: 0.00000367 Testing Loss: 0.00000975 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50600/100000] Training Loss: 0.00000373 Testing Loss: 0.00000994 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50650/100000] Training Loss: 0.00000374 Testing Loss: 0.00000980 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50700/100000] Training Loss: 0.00000418 Testing Loss: 0.00001036 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50750/100000] Training Loss: 0.00000402 Testing Loss: 0.00001000 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50800/100000] Training Loss: 0.00000375 Testing Loss: 0.00001000 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50850/100000] Training Loss: 0.00000416 Testing Loss: 0.00001022 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50900/100000] Training Loss: 0.00000982 Testing Loss: 0.00001519 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [50950/100000] Training Loss: 0.00000330 Testing Loss: 0.00000929 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51000/100000] Training Loss: 0.00000679 Testing Loss: 0.00001297 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51050/100000] Training Loss: 0.00000392 Testing Loss: 0.00000996 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51100/100000] Training Loss: 0.00000349 Testing Loss: 0.00000950 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51150/100000] Training Loss: 0.00000410 Testing Loss: 0.00001026 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51200/100000] Training Loss: 0.00000339 Testing Loss: 0.00000936 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51250/100000] Training Loss: 0.00000356 Testing Loss: 0.00000957 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51300/100000] Training Loss: 0.00000424 Testing Loss: 0.00001046 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51350/100000] Training Loss: 0.00000331 Testing Loss: 0.00000928 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51400/100000] Training Loss: 0.00000977 Testing Loss: 0.00001405 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51450/100000] Training Loss: 0.00000327 Testing Loss: 0.00000921 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51500/100000] Training Loss: 0.00000350 Testing Loss: 0.00000947 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51550/100000] Training Loss: 0.00000347 Testing Loss: 0.00000938 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51600/100000] Training Loss: 0.00000672 Testing Loss: 0.00001310 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51650/100000] Training Loss: 0.00000329 Testing Loss: 0.00000923 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51700/100000] Training Loss: 0.00000472 Testing Loss: 0.00001031 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51750/100000] Training Loss: 0.00000375 Testing Loss: 0.00000962 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51800/100000] Training Loss: 0.00000754 Testing Loss: 0.00001489 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51850/100000] Training Loss: 0.00000328 Testing Loss: 0.00000919 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51900/100000] Training Loss: 0.00000396 Testing Loss: 0.00000974 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [51950/100000] Training Loss: 0.00001005 Testing Loss: 0.00001855 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52000/100000] Training Loss: 0.00000324 Testing Loss: 0.00000913 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52050/100000] Training Loss: 0.00000373 Testing Loss: 0.00000966 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52100/100000] Training Loss: 0.00000377 Testing Loss: 0.00000969 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52150/100000] Training Loss: 0.00000414 Testing Loss: 0.00001024 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52200/100000] Training Loss: 0.00000383 Testing Loss: 0.00000981 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52250/100000] Training Loss: 0.00000407 Testing Loss: 0.00001018 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52300/100000] Training Loss: 0.00000325 Testing Loss: 0.00000913 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52350/100000] Training Loss: 0.00000540 Testing Loss: 0.00001205 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52400/100000] Training Loss: 0.00000325 Testing Loss: 0.00000912 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52450/100000] Training Loss: 0.00000350 Testing Loss: 0.00000936 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52500/100000] Training Loss: 0.00001225 Testing Loss: 0.00001995 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52550/100000] Training Loss: 0.00000322 Testing Loss: 0.00000908 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52600/100000] Training Loss: 0.00000411 Testing Loss: 0.00001006 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52650/100000] Training Loss: 0.00000434 Testing Loss: 0.00000987 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52700/100000] Training Loss: 0.00000389 Testing Loss: 0.00000982 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52750/100000] Training Loss: 0.00000342 Testing Loss: 0.00000925 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52800/100000] Training Loss: 0.00000453 Testing Loss: 0.00000987 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52850/100000] Training Loss: 0.00000362 Testing Loss: 0.00000952 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52900/100000] Training Loss: 0.00000366 Testing Loss: 0.00000949 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [52950/100000] Training Loss: 0.00000374 Testing Loss: 0.00000950 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53000/100000] Training Loss: 0.00000340 Testing Loss: 0.00000926 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53050/100000] Training Loss: 0.00000431 Testing Loss: 0.00001032 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53100/100000] Training Loss: 0.00000330 Testing Loss: 0.00000911 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53150/100000] Training Loss: 0.00000444 Testing Loss: 0.00001058 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53200/100000] Training Loss: 0.00000319 Testing Loss: 0.00000900 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53250/100000] Training Loss: 0.00000344 Testing Loss: 0.00000926 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53300/100000] Training Loss: 0.00000377 Testing Loss: 0.00000944 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53350/100000] Training Loss: 0.00000339 Testing Loss: 0.00000925 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53400/100000] Training Loss: 0.00000451 Testing Loss: 0.00000973 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53450/100000] Training Loss: 0.00000406 Testing Loss: 0.00000993 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53500/100000] Training Loss: 0.00000388 Testing Loss: 0.00000967 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53550/100000] Training Loss: 0.00000353 Testing Loss: 0.00000933 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53600/100000] Training Loss: 0.00000387 Testing Loss: 0.00000952 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53650/100000] Training Loss: 0.00000354 Testing Loss: 0.00000946 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53700/100000] Training Loss: 0.00000329 Testing Loss: 0.00000908 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53750/100000] Training Loss: 0.00000350 Testing Loss: 0.00000930 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53800/100000] Training Loss: 0.00000313 Testing Loss: 0.00000889 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53850/100000] Training Loss: 0.00000384 Testing Loss: 0.00000966 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53900/100000] Training Loss: 0.00000327 Testing Loss: 0.00000901 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [53950/100000] Training Loss: 0.00000640 Testing Loss: 0.00001300 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54000/100000] Training Loss: 0.00000317 Testing Loss: 0.00000891 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54050/100000] Training Loss: 0.00000361 Testing Loss: 0.00000938 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54100/100000] Training Loss: 0.00000325 Testing Loss: 0.00000908 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54150/100000] Training Loss: 0.00000348 Testing Loss: 0.00000925 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54200/100000] Training Loss: 0.00000403 Testing Loss: 0.00001006 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54250/100000] Training Loss: 0.00000403 Testing Loss: 0.00000979 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54300/100000] Training Loss: 0.00000457 Testing Loss: 0.00000996 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54350/100000] Training Loss: 0.00000347 Testing Loss: 0.00000918 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54400/100000] Training Loss: 0.00000355 Testing Loss: 0.00000938 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54450/100000] Training Loss: 0.00000370 Testing Loss: 0.00000953 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54500/100000] Training Loss: 0.00000499 Testing Loss: 0.00001108 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54550/100000] Training Loss: 0.00000331 Testing Loss: 0.00000898 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54600/100000] Training Loss: 0.00000415 Testing Loss: 0.00001013 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54650/100000] Training Loss: 0.00000318 Testing Loss: 0.00000887 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54700/100000] Training Loss: 0.00000329 Testing Loss: 0.00000898 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54750/100000] Training Loss: 0.00000333 Testing Loss: 0.00000897 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54800/100000] Training Loss: 0.00000453 Testing Loss: 0.00000995 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54850/100000] Training Loss: 0.00000338 Testing Loss: 0.00000906 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54900/100000] Training Loss: 0.00000341 Testing Loss: 0.00000905 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [54950/100000] Training Loss: 0.00000481 Testing Loss: 0.00000951 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55000/100000] Training Loss: 0.00000369 Testing Loss: 0.00000944 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55050/100000] Training Loss: 0.00000369 Testing Loss: 0.00000948 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55100/100000] Training Loss: 0.00000316 Testing Loss: 0.00000880 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55150/100000] Training Loss: 0.00000838 Testing Loss: 0.00001362 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55200/100000] Training Loss: 0.00000307 Testing Loss: 0.00000872 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55250/100000] Training Loss: 0.00000482 Testing Loss: 0.00001059 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55300/100000] Training Loss: 0.00000409 Testing Loss: 0.00000970 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55350/100000] Training Loss: 0.00000640 Testing Loss: 0.00001334 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55400/100000] Training Loss: 0.00000306 Testing Loss: 0.00000869 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55450/100000] Training Loss: 0.00000340 Testing Loss: 0.00000901 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55500/100000] Training Loss: 0.00000350 Testing Loss: 0.00000913 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55550/100000] Training Loss: 0.00000363 Testing Loss: 0.00000919 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55600/100000] Training Loss: 0.00000341 Testing Loss: 0.00000905 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55650/100000] Training Loss: 0.00000351 Testing Loss: 0.00000918 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55700/100000] Training Loss: 0.00000323 Testing Loss: 0.00000881 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55750/100000] Training Loss: 0.00000603 Testing Loss: 0.00001149 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55800/100000] Training Loss: 0.00000346 Testing Loss: 0.00000902 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55850/100000] Training Loss: 0.00000319 Testing Loss: 0.00000898 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55900/100000] Training Loss: 0.00000401 Testing Loss: 0.00000966 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [55950/100000] Training Loss: 0.00000337 Testing Loss: 0.00000902 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56000/100000] Training Loss: 0.00000387 Testing Loss: 0.00000962 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56050/100000] Training Loss: 0.00000302 Testing Loss: 0.00000860 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56100/100000] Training Loss: 0.00000319 Testing Loss: 0.00000875 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56150/100000] Training Loss: 0.00000610 Testing Loss: 0.00001166 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56200/100000] Training Loss: 0.00000349 Testing Loss: 0.00000907 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56250/100000] Training Loss: 0.00000323 Testing Loss: 0.00000877 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56300/100000] Training Loss: 0.00000347 Testing Loss: 0.00000907 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56350/100000] Training Loss: 0.00000312 Testing Loss: 0.00000868 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56400/100000] Training Loss: 0.00000442 Testing Loss: 0.00000997 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56450/100000] Training Loss: 0.00000430 Testing Loss: 0.00000953 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56500/100000] Training Loss: 0.00000396 Testing Loss: 0.00000949 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56550/100000] Training Loss: 0.00000342 Testing Loss: 0.00000890 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56600/100000] Training Loss: 0.00000382 Testing Loss: 0.00000945 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56650/100000] Training Loss: 0.00000339 Testing Loss: 0.00000895 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56700/100000] Training Loss: 0.00000346 Testing Loss: 0.00000905 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56750/100000] Training Loss: 0.00000321 Testing Loss: 0.00000883 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56800/100000] Training Loss: 0.00000363 Testing Loss: 0.00000924 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56850/100000] Training Loss: 0.00000325 Testing Loss: 0.00000880 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56900/100000] Training Loss: 0.00000357 Testing Loss: 0.00000918 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [56950/100000] Training Loss: 0.00000381 Testing Loss: 0.00000938 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57000/100000] Training Loss: 0.00000305 Testing Loss: 0.00000857 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57050/100000] Training Loss: 0.00000324 Testing Loss: 0.00000877 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57100/100000] Training Loss: 0.00000312 Testing Loss: 0.00000862 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57150/100000] Training Loss: 0.00000382 Testing Loss: 0.00000906 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57200/100000] Training Loss: 0.00000393 Testing Loss: 0.00000967 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57250/100000] Training Loss: 0.00000305 Testing Loss: 0.00000854 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57300/100000] Training Loss: 0.00000396 Testing Loss: 0.00000939 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57350/100000] Training Loss: 0.00000314 Testing Loss: 0.00000858 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57400/100000] Training Loss: 0.00000336 Testing Loss: 0.00000889 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57450/100000] Training Loss: 0.00000308 Testing Loss: 0.00000859 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57500/100000] Training Loss: 0.00000331 Testing Loss: 0.00000882 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57550/100000] Training Loss: 0.00000378 Testing Loss: 0.00000896 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57600/100000] Training Loss: 0.00000322 Testing Loss: 0.00000879 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57650/100000] Training Loss: 0.00000388 Testing Loss: 0.00000958 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57700/100000] Training Loss: 0.00000305 Testing Loss: 0.00000850 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57750/100000] Training Loss: 0.00000324 Testing Loss: 0.00000870 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57800/100000] Training Loss: 0.00000446 Testing Loss: 0.00001047 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57850/100000] Training Loss: 0.00000320 Testing Loss: 0.00000874 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57900/100000] Training Loss: 0.00000657 Testing Loss: 0.00001309 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [57950/100000] Training Loss: 0.00000299 Testing Loss: 0.00000845 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58000/100000] Training Loss: 0.00000358 Testing Loss: 0.00000911 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58050/100000] Training Loss: 0.00000360 Testing Loss: 0.00000940 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58100/100000] Training Loss: 0.00000326 Testing Loss: 0.00000881 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58150/100000] Training Loss: 0.00000326 Testing Loss: 0.00000872 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58200/100000] Training Loss: 0.00000410 Testing Loss: 0.00000897 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58250/100000] Training Loss: 0.00000335 Testing Loss: 0.00000888 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58300/100000] Training Loss: 0.00000479 Testing Loss: 0.00000933 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58350/100000] Training Loss: 0.00000365 Testing Loss: 0.00000922 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58400/100000] Training Loss: 0.00000384 Testing Loss: 0.00000960 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58450/100000] Training Loss: 0.00000298 Testing Loss: 0.00000837 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58500/100000] Training Loss: 0.00000322 Testing Loss: 0.00000862 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58550/100000] Training Loss: 0.00000392 Testing Loss: 0.00000999 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58600/100000] Training Loss: 0.00000357 Testing Loss: 0.00000899 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58650/100000] Training Loss: 0.00000521 Testing Loss: 0.00001116 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58700/100000] Training Loss: 0.00000295 Testing Loss: 0.00000834 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58750/100000] Training Loss: 0.00000871 Testing Loss: 0.00001423 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58800/100000] Training Loss: 0.00000297 Testing Loss: 0.00000837 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58850/100000] Training Loss: 0.00000512 Testing Loss: 0.00000984 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58900/100000] Training Loss: 0.00000416 Testing Loss: 0.00000949 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [58950/100000] Training Loss: 0.00000323 Testing Loss: 0.00000856 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59000/100000] Training Loss: 0.00000805 Testing Loss: 0.00001487 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59050/100000] Training Loss: 0.00000290 Testing Loss: 0.00000828 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59100/100000] Training Loss: 0.00000324 Testing Loss: 0.00000869 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59150/100000] Training Loss: 0.00000342 Testing Loss: 0.00000852 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59200/100000] Training Loss: 0.00000329 Testing Loss: 0.00000869 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59250/100000] Training Loss: 0.00000323 Testing Loss: 0.00000860 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59300/100000] Training Loss: 0.00000337 Testing Loss: 0.00000875 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59350/100000] Training Loss: 0.00000403 Testing Loss: 0.00000911 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59400/100000] Training Loss: 0.00000328 Testing Loss: 0.00000867 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59450/100000] Training Loss: 0.00000332 Testing Loss: 0.00000850 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59500/100000] Training Loss: 0.00000344 Testing Loss: 0.00000875 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59550/100000] Training Loss: 0.00000317 Testing Loss: 0.00000849 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59600/100000] Training Loss: 0.00000373 Testing Loss: 0.00000912 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59650/100000] Training Loss: 0.00000291 Testing Loss: 0.00000826 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59700/100000] Training Loss: 0.00000353 Testing Loss: 0.00000887 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59750/100000] Training Loss: 0.00000332 Testing Loss: 0.00000846 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59800/100000] Training Loss: 0.00000329 Testing Loss: 0.00000857 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59850/100000] Training Loss: 0.00000536 Testing Loss: 0.00001159 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59900/100000] Training Loss: 0.00000291 Testing Loss: 0.00000824 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [59950/100000] Training Loss: 0.00000319 Testing Loss: 0.00000850 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60000/100000] Training Loss: 0.00000441 Testing Loss: 0.00000888 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60050/100000] Training Loss: 0.00000333 Testing Loss: 0.00000870 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60100/100000] Training Loss: 0.00000859 Testing Loss: 0.00001359 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60150/100000] Training Loss: 0.00000294 Testing Loss: 0.00000826 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60200/100000] Training Loss: 0.00000672 Testing Loss: 0.00001321 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60250/100000] Training Loss: 0.00000287 Testing Loss: 0.00000817 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60300/100000] Training Loss: 0.00000308 Testing Loss: 0.00000836 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60350/100000] Training Loss: 0.00000370 Testing Loss: 0.00000931 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60400/100000] Training Loss: 0.00000325 Testing Loss: 0.00000865 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60450/100000] Training Loss: 0.00000352 Testing Loss: 0.00000894 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60500/100000] Training Loss: 0.00000288 Testing Loss: 0.00000816 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60550/100000] Training Loss: 0.00000598 Testing Loss: 0.00001209 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60600/100000] Training Loss: 0.00000283 Testing Loss: 0.00000812 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60650/100000] Training Loss: 0.00000440 Testing Loss: 0.00001019 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60700/100000] Training Loss: 0.00000289 Testing Loss: 0.00000816 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60750/100000] Training Loss: 0.00000478 Testing Loss: 0.00000954 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60800/100000] Training Loss: 0.00000365 Testing Loss: 0.00000902 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60850/100000] Training Loss: 0.00000450 Testing Loss: 0.00000851 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60900/100000] Training Loss: 0.00000283 Testing Loss: 0.00000810 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [60950/100000] Training Loss: 0.00000307 Testing Loss: 0.00000832 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61000/100000] Training Loss: 0.00000566 Testing Loss: 0.00001152 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61050/100000] Training Loss: 0.00000293 Testing Loss: 0.00000820 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61100/100000] Training Loss: 0.00000669 Testing Loss: 0.00001103 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61150/100000] Training Loss: 0.00000300 Testing Loss: 0.00000829 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61200/100000] Training Loss: 0.00000903 Testing Loss: 0.00001255 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61250/100000] Training Loss: 0.00000283 Testing Loss: 0.00000808 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61300/100000] Training Loss: 0.00000339 Testing Loss: 0.00000872 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61350/100000] Training Loss: 0.00000316 Testing Loss: 0.00000842 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61400/100000] Training Loss: 0.00000313 Testing Loss: 0.00000835 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61450/100000] Training Loss: 0.00000380 Testing Loss: 0.00000892 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61500/100000] Training Loss: 0.00000313 Testing Loss: 0.00000833 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61550/100000] Training Loss: 0.00000355 Testing Loss: 0.00000886 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61600/100000] Training Loss: 0.00000283 Testing Loss: 0.00000804 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61650/100000] Training Loss: 0.00000314 Testing Loss: 0.00000833 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61700/100000] Training Loss: 0.00000294 Testing Loss: 0.00000815 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61750/100000] Training Loss: 0.00000281 Testing Loss: 0.00000803 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61800/100000] Training Loss: 0.00000342 Testing Loss: 0.00000869 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61850/100000] Training Loss: 0.00000334 Testing Loss: 0.00000859 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61900/100000] Training Loss: 0.00000291 Testing Loss: 0.00000810 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [61950/100000] Training Loss: 0.00000562 Testing Loss: 0.00001193 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62000/100000] Training Loss: 0.00000283 Testing Loss: 0.00000803 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62050/100000] Training Loss: 0.00000312 Testing Loss: 0.00000833 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62100/100000] Training Loss: 0.00000333 Testing Loss: 0.00000849 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62150/100000] Training Loss: 0.00000379 Testing Loss: 0.00000880 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62200/100000] Training Loss: 0.00000296 Testing Loss: 0.00000819 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62250/100000] Training Loss: 0.00000337 Testing Loss: 0.00000859 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62300/100000] Training Loss: 0.00000280 Testing Loss: 0.00000798 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62350/100000] Training Loss: 0.00000329 Testing Loss: 0.00000845 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62400/100000] Training Loss: 0.00000298 Testing Loss: 0.00000811 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62450/100000] Training Loss: 0.00000417 Testing Loss: 0.00000885 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62500/100000] Training Loss: 0.00000317 Testing Loss: 0.00000834 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62550/100000] Training Loss: 0.00000324 Testing Loss: 0.00000835 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62600/100000] Training Loss: 0.00000351 Testing Loss: 0.00000845 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62650/100000] Training Loss: 0.00000515 Testing Loss: 0.00001009 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62700/100000] Training Loss: 0.00000367 Testing Loss: 0.00000869 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62750/100000] Training Loss: 0.00000663 Testing Loss: 0.00001221 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62800/100000] Training Loss: 0.00000294 Testing Loss: 0.00000816 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62850/100000] Training Loss: 0.00000348 Testing Loss: 0.00000859 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62900/100000] Training Loss: 0.00000325 Testing Loss: 0.00000841 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [62950/100000] Training Loss: 0.00000302 Testing Loss: 0.00000805 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63000/100000] Training Loss: 0.00000306 Testing Loss: 0.00000823 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63050/100000] Training Loss: 0.00000693 Testing Loss: 0.00001088 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63100/100000] Training Loss: 0.00000273 Testing Loss: 0.00000786 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63150/100000] Training Loss: 0.00000316 Testing Loss: 0.00000841 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63200/100000] Training Loss: 0.00000368 Testing Loss: 0.00000877 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63250/100000] Training Loss: 0.00000281 Testing Loss: 0.00000794 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63300/100000] Training Loss: 0.00000347 Testing Loss: 0.00000863 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63350/100000] Training Loss: 0.00000339 Testing Loss: 0.00000796 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63400/100000] Training Loss: 0.00000356 Testing Loss: 0.00000861 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63450/100000] Training Loss: 0.00000356 Testing Loss: 0.00000868 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63500/100000] Training Loss: 0.00000671 Testing Loss: 0.00001150 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63550/100000] Training Loss: 0.00000397 Testing Loss: 0.00000918 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63600/100000] Training Loss: 0.00000360 Testing Loss: 0.00000871 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63650/100000] Training Loss: 0.00000317 Testing Loss: 0.00000829 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63700/100000] Training Loss: 0.00000292 Testing Loss: 0.00000804 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63750/100000] Training Loss: 0.00000312 Testing Loss: 0.00000822 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63800/100000] Training Loss: 0.00000305 Testing Loss: 0.00000807 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63850/100000] Training Loss: 0.00000364 Testing Loss: 0.00000891 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63900/100000] Training Loss: 0.00000279 Testing Loss: 0.00000783 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [63950/100000] Training Loss: 0.00000303 Testing Loss: 0.00000810 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64000/100000] Training Loss: 0.00000554 Testing Loss: 0.00000913 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64050/100000] Training Loss: 0.00000269 Testing Loss: 0.00000776 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64100/100000] Training Loss: 0.00000291 Testing Loss: 0.00000797 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64150/100000] Training Loss: 0.00000316 Testing Loss: 0.00000825 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64200/100000] Training Loss: 0.00000323 Testing Loss: 0.00000814 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64250/100000] Training Loss: 0.00000312 Testing Loss: 0.00000820 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64300/100000] Training Loss: 0.00000299 Testing Loss: 0.00000814 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64350/100000] Training Loss: 0.00000493 Testing Loss: 0.00001045 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64400/100000] Training Loss: 0.00000287 Testing Loss: 0.00000795 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64450/100000] Training Loss: 0.00000481 Testing Loss: 0.00000958 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64500/100000] Training Loss: 0.00000276 Testing Loss: 0.00000783 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64550/100000] Training Loss: 0.00000330 Testing Loss: 0.00000835 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64600/100000] Training Loss: 0.00000292 Testing Loss: 0.00000788 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64650/100000] Training Loss: 0.00000312 Testing Loss: 0.00000814 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64700/100000] Training Loss: 0.00000297 Testing Loss: 0.00000800 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64750/100000] Training Loss: 0.00000298 Testing Loss: 0.00000794 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64800/100000] Training Loss: 0.00000374 Testing Loss: 0.00000858 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64850/100000] Training Loss: 0.00000653 Testing Loss: 0.00001233 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64900/100000] Training Loss: 0.00000274 Testing Loss: 0.00000777 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [64950/100000] Training Loss: 0.00000353 Testing Loss: 0.00000894 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65000/100000] Training Loss: 0.00000355 Testing Loss: 0.00000868 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65050/100000] Training Loss: 0.00000330 Testing Loss: 0.00000839 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65100/100000] Training Loss: 0.00000364 Testing Loss: 0.00000872 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65150/100000] Training Loss: 0.00000366 Testing Loss: 0.00000887 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65200/100000] Training Loss: 0.00000272 Testing Loss: 0.00000772 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65250/100000] Training Loss: 0.00000384 Testing Loss: 0.00000904 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65300/100000] Training Loss: 0.00000318 Testing Loss: 0.00000827 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65350/100000] Training Loss: 0.00000279 Testing Loss: 0.00000777 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65400/100000] Training Loss: 0.00000591 Testing Loss: 0.00001155 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65450/100000] Training Loss: 0.00000274 Testing Loss: 0.00000775 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65500/100000] Training Loss: 0.00000307 Testing Loss: 0.00000811 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65550/100000] Training Loss: 0.00000305 Testing Loss: 0.00000827 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65600/100000] Training Loss: 0.00000261 Testing Loss: 0.00000760 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65650/100000] Training Loss: 0.00000277 Testing Loss: 0.00000777 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65700/100000] Training Loss: 0.00000303 Testing Loss: 0.00000800 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65750/100000] Training Loss: 0.00000312 Testing Loss: 0.00000802 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65800/100000] Training Loss: 0.00000318 Testing Loss: 0.00000811 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65850/100000] Training Loss: 0.00000320 Testing Loss: 0.00000789 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65900/100000] Training Loss: 0.00000374 Testing Loss: 0.00000846 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [65950/100000] Training Loss: 0.00000293 Testing Loss: 0.00000793 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66000/100000] Training Loss: 0.00000289 Testing Loss: 0.00000782 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66050/100000] Training Loss: 0.00000484 Testing Loss: 0.00000916 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66100/100000] Training Loss: 0.00000265 Testing Loss: 0.00000762 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66150/100000] Training Loss: 0.00000439 Testing Loss: 0.00000924 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66200/100000] Training Loss: 0.00000277 Testing Loss: 0.00000778 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66250/100000] Training Loss: 0.00000353 Testing Loss: 0.00000820 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66300/100000] Training Loss: 0.00000360 Testing Loss: 0.00000850 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66350/100000] Training Loss: 0.00000326 Testing Loss: 0.00000788 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66400/100000] Training Loss: 0.00000323 Testing Loss: 0.00000807 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66450/100000] Training Loss: 0.00000339 Testing Loss: 0.00000847 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66500/100000] Training Loss: 0.00000273 Testing Loss: 0.00000767 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66550/100000] Training Loss: 0.00000521 Testing Loss: 0.00000914 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66600/100000] Training Loss: 0.00000280 Testing Loss: 0.00000779 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66650/100000] Training Loss: 0.00000628 Testing Loss: 0.00001259 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66700/100000] Training Loss: 0.00000265 Testing Loss: 0.00000757 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66750/100000] Training Loss: 0.00000314 Testing Loss: 0.00000811 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66800/100000] Training Loss: 0.00000371 Testing Loss: 0.00000843 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66850/100000] Training Loss: 0.00000343 Testing Loss: 0.00000849 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66900/100000] Training Loss: 0.00000273 Testing Loss: 0.00000765 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [66950/100000] Training Loss: 0.00000311 Testing Loss: 0.00000805 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67000/100000] Training Loss: 0.00000315 Testing Loss: 0.00000810 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67050/100000] Training Loss: 0.00000305 Testing Loss: 0.00000791 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67100/100000] Training Loss: 0.00000427 Testing Loss: 0.00000925 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67150/100000] Training Loss: 0.00000302 Testing Loss: 0.00000798 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67200/100000] Training Loss: 0.00001017 Testing Loss: 0.00001413 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67250/100000] Training Loss: 0.00000259 Testing Loss: 0.00000748 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67300/100000] Training Loss: 0.00000283 Testing Loss: 0.00000777 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67350/100000] Training Loss: 0.00000277 Testing Loss: 0.00000763 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67400/100000] Training Loss: 0.00000284 Testing Loss: 0.00000768 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67450/100000] Training Loss: 0.00000299 Testing Loss: 0.00000769 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67500/100000] Training Loss: 0.00000365 Testing Loss: 0.00000841 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67550/100000] Training Loss: 0.00000896 Testing Loss: 0.00001594 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67600/100000] Training Loss: 0.00000257 Testing Loss: 0.00000747 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67650/100000] Training Loss: 0.00000321 Testing Loss: 0.00000814 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67700/100000] Training Loss: 0.00000317 Testing Loss: 0.00000806 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67750/100000] Training Loss: 0.00000276 Testing Loss: 0.00000753 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67800/100000] Training Loss: 0.00000348 Testing Loss: 0.00000859 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67850/100000] Training Loss: 0.00000355 Testing Loss: 0.00000859 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67900/100000] Training Loss: 0.00000273 Testing Loss: 0.00000761 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [67950/100000] Training Loss: 0.00000292 Testing Loss: 0.00000772 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68000/100000] Training Loss: 0.00000324 Testing Loss: 0.00000819 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68050/100000] Training Loss: 0.00000277 Testing Loss: 0.00000763 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68100/100000] Training Loss: 0.00000297 Testing Loss: 0.00000786 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68150/100000] Training Loss: 0.00000316 Testing Loss: 0.00000785 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68200/100000] Training Loss: 0.00000289 Testing Loss: 0.00000773 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68250/100000] Training Loss: 0.00000352 Testing Loss: 0.00000827 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68300/100000] Training Loss: 0.00000304 Testing Loss: 0.00000794 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68350/100000] Training Loss: 0.00000293 Testing Loss: 0.00000780 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68400/100000] Training Loss: 0.00000352 Testing Loss: 0.00000847 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68450/100000] Training Loss: 0.00000414 Testing Loss: 0.00000870 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68500/100000] Training Loss: 0.00000262 Testing Loss: 0.00000748 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68550/100000] Training Loss: 0.00000737 Testing Loss: 0.00001292 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68600/100000] Training Loss: 0.00000268 Testing Loss: 0.00000755 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68650/100000] Training Loss: 0.00000755 Testing Loss: 0.00001255 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68700/100000] Training Loss: 0.00000306 Testing Loss: 0.00000789 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68750/100000] Training Loss: 0.00001037 Testing Loss: 0.00001650 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68800/100000] Training Loss: 0.00000255 Testing Loss: 0.00000736 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68850/100000] Training Loss: 0.00000303 Testing Loss: 0.00000778 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68900/100000] Training Loss: 0.00000366 Testing Loss: 0.00000827 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [68950/100000] Training Loss: 0.00000344 Testing Loss: 0.00000808 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69000/100000] Training Loss: 0.00000317 Testing Loss: 0.00000798 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69050/100000] Training Loss: 0.00000359 Testing Loss: 0.00000834 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69100/100000] Training Loss: 0.00000251 Testing Loss: 0.00000731 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69150/100000] Training Loss: 0.00000264 Testing Loss: 0.00000744 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69200/100000] Training Loss: 0.00000357 Testing Loss: 0.00000843 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69250/100000] Training Loss: 0.00000270 Testing Loss: 0.00000754 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69300/100000] Training Loss: 0.00000302 Testing Loss: 0.00000765 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69350/100000] Training Loss: 0.00000391 Testing Loss: 0.00000858 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69400/100000] Training Loss: 0.00000338 Testing Loss: 0.00000848 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69450/100000] Training Loss: 0.00000297 Testing Loss: 0.00000784 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69500/100000] Training Loss: 0.00000310 Testing Loss: 0.00000800 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69550/100000] Training Loss: 0.00000265 Testing Loss: 0.00000751 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69600/100000] Training Loss: 0.00000314 Testing Loss: 0.00000792 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69650/100000] Training Loss: 0.00000279 Testing Loss: 0.00000756 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69700/100000] Training Loss: 0.00000271 Testing Loss: 0.00000746 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69750/100000] Training Loss: 0.00000376 Testing Loss: 0.00000845 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69800/100000] Training Loss: 0.00000264 Testing Loss: 0.00000743 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69850/100000] Training Loss: 0.00000354 Testing Loss: 0.00000849 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69900/100000] Training Loss: 0.00000588 Testing Loss: 0.00001181 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [69950/100000] Training Loss: 0.00000259 Testing Loss: 0.00000737 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70000/100000] Training Loss: 0.00000263 Testing Loss: 0.00000740 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70050/100000] Training Loss: 0.00000289 Testing Loss: 0.00000784 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70100/100000] Training Loss: 0.00000304 Testing Loss: 0.00000799 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70150/100000] Training Loss: 0.00000378 Testing Loss: 0.00000856 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70200/100000] Training Loss: 0.00000293 Testing Loss: 0.00000779 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70250/100000] Training Loss: 0.00000268 Testing Loss: 0.00000737 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70300/100000] Training Loss: 0.00000314 Testing Loss: 0.00000790 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70350/100000] Training Loss: 0.00000293 Testing Loss: 0.00000758 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70400/100000] Training Loss: 0.00000479 Testing Loss: 0.00001007 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70450/100000] Training Loss: 0.00000260 Testing Loss: 0.00000737 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70500/100000] Training Loss: 0.00000341 Testing Loss: 0.00000817 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70550/100000] Training Loss: 0.00000313 Testing Loss: 0.00000795 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70600/100000] Training Loss: 0.00000256 Testing Loss: 0.00000728 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70650/100000] Training Loss: 0.00000511 Testing Loss: 0.00001068 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70700/100000] Training Loss: 0.00000271 Testing Loss: 0.00000744 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70750/100000] Training Loss: 0.00000290 Testing Loss: 0.00000764 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70800/100000] Training Loss: 0.00000259 Testing Loss: 0.00000733 lr: [0.00012959999999999998]\n",
      "**************************************************\n",
      "Epoch [70850/100000] Training Loss: 0.00000246 Testing Loss: 0.00000719 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [70900/100000] Training Loss: 0.00000244 Testing Loss: 0.00000716 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [70950/100000] Training Loss: 0.00000244 Testing Loss: 0.00000715 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71000/100000] Training Loss: 0.00000244 Testing Loss: 0.00000715 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71050/100000] Training Loss: 0.00000244 Testing Loss: 0.00000715 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71100/100000] Training Loss: 0.00000243 Testing Loss: 0.00000714 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71150/100000] Training Loss: 0.00000243 Testing Loss: 0.00000714 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71200/100000] Training Loss: 0.00000243 Testing Loss: 0.00000714 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71250/100000] Training Loss: 0.00000243 Testing Loss: 0.00000714 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71300/100000] Training Loss: 0.00000243 Testing Loss: 0.00000713 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71350/100000] Training Loss: 0.00000243 Testing Loss: 0.00000713 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71400/100000] Training Loss: 0.00000243 Testing Loss: 0.00000713 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71450/100000] Training Loss: 0.00000243 Testing Loss: 0.00000712 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71500/100000] Training Loss: 0.00000242 Testing Loss: 0.00000712 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71550/100000] Training Loss: 0.00000242 Testing Loss: 0.00000711 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71600/100000] Training Loss: 0.00000242 Testing Loss: 0.00000711 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71650/100000] Training Loss: 0.00000242 Testing Loss: 0.00000711 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71700/100000] Training Loss: 0.00000242 Testing Loss: 0.00000710 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71750/100000] Training Loss: 0.00000242 Testing Loss: 0.00000710 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71800/100000] Training Loss: 0.00000242 Testing Loss: 0.00000709 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71850/100000] Training Loss: 0.00000241 Testing Loss: 0.00000709 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71900/100000] Training Loss: 0.00000323 Testing Loss: 0.00000817 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [71950/100000] Training Loss: 0.00000245 Testing Loss: 0.00000712 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72000/100000] Training Loss: 0.00000241 Testing Loss: 0.00000709 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72050/100000] Training Loss: 0.00000255 Testing Loss: 0.00000724 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72100/100000] Training Loss: 0.00000247 Testing Loss: 0.00000714 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72150/100000] Training Loss: 0.00000256 Testing Loss: 0.00000719 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72200/100000] Training Loss: 0.00000269 Testing Loss: 0.00000733 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72250/100000] Training Loss: 0.00000249 Testing Loss: 0.00000713 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72300/100000] Training Loss: 0.00000266 Testing Loss: 0.00000733 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72350/100000] Training Loss: 0.00000249 Testing Loss: 0.00000714 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72400/100000] Training Loss: 0.00000251 Testing Loss: 0.00000719 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72450/100000] Training Loss: 0.00000281 Testing Loss: 0.00000745 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72500/100000] Training Loss: 0.00000243 Testing Loss: 0.00000709 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72550/100000] Training Loss: 0.00000256 Testing Loss: 0.00000719 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72600/100000] Training Loss: 0.00000310 Testing Loss: 0.00000748 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72650/100000] Training Loss: 0.00000270 Testing Loss: 0.00000735 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72700/100000] Training Loss: 0.00000261 Testing Loss: 0.00000723 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72750/100000] Training Loss: 0.00000264 Testing Loss: 0.00000738 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72800/100000] Training Loss: 0.00000240 Testing Loss: 0.00000704 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72850/100000] Training Loss: 0.00000252 Testing Loss: 0.00000717 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72900/100000] Training Loss: 0.00000245 Testing Loss: 0.00000708 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [72950/100000] Training Loss: 0.00000257 Testing Loss: 0.00000723 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73000/100000] Training Loss: 0.00000282 Testing Loss: 0.00000770 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73050/100000] Training Loss: 0.00000239 Testing Loss: 0.00000702 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73100/100000] Training Loss: 0.00000252 Testing Loss: 0.00000716 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73150/100000] Training Loss: 0.00000240 Testing Loss: 0.00000701 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73200/100000] Training Loss: 0.00000265 Testing Loss: 0.00000727 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73250/100000] Training Loss: 0.00000251 Testing Loss: 0.00000709 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73300/100000] Training Loss: 0.00000261 Testing Loss: 0.00000722 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73350/100000] Training Loss: 0.00000332 Testing Loss: 0.00000812 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73400/100000] Training Loss: 0.00000238 Testing Loss: 0.00000698 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73450/100000] Training Loss: 0.00000251 Testing Loss: 0.00000713 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73500/100000] Training Loss: 0.00000248 Testing Loss: 0.00000708 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73550/100000] Training Loss: 0.00000313 Testing Loss: 0.00000792 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73600/100000] Training Loss: 0.00000238 Testing Loss: 0.00000698 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73650/100000] Training Loss: 0.00000248 Testing Loss: 0.00000707 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73700/100000] Training Loss: 0.00000263 Testing Loss: 0.00000727 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73750/100000] Training Loss: 0.00000247 Testing Loss: 0.00000707 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73800/100000] Training Loss: 0.00000248 Testing Loss: 0.00000706 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73850/100000] Training Loss: 0.00000248 Testing Loss: 0.00000707 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73900/100000] Training Loss: 0.00000290 Testing Loss: 0.00000749 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [73950/100000] Training Loss: 0.00000239 Testing Loss: 0.00000697 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74000/100000] Training Loss: 0.00000295 Testing Loss: 0.00000746 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74050/100000] Training Loss: 0.00000275 Testing Loss: 0.00000740 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74100/100000] Training Loss: 0.00000248 Testing Loss: 0.00000704 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74150/100000] Training Loss: 0.00000283 Testing Loss: 0.00000744 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74200/100000] Training Loss: 0.00000239 Testing Loss: 0.00000697 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74250/100000] Training Loss: 0.00000250 Testing Loss: 0.00000709 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74300/100000] Training Loss: 0.00000255 Testing Loss: 0.00000713 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74350/100000] Training Loss: 0.00000255 Testing Loss: 0.00000712 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74400/100000] Training Loss: 0.00000276 Testing Loss: 0.00000728 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74450/100000] Training Loss: 0.00000241 Testing Loss: 0.00000695 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74500/100000] Training Loss: 0.00000260 Testing Loss: 0.00000718 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74550/100000] Training Loss: 0.00000236 Testing Loss: 0.00000692 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74600/100000] Training Loss: 0.00000244 Testing Loss: 0.00000698 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74650/100000] Training Loss: 0.00000283 Testing Loss: 0.00000751 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74700/100000] Training Loss: 0.00000243 Testing Loss: 0.00000698 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74750/100000] Training Loss: 0.00000294 Testing Loss: 0.00000766 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74800/100000] Training Loss: 0.00000236 Testing Loss: 0.00000690 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74850/100000] Training Loss: 0.00000356 Testing Loss: 0.00000831 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74900/100000] Training Loss: 0.00000239 Testing Loss: 0.00000694 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [74950/100000] Training Loss: 0.00000251 Testing Loss: 0.00000697 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75000/100000] Training Loss: 0.00000265 Testing Loss: 0.00000714 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75050/100000] Training Loss: 0.00000574 Testing Loss: 0.00001101 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75100/100000] Training Loss: 0.00000235 Testing Loss: 0.00000687 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75150/100000] Training Loss: 0.00000241 Testing Loss: 0.00000694 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75200/100000] Training Loss: 0.00000289 Testing Loss: 0.00000745 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75250/100000] Training Loss: 0.00000290 Testing Loss: 0.00000751 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75300/100000] Training Loss: 0.00000241 Testing Loss: 0.00000694 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75350/100000] Training Loss: 0.00000245 Testing Loss: 0.00000695 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75400/100000] Training Loss: 0.00000248 Testing Loss: 0.00000701 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75450/100000] Training Loss: 0.00000347 Testing Loss: 0.00000847 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75500/100000] Training Loss: 0.00000231 Testing Loss: 0.00000683 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75550/100000] Training Loss: 0.00000244 Testing Loss: 0.00000693 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75600/100000] Training Loss: 0.00000334 Testing Loss: 0.00000806 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75650/100000] Training Loss: 0.00000239 Testing Loss: 0.00000691 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75700/100000] Training Loss: 0.00000322 Testing Loss: 0.00000750 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75750/100000] Training Loss: 0.00000251 Testing Loss: 0.00000702 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75800/100000] Training Loss: 0.00000238 Testing Loss: 0.00000690 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75850/100000] Training Loss: 0.00000251 Testing Loss: 0.00000702 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75900/100000] Training Loss: 0.00000238 Testing Loss: 0.00000686 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [75950/100000] Training Loss: 0.00000245 Testing Loss: 0.00000693 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76000/100000] Training Loss: 0.00000236 Testing Loss: 0.00000684 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76050/100000] Training Loss: 0.00000270 Testing Loss: 0.00000727 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76100/100000] Training Loss: 0.00000231 Testing Loss: 0.00000680 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76150/100000] Training Loss: 0.00000242 Testing Loss: 0.00000690 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76200/100000] Training Loss: 0.00000248 Testing Loss: 0.00000692 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76250/100000] Training Loss: 0.00000269 Testing Loss: 0.00000722 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76300/100000] Training Loss: 0.00000240 Testing Loss: 0.00000686 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76350/100000] Training Loss: 0.00000243 Testing Loss: 0.00000691 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76400/100000] Training Loss: 0.00000248 Testing Loss: 0.00000685 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76450/100000] Training Loss: 0.00000248 Testing Loss: 0.00000697 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76500/100000] Training Loss: 0.00000252 Testing Loss: 0.00000700 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76550/100000] Training Loss: 0.00000390 Testing Loss: 0.00000737 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76600/100000] Training Loss: 0.00000229 Testing Loss: 0.00000675 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76650/100000] Training Loss: 0.00000241 Testing Loss: 0.00000686 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76700/100000] Training Loss: 0.00000248 Testing Loss: 0.00000697 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76750/100000] Training Loss: 0.00000235 Testing Loss: 0.00000681 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76800/100000] Training Loss: 0.00000242 Testing Loss: 0.00000690 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76850/100000] Training Loss: 0.00000233 Testing Loss: 0.00000678 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76900/100000] Training Loss: 0.00000247 Testing Loss: 0.00000697 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [76950/100000] Training Loss: 0.00000240 Testing Loss: 0.00000679 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77000/100000] Training Loss: 0.00000245 Testing Loss: 0.00000688 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77050/100000] Training Loss: 0.00000253 Testing Loss: 0.00000708 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77100/100000] Training Loss: 0.00000229 Testing Loss: 0.00000674 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77150/100000] Training Loss: 0.00000322 Testing Loss: 0.00000805 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77200/100000] Training Loss: 0.00000228 Testing Loss: 0.00000672 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77250/100000] Training Loss: 0.00000237 Testing Loss: 0.00000682 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77300/100000] Training Loss: 0.00000313 Testing Loss: 0.00000756 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77350/100000] Training Loss: 0.00000266 Testing Loss: 0.00000715 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77400/100000] Training Loss: 0.00000232 Testing Loss: 0.00000676 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77450/100000] Training Loss: 0.00000235 Testing Loss: 0.00000679 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77500/100000] Training Loss: 0.00000234 Testing Loss: 0.00000678 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77550/100000] Training Loss: 0.00000245 Testing Loss: 0.00000684 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77600/100000] Training Loss: 0.00000238 Testing Loss: 0.00000684 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77650/100000] Training Loss: 0.00000242 Testing Loss: 0.00000684 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77700/100000] Training Loss: 0.00000321 Testing Loss: 0.00000715 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77750/100000] Training Loss: 0.00000228 Testing Loss: 0.00000670 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77800/100000] Training Loss: 0.00000252 Testing Loss: 0.00000696 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77850/100000] Training Loss: 0.00000373 Testing Loss: 0.00000814 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77900/100000] Training Loss: 0.00000241 Testing Loss: 0.00000681 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [77950/100000] Training Loss: 0.00000234 Testing Loss: 0.00000674 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78000/100000] Training Loss: 0.00000297 Testing Loss: 0.00000765 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78050/100000] Training Loss: 0.00000228 Testing Loss: 0.00000668 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78100/100000] Training Loss: 0.00000240 Testing Loss: 0.00000681 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78150/100000] Training Loss: 0.00000244 Testing Loss: 0.00000686 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78200/100000] Training Loss: 0.00000258 Testing Loss: 0.00000688 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78250/100000] Training Loss: 0.00000233 Testing Loss: 0.00000673 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78300/100000] Training Loss: 0.00000238 Testing Loss: 0.00000677 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78350/100000] Training Loss: 0.00000250 Testing Loss: 0.00000682 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78400/100000] Training Loss: 0.00000258 Testing Loss: 0.00000697 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78450/100000] Training Loss: 0.00000482 Testing Loss: 0.00000892 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78500/100000] Training Loss: 0.00000229 Testing Loss: 0.00000669 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78550/100000] Training Loss: 0.00000240 Testing Loss: 0.00000680 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78600/100000] Training Loss: 0.00000228 Testing Loss: 0.00000667 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78650/100000] Training Loss: 0.00000244 Testing Loss: 0.00000686 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78700/100000] Training Loss: 0.00000227 Testing Loss: 0.00000664 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78750/100000] Training Loss: 0.00000259 Testing Loss: 0.00000699 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78800/100000] Training Loss: 0.00000236 Testing Loss: 0.00000670 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78850/100000] Training Loss: 0.00000252 Testing Loss: 0.00000688 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78900/100000] Training Loss: 0.00000231 Testing Loss: 0.00000669 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [78950/100000] Training Loss: 0.00000239 Testing Loss: 0.00000676 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79000/100000] Training Loss: 0.00000277 Testing Loss: 0.00000704 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79050/100000] Training Loss: 0.00000222 Testing Loss: 0.00000659 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79100/100000] Training Loss: 0.00000231 Testing Loss: 0.00000667 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79150/100000] Training Loss: 0.00000232 Testing Loss: 0.00000668 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79200/100000] Training Loss: 0.00000231 Testing Loss: 0.00000670 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79250/100000] Training Loss: 0.00000288 Testing Loss: 0.00000724 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79300/100000] Training Loss: 0.00000380 Testing Loss: 0.00000847 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79350/100000] Training Loss: 0.00000222 Testing Loss: 0.00000658 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79400/100000] Training Loss: 0.00000433 Testing Loss: 0.00000860 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79450/100000] Training Loss: 0.00000224 Testing Loss: 0.00000659 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79500/100000] Training Loss: 0.00000277 Testing Loss: 0.00000712 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79550/100000] Training Loss: 0.00000230 Testing Loss: 0.00000667 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79600/100000] Training Loss: 0.00000262 Testing Loss: 0.00000685 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79650/100000] Training Loss: 0.00000241 Testing Loss: 0.00000678 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79700/100000] Training Loss: 0.00000227 Testing Loss: 0.00000662 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79750/100000] Training Loss: 0.00000319 Testing Loss: 0.00000786 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79800/100000] Training Loss: 0.00000225 Testing Loss: 0.00000659 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79850/100000] Training Loss: 0.00000243 Testing Loss: 0.00000678 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79900/100000] Training Loss: 0.00000224 Testing Loss: 0.00000657 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [79950/100000] Training Loss: 0.00000279 Testing Loss: 0.00000716 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80000/100000] Training Loss: 0.00000223 Testing Loss: 0.00000656 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80050/100000] Training Loss: 0.00000268 Testing Loss: 0.00000705 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80100/100000] Training Loss: 0.00000255 Testing Loss: 0.00000690 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80150/100000] Training Loss: 0.00000531 Testing Loss: 0.00000916 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80200/100000] Training Loss: 0.00000220 Testing Loss: 0.00000653 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80250/100000] Training Loss: 0.00000228 Testing Loss: 0.00000661 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80300/100000] Training Loss: 0.00000242 Testing Loss: 0.00000673 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80350/100000] Training Loss: 0.00000294 Testing Loss: 0.00000743 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80400/100000] Training Loss: 0.00000231 Testing Loss: 0.00000664 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80450/100000] Training Loss: 0.00000266 Testing Loss: 0.00000712 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80500/100000] Training Loss: 0.00000223 Testing Loss: 0.00000654 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80550/100000] Training Loss: 0.00000238 Testing Loss: 0.00000668 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80600/100000] Training Loss: 0.00000254 Testing Loss: 0.00000693 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80650/100000] Training Loss: 0.00000222 Testing Loss: 0.00000652 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80700/100000] Training Loss: 0.00000230 Testing Loss: 0.00000660 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80750/100000] Training Loss: 0.00000224 Testing Loss: 0.00000654 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80800/100000] Training Loss: 0.00000234 Testing Loss: 0.00000666 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80850/100000] Training Loss: 0.00000226 Testing Loss: 0.00000656 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80900/100000] Training Loss: 0.00000236 Testing Loss: 0.00000668 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [80950/100000] Training Loss: 0.00000224 Testing Loss: 0.00000651 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81000/100000] Training Loss: 0.00000248 Testing Loss: 0.00000680 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81050/100000] Training Loss: 0.00000237 Testing Loss: 0.00000669 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81100/100000] Training Loss: 0.00000222 Testing Loss: 0.00000651 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81150/100000] Training Loss: 0.00000236 Testing Loss: 0.00000667 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81200/100000] Training Loss: 0.00000267 Testing Loss: 0.00000673 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81250/100000] Training Loss: 0.00000233 Testing Loss: 0.00000661 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81300/100000] Training Loss: 0.00000249 Testing Loss: 0.00000680 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81350/100000] Training Loss: 0.00000239 Testing Loss: 0.00000668 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81400/100000] Training Loss: 0.00000227 Testing Loss: 0.00000655 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81450/100000] Training Loss: 0.00000321 Testing Loss: 0.00000760 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81500/100000] Training Loss: 0.00000235 Testing Loss: 0.00000664 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81550/100000] Training Loss: 0.00000227 Testing Loss: 0.00000652 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81600/100000] Training Loss: 0.00000233 Testing Loss: 0.00000663 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81650/100000] Training Loss: 0.00000221 Testing Loss: 0.00000649 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81700/100000] Training Loss: 0.00000288 Testing Loss: 0.00000654 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81750/100000] Training Loss: 0.00000216 Testing Loss: 0.00000643 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81800/100000] Training Loss: 0.00000228 Testing Loss: 0.00000653 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81850/100000] Training Loss: 0.00000230 Testing Loss: 0.00000657 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81900/100000] Training Loss: 0.00000246 Testing Loss: 0.00000675 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [81950/100000] Training Loss: 0.00000246 Testing Loss: 0.00000672 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82000/100000] Training Loss: 0.00000232 Testing Loss: 0.00000655 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82050/100000] Training Loss: 0.00000235 Testing Loss: 0.00000664 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82100/100000] Training Loss: 0.00000219 Testing Loss: 0.00000644 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82150/100000] Training Loss: 0.00000234 Testing Loss: 0.00000655 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82200/100000] Training Loss: 0.00000225 Testing Loss: 0.00000646 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82250/100000] Training Loss: 0.00000230 Testing Loss: 0.00000655 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82300/100000] Training Loss: 0.00000223 Testing Loss: 0.00000647 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82350/100000] Training Loss: 0.00000278 Testing Loss: 0.00000714 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82400/100000] Training Loss: 0.00000219 Testing Loss: 0.00000644 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82450/100000] Training Loss: 0.00000235 Testing Loss: 0.00000654 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82500/100000] Training Loss: 0.00000247 Testing Loss: 0.00000676 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82550/100000] Training Loss: 0.00000227 Testing Loss: 0.00000652 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82600/100000] Training Loss: 0.00000229 Testing Loss: 0.00000650 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82650/100000] Training Loss: 0.00000222 Testing Loss: 0.00000647 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82700/100000] Training Loss: 0.00000349 Testing Loss: 0.00000816 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82750/100000] Training Loss: 0.00000215 Testing Loss: 0.00000638 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82800/100000] Training Loss: 0.00000267 Testing Loss: 0.00000692 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82850/100000] Training Loss: 0.00000218 Testing Loss: 0.00000642 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82900/100000] Training Loss: 0.00000245 Testing Loss: 0.00000669 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [82950/100000] Training Loss: 0.00000215 Testing Loss: 0.00000639 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83000/100000] Training Loss: 0.00000222 Testing Loss: 0.00000643 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83050/100000] Training Loss: 0.00000254 Testing Loss: 0.00000678 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83100/100000] Training Loss: 0.00000255 Testing Loss: 0.00000671 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83150/100000] Training Loss: 0.00000257 Testing Loss: 0.00000670 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83200/100000] Training Loss: 0.00000267 Testing Loss: 0.00000700 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83250/100000] Training Loss: 0.00000218 Testing Loss: 0.00000640 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83300/100000] Training Loss: 0.00000243 Testing Loss: 0.00000667 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83350/100000] Training Loss: 0.00000238 Testing Loss: 0.00000662 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83400/100000] Training Loss: 0.00000222 Testing Loss: 0.00000641 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83450/100000] Training Loss: 0.00000232 Testing Loss: 0.00000655 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83500/100000] Training Loss: 0.00000220 Testing Loss: 0.00000639 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83550/100000] Training Loss: 0.00000226 Testing Loss: 0.00000647 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83600/100000] Training Loss: 0.00000215 Testing Loss: 0.00000636 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83650/100000] Training Loss: 0.00000240 Testing Loss: 0.00000657 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83700/100000] Training Loss: 0.00000324 Testing Loss: 0.00000715 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83750/100000] Training Loss: 0.00000216 Testing Loss: 0.00000637 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83800/100000] Training Loss: 0.00000413 Testing Loss: 0.00000758 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83850/100000] Training Loss: 0.00000216 Testing Loss: 0.00000636 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83900/100000] Training Loss: 0.00000225 Testing Loss: 0.00000646 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [83950/100000] Training Loss: 0.00000218 Testing Loss: 0.00000637 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84000/100000] Training Loss: 0.00000229 Testing Loss: 0.00000652 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84050/100000] Training Loss: 0.00000216 Testing Loss: 0.00000633 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84100/100000] Training Loss: 0.00000220 Testing Loss: 0.00000639 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84150/100000] Training Loss: 0.00000242 Testing Loss: 0.00000648 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84200/100000] Training Loss: 0.00000225 Testing Loss: 0.00000646 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84250/100000] Training Loss: 0.00000275 Testing Loss: 0.00000658 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84300/100000] Training Loss: 0.00000212 Testing Loss: 0.00000631 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84350/100000] Training Loss: 0.00000248 Testing Loss: 0.00000663 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84400/100000] Training Loss: 0.00000212 Testing Loss: 0.00000630 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84450/100000] Training Loss: 0.00000226 Testing Loss: 0.00000639 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84500/100000] Training Loss: 0.00000246 Testing Loss: 0.00000673 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84550/100000] Training Loss: 0.00000211 Testing Loss: 0.00000629 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84600/100000] Training Loss: 0.00000219 Testing Loss: 0.00000637 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84650/100000] Training Loss: 0.00000217 Testing Loss: 0.00000633 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84700/100000] Training Loss: 0.00000219 Testing Loss: 0.00000637 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84750/100000] Training Loss: 0.00000222 Testing Loss: 0.00000637 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84800/100000] Training Loss: 0.00000234 Testing Loss: 0.00000646 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84850/100000] Training Loss: 0.00000227 Testing Loss: 0.00000646 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84900/100000] Training Loss: 0.00000243 Testing Loss: 0.00000650 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [84950/100000] Training Loss: 0.00000234 Testing Loss: 0.00000653 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85000/100000] Training Loss: 0.00000218 Testing Loss: 0.00000634 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85050/100000] Training Loss: 0.00000243 Testing Loss: 0.00000662 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85100/100000] Training Loss: 0.00000236 Testing Loss: 0.00000657 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85150/100000] Training Loss: 0.00000222 Testing Loss: 0.00000634 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85200/100000] Training Loss: 0.00000231 Testing Loss: 0.00000647 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85250/100000] Training Loss: 0.00000214 Testing Loss: 0.00000629 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85300/100000] Training Loss: 0.00000250 Testing Loss: 0.00000666 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85350/100000] Training Loss: 0.00000211 Testing Loss: 0.00000626 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85400/100000] Training Loss: 0.00000222 Testing Loss: 0.00000639 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85450/100000] Training Loss: 0.00000305 Testing Loss: 0.00000672 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85500/100000] Training Loss: 0.00000208 Testing Loss: 0.00000622 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85550/100000] Training Loss: 0.00000221 Testing Loss: 0.00000634 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85600/100000] Training Loss: 0.00000262 Testing Loss: 0.00000650 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85650/100000] Training Loss: 0.00000227 Testing Loss: 0.00000644 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85700/100000] Training Loss: 0.00000241 Testing Loss: 0.00000658 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85750/100000] Training Loss: 0.00000210 Testing Loss: 0.00000622 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85800/100000] Training Loss: 0.00000277 Testing Loss: 0.00000700 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85850/100000] Training Loss: 0.00000209 Testing Loss: 0.00000622 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85900/100000] Training Loss: 0.00000236 Testing Loss: 0.00000649 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [85950/100000] Training Loss: 0.00000224 Testing Loss: 0.00000637 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86000/100000] Training Loss: 0.00000406 Testing Loss: 0.00000763 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86050/100000] Training Loss: 0.00000207 Testing Loss: 0.00000619 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86100/100000] Training Loss: 0.00000216 Testing Loss: 0.00000628 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86150/100000] Training Loss: 0.00000440 Testing Loss: 0.00001006 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86200/100000] Training Loss: 0.00000209 Testing Loss: 0.00000618 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86250/100000] Training Loss: 0.00000223 Testing Loss: 0.00000635 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86300/100000] Training Loss: 0.00000228 Testing Loss: 0.00000637 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86350/100000] Training Loss: 0.00000220 Testing Loss: 0.00000630 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86400/100000] Training Loss: 0.00000237 Testing Loss: 0.00000647 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86450/100000] Training Loss: 0.00000224 Testing Loss: 0.00000632 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86500/100000] Training Loss: 0.00000243 Testing Loss: 0.00000652 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86550/100000] Training Loss: 0.00000222 Testing Loss: 0.00000628 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86600/100000] Training Loss: 0.00000224 Testing Loss: 0.00000635 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86650/100000] Training Loss: 0.00000226 Testing Loss: 0.00000673 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86700/100000] Training Loss: 0.00000236 Testing Loss: 0.00000651 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86750/100000] Training Loss: 0.00000218 Testing Loss: 0.00000628 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86800/100000] Training Loss: 0.00000286 Testing Loss: 0.00000707 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86850/100000] Training Loss: 0.00000217 Testing Loss: 0.00000627 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86900/100000] Training Loss: 0.00000217 Testing Loss: 0.00000625 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [86950/100000] Training Loss: 0.00000299 Testing Loss: 0.00000725 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87000/100000] Training Loss: 0.00000213 Testing Loss: 0.00000624 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87050/100000] Training Loss: 0.00000230 Testing Loss: 0.00000628 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87100/100000] Training Loss: 0.00000239 Testing Loss: 0.00000642 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87150/100000] Training Loss: 0.00000409 Testing Loss: 0.00000819 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87200/100000] Training Loss: 0.00000224 Testing Loss: 0.00000636 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87250/100000] Training Loss: 0.00000308 Testing Loss: 0.00000704 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87300/100000] Training Loss: 0.00000215 Testing Loss: 0.00000624 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87350/100000] Training Loss: 0.00000209 Testing Loss: 0.00000617 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87400/100000] Training Loss: 0.00000357 Testing Loss: 0.00000730 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87450/100000] Training Loss: 0.00000234 Testing Loss: 0.00000641 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87500/100000] Training Loss: 0.00000234 Testing Loss: 0.00000633 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87550/100000] Training Loss: 0.00000254 Testing Loss: 0.00000672 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87600/100000] Training Loss: 0.00000209 Testing Loss: 0.00000616 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87650/100000] Training Loss: 0.00000215 Testing Loss: 0.00000623 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87700/100000] Training Loss: 0.00000215 Testing Loss: 0.00000629 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87750/100000] Training Loss: 0.00000224 Testing Loss: 0.00000636 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87800/100000] Training Loss: 0.00000212 Testing Loss: 0.00000618 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87850/100000] Training Loss: 0.00000217 Testing Loss: 0.00000624 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87900/100000] Training Loss: 0.00000209 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [87950/100000] Training Loss: 0.00000504 Testing Loss: 0.00000865 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88000/100000] Training Loss: 0.00000204 Testing Loss: 0.00000609 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88050/100000] Training Loss: 0.00000245 Testing Loss: 0.00000657 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88100/100000] Training Loss: 0.00000206 Testing Loss: 0.00000612 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88150/100000] Training Loss: 0.00000244 Testing Loss: 0.00000650 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88200/100000] Training Loss: 0.00000224 Testing Loss: 0.00000628 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88250/100000] Training Loss: 0.00000225 Testing Loss: 0.00000639 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88300/100000] Training Loss: 0.00000212 Testing Loss: 0.00000617 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88350/100000] Training Loss: 0.00000345 Testing Loss: 0.00000781 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88400/100000] Training Loss: 0.00000203 Testing Loss: 0.00000608 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88450/100000] Training Loss: 0.00000246 Testing Loss: 0.00000660 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88500/100000] Training Loss: 0.00000205 Testing Loss: 0.00000610 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88550/100000] Training Loss: 0.00000211 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88600/100000] Training Loss: 0.00000232 Testing Loss: 0.00000637 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88650/100000] Training Loss: 0.00000211 Testing Loss: 0.00000616 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88700/100000] Training Loss: 0.00000272 Testing Loss: 0.00000647 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88750/100000] Training Loss: 0.00000202 Testing Loss: 0.00000605 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88800/100000] Training Loss: 0.00000242 Testing Loss: 0.00000663 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88850/100000] Training Loss: 0.00000207 Testing Loss: 0.00000609 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88900/100000] Training Loss: 0.00000215 Testing Loss: 0.00000620 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [88950/100000] Training Loss: 0.00000216 Testing Loss: 0.00000620 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89000/100000] Training Loss: 0.00000206 Testing Loss: 0.00000609 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89050/100000] Training Loss: 0.00000223 Testing Loss: 0.00000632 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89100/100000] Training Loss: 0.00000209 Testing Loss: 0.00000611 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89150/100000] Training Loss: 0.00000228 Testing Loss: 0.00000636 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89200/100000] Training Loss: 0.00000205 Testing Loss: 0.00000607 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89250/100000] Training Loss: 0.00000223 Testing Loss: 0.00000626 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89300/100000] Training Loss: 0.00000229 Testing Loss: 0.00000627 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89350/100000] Training Loss: 0.00000219 Testing Loss: 0.00000607 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89400/100000] Training Loss: 0.00000200 Testing Loss: 0.00000602 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89450/100000] Training Loss: 0.00000209 Testing Loss: 0.00000611 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89500/100000] Training Loss: 0.00000214 Testing Loss: 0.00000615 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89550/100000] Training Loss: 0.00000257 Testing Loss: 0.00000652 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89600/100000] Training Loss: 0.00000308 Testing Loss: 0.00000720 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89650/100000] Training Loss: 0.00000211 Testing Loss: 0.00000612 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89700/100000] Training Loss: 0.00000238 Testing Loss: 0.00000633 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89750/100000] Training Loss: 0.00000249 Testing Loss: 0.00000661 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89800/100000] Training Loss: 0.00000211 Testing Loss: 0.00000616 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89850/100000] Training Loss: 0.00000203 Testing Loss: 0.00000603 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89900/100000] Training Loss: 0.00000209 Testing Loss: 0.00000609 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [89950/100000] Training Loss: 0.00000362 Testing Loss: 0.00000828 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90000/100000] Training Loss: 0.00000200 Testing Loss: 0.00000600 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90050/100000] Training Loss: 0.00000216 Testing Loss: 0.00000618 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90100/100000] Training Loss: 0.00000259 Testing Loss: 0.00000677 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90150/100000] Training Loss: 0.00000208 Testing Loss: 0.00000607 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90200/100000] Training Loss: 0.00000270 Testing Loss: 0.00000683 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90250/100000] Training Loss: 0.00000212 Testing Loss: 0.00000612 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90300/100000] Training Loss: 0.00000236 Testing Loss: 0.00000631 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90350/100000] Training Loss: 0.00000230 Testing Loss: 0.00000632 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90400/100000] Training Loss: 0.00000219 Testing Loss: 0.00000620 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90450/100000] Training Loss: 0.00000209 Testing Loss: 0.00000607 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90500/100000] Training Loss: 0.00000318 Testing Loss: 0.00000678 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90550/100000] Training Loss: 0.00000230 Testing Loss: 0.00000625 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90600/100000] Training Loss: 0.00000449 Testing Loss: 0.00000827 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90650/100000] Training Loss: 0.00000213 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90700/100000] Training Loss: 0.00000214 Testing Loss: 0.00000611 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90750/100000] Training Loss: 0.00000213 Testing Loss: 0.00000607 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90800/100000] Training Loss: 0.00000270 Testing Loss: 0.00000657 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90850/100000] Training Loss: 0.00000312 Testing Loss: 0.00000696 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90900/100000] Training Loss: 0.00000223 Testing Loss: 0.00000618 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [90950/100000] Training Loss: 0.00000213 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91000/100000] Training Loss: 0.00000249 Testing Loss: 0.00000646 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91050/100000] Training Loss: 0.00000253 Testing Loss: 0.00000644 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91100/100000] Training Loss: 0.00000237 Testing Loss: 0.00000626 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91150/100000] Training Loss: 0.00000465 Testing Loss: 0.00000924 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91200/100000] Training Loss: 0.00000198 Testing Loss: 0.00000594 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91250/100000] Training Loss: 0.00000222 Testing Loss: 0.00000616 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91300/100000] Training Loss: 0.00000372 Testing Loss: 0.00000709 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91350/100000] Training Loss: 0.00000198 Testing Loss: 0.00000593 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91400/100000] Training Loss: 0.00000223 Testing Loss: 0.00000624 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91450/100000] Training Loss: 0.00000199 Testing Loss: 0.00000594 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91500/100000] Training Loss: 0.00000314 Testing Loss: 0.00000730 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91550/100000] Training Loss: 0.00000204 Testing Loss: 0.00000601 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91600/100000] Training Loss: 0.00000229 Testing Loss: 0.00000628 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91650/100000] Training Loss: 0.00000211 Testing Loss: 0.00000603 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91700/100000] Training Loss: 0.00000231 Testing Loss: 0.00000618 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91750/100000] Training Loss: 0.00000217 Testing Loss: 0.00000619 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91800/100000] Training Loss: 0.00000206 Testing Loss: 0.00000601 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91850/100000] Training Loss: 0.00000204 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91900/100000] Training Loss: 0.00000233 Testing Loss: 0.00000623 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [91950/100000] Training Loss: 0.00000362 Testing Loss: 0.00000757 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92000/100000] Training Loss: 0.00000197 Testing Loss: 0.00000590 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92050/100000] Training Loss: 0.00000243 Testing Loss: 0.00000653 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92100/100000] Training Loss: 0.00000229 Testing Loss: 0.00000618 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92150/100000] Training Loss: 0.00000220 Testing Loss: 0.00000616 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92200/100000] Training Loss: 0.00000202 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92250/100000] Training Loss: 0.00000251 Testing Loss: 0.00000641 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92300/100000] Training Loss: 0.00000226 Testing Loss: 0.00000624 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92350/100000] Training Loss: 0.00000200 Testing Loss: 0.00000593 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92400/100000] Training Loss: 0.00000209 Testing Loss: 0.00000600 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92450/100000] Training Loss: 0.00000226 Testing Loss: 0.00000625 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92500/100000] Training Loss: 0.00000228 Testing Loss: 0.00000611 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92550/100000] Training Loss: 0.00000219 Testing Loss: 0.00000608 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92600/100000] Training Loss: 0.00000200 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92650/100000] Training Loss: 0.00000207 Testing Loss: 0.00000599 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92700/100000] Training Loss: 0.00000206 Testing Loss: 0.00000596 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92750/100000] Training Loss: 0.00000211 Testing Loss: 0.00000602 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92800/100000] Training Loss: 0.00000242 Testing Loss: 0.00000644 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92850/100000] Training Loss: 0.00000194 Testing Loss: 0.00000585 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92900/100000] Training Loss: 0.00000204 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [92950/100000] Training Loss: 0.00000207 Testing Loss: 0.00000600 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93000/100000] Training Loss: 0.00000199 Testing Loss: 0.00000590 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93050/100000] Training Loss: 0.00000306 Testing Loss: 0.00000725 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93100/100000] Training Loss: 0.00000195 Testing Loss: 0.00000586 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93150/100000] Training Loss: 0.00000215 Testing Loss: 0.00000605 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93200/100000] Training Loss: 0.00000196 Testing Loss: 0.00000588 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93250/100000] Training Loss: 0.00000275 Testing Loss: 0.00000649 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93300/100000] Training Loss: 0.00000276 Testing Loss: 0.00000666 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93350/100000] Training Loss: 0.00000213 Testing Loss: 0.00000607 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93400/100000] Training Loss: 0.00000199 Testing Loss: 0.00000590 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93450/100000] Training Loss: 0.00000222 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93500/100000] Training Loss: 0.00000201 Testing Loss: 0.00000594 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93550/100000] Training Loss: 0.00000220 Testing Loss: 0.00000604 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93600/100000] Training Loss: 0.00000378 Testing Loss: 0.00000828 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93650/100000] Training Loss: 0.00000203 Testing Loss: 0.00000592 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93700/100000] Training Loss: 0.00000215 Testing Loss: 0.00000603 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93750/100000] Training Loss: 0.00000447 Testing Loss: 0.00000765 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93800/100000] Training Loss: 0.00000193 Testing Loss: 0.00000582 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93850/100000] Training Loss: 0.00000229 Testing Loss: 0.00000624 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93900/100000] Training Loss: 0.00000199 Testing Loss: 0.00000588 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [93950/100000] Training Loss: 0.00000219 Testing Loss: 0.00000619 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94000/100000] Training Loss: 0.00000196 Testing Loss: 0.00000585 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94050/100000] Training Loss: 0.00000238 Testing Loss: 0.00000632 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94100/100000] Training Loss: 0.00000220 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94150/100000] Training Loss: 0.00000208 Testing Loss: 0.00000592 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94200/100000] Training Loss: 0.00000232 Testing Loss: 0.00000624 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94250/100000] Training Loss: 0.00000195 Testing Loss: 0.00000583 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94300/100000] Training Loss: 0.00000201 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94350/100000] Training Loss: 0.00000204 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94400/100000] Training Loss: 0.00000245 Testing Loss: 0.00000635 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94450/100000] Training Loss: 0.00000212 Testing Loss: 0.00000601 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94500/100000] Training Loss: 0.00000207 Testing Loss: 0.00000597 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94550/100000] Training Loss: 0.00000231 Testing Loss: 0.00000615 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94600/100000] Training Loss: 0.00000193 Testing Loss: 0.00000579 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94650/100000] Training Loss: 0.00000199 Testing Loss: 0.00000584 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94700/100000] Training Loss: 0.00000256 Testing Loss: 0.00000642 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94750/100000] Training Loss: 0.00000199 Testing Loss: 0.00000583 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94800/100000] Training Loss: 0.00000218 Testing Loss: 0.00000605 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94850/100000] Training Loss: 0.00000195 Testing Loss: 0.00000581 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94900/100000] Training Loss: 0.00000452 Testing Loss: 0.00000796 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [94950/100000] Training Loss: 0.00000192 Testing Loss: 0.00000577 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95000/100000] Training Loss: 0.00000208 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95050/100000] Training Loss: 0.00000202 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95100/100000] Training Loss: 0.00000216 Testing Loss: 0.00000606 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95150/100000] Training Loss: 0.00000220 Testing Loss: 0.00000601 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95200/100000] Training Loss: 0.00000193 Testing Loss: 0.00000579 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95250/100000] Training Loss: 0.00000212 Testing Loss: 0.00000597 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95300/100000] Training Loss: 0.00000199 Testing Loss: 0.00000585 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95350/100000] Training Loss: 0.00000320 Testing Loss: 0.00000729 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95400/100000] Training Loss: 0.00000191 Testing Loss: 0.00000576 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95450/100000] Training Loss: 0.00000214 Testing Loss: 0.00000602 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95500/100000] Training Loss: 0.00000192 Testing Loss: 0.00000577 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95550/100000] Training Loss: 0.00000212 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95600/100000] Training Loss: 0.00000224 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95650/100000] Training Loss: 0.00000220 Testing Loss: 0.00000610 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95700/100000] Training Loss: 0.00000213 Testing Loss: 0.00000588 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95750/100000] Training Loss: 0.00000220 Testing Loss: 0.00000607 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95800/100000] Training Loss: 0.00000194 Testing Loss: 0.00000577 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95850/100000] Training Loss: 0.00000211 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95900/100000] Training Loss: 0.00000205 Testing Loss: 0.00000588 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [95950/100000] Training Loss: 0.00000195 Testing Loss: 0.00000577 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96000/100000] Training Loss: 0.00000201 Testing Loss: 0.00000583 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96050/100000] Training Loss: 0.00000203 Testing Loss: 0.00000578 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96100/100000] Training Loss: 0.00000233 Testing Loss: 0.00000618 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96150/100000] Training Loss: 0.00000207 Testing Loss: 0.00000588 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96200/100000] Training Loss: 0.00000208 Testing Loss: 0.00000586 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96250/100000] Training Loss: 0.00000201 Testing Loss: 0.00000587 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96300/100000] Training Loss: 0.00000232 Testing Loss: 0.00000633 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96350/100000] Training Loss: 0.00000222 Testing Loss: 0.00000601 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96400/100000] Training Loss: 0.00000371 Testing Loss: 0.00000800 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96450/100000] Training Loss: 0.00000190 Testing Loss: 0.00000572 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96500/100000] Training Loss: 0.00000204 Testing Loss: 0.00000586 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96550/100000] Training Loss: 0.00000192 Testing Loss: 0.00000574 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96600/100000] Training Loss: 0.00000203 Testing Loss: 0.00000584 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96650/100000] Training Loss: 0.00000196 Testing Loss: 0.00000576 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96700/100000] Training Loss: 0.00000260 Testing Loss: 0.00000640 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96750/100000] Training Loss: 0.00000210 Testing Loss: 0.00000575 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96800/100000] Training Loss: 0.00000188 Testing Loss: 0.00000569 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96850/100000] Training Loss: 0.00000195 Testing Loss: 0.00000575 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96900/100000] Training Loss: 0.00000216 Testing Loss: 0.00000600 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [96950/100000] Training Loss: 0.00000201 Testing Loss: 0.00000582 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97000/100000] Training Loss: 0.00000195 Testing Loss: 0.00000575 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97050/100000] Training Loss: 0.00000195 Testing Loss: 0.00000579 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97100/100000] Training Loss: 0.00000209 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97150/100000] Training Loss: 0.00000515 Testing Loss: 0.00000833 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97200/100000] Training Loss: 0.00000188 Testing Loss: 0.00000567 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97250/100000] Training Loss: 0.00000206 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97300/100000] Training Loss: 0.00000202 Testing Loss: 0.00000579 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97350/100000] Training Loss: 0.00000316 Testing Loss: 0.00000692 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97400/100000] Training Loss: 0.00000212 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97450/100000] Training Loss: 0.00000301 Testing Loss: 0.00000678 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97500/100000] Training Loss: 0.00000238 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97550/100000] Training Loss: 0.00000201 Testing Loss: 0.00000580 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97600/100000] Training Loss: 0.00000225 Testing Loss: 0.00000596 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97650/100000] Training Loss: 0.00000206 Testing Loss: 0.00000586 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97700/100000] Training Loss: 0.00000195 Testing Loss: 0.00000574 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97750/100000] Training Loss: 0.00000208 Testing Loss: 0.00000587 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97800/100000] Training Loss: 0.00000342 Testing Loss: 0.00000613 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97850/100000] Training Loss: 0.00000187 Testing Loss: 0.00000566 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97900/100000] Training Loss: 0.00000201 Testing Loss: 0.00000576 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [97950/100000] Training Loss: 0.00000212 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98000/100000] Training Loss: 0.00000213 Testing Loss: 0.00000593 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98050/100000] Training Loss: 0.00000235 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98100/100000] Training Loss: 0.00000200 Testing Loss: 0.00000569 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98150/100000] Training Loss: 0.00000226 Testing Loss: 0.00000595 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98200/100000] Training Loss: 0.00000206 Testing Loss: 0.00000582 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98250/100000] Training Loss: 0.00000321 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98300/100000] Training Loss: 0.00000186 Testing Loss: 0.00000563 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98350/100000] Training Loss: 0.00000198 Testing Loss: 0.00000575 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98400/100000] Training Loss: 0.00000201 Testing Loss: 0.00000577 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98450/100000] Training Loss: 0.00000199 Testing Loss: 0.00000574 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98500/100000] Training Loss: 0.00000200 Testing Loss: 0.00000577 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98550/100000] Training Loss: 0.00000189 Testing Loss: 0.00000571 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98600/100000] Training Loss: 0.00000242 Testing Loss: 0.00000604 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98650/100000] Training Loss: 0.00000194 Testing Loss: 0.00000571 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98700/100000] Training Loss: 0.00000251 Testing Loss: 0.00000643 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98750/100000] Training Loss: 0.00000186 Testing Loss: 0.00000561 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98800/100000] Training Loss: 0.00000212 Testing Loss: 0.00000593 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98850/100000] Training Loss: 0.00000188 Testing Loss: 0.00000563 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98900/100000] Training Loss: 0.00000199 Testing Loss: 0.00000576 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [98950/100000] Training Loss: 0.00000189 Testing Loss: 0.00000564 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99000/100000] Training Loss: 0.00000210 Testing Loss: 0.00000587 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99050/100000] Training Loss: 0.00000212 Testing Loss: 0.00000569 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99100/100000] Training Loss: 0.00000250 Testing Loss: 0.00000621 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99150/100000] Training Loss: 0.00000196 Testing Loss: 0.00000572 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99200/100000] Training Loss: 0.00000243 Testing Loss: 0.00000618 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99250/100000] Training Loss: 0.00000191 Testing Loss: 0.00000568 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99300/100000] Training Loss: 0.00000215 Testing Loss: 0.00000614 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99350/100000] Training Loss: 0.00000216 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99400/100000] Training Loss: 0.00000190 Testing Loss: 0.00000565 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99450/100000] Training Loss: 0.00000194 Testing Loss: 0.00000564 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99500/100000] Training Loss: 0.00000216 Testing Loss: 0.00000582 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99550/100000] Training Loss: 0.00000234 Testing Loss: 0.00000621 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99600/100000] Training Loss: 0.00000187 Testing Loss: 0.00000560 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99650/100000] Training Loss: 0.00000208 Testing Loss: 0.00000583 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99700/100000] Training Loss: 0.00000187 Testing Loss: 0.00000560 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99750/100000] Training Loss: 0.00000204 Testing Loss: 0.00000581 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99800/100000] Training Loss: 0.00000188 Testing Loss: 0.00000563 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99850/100000] Training Loss: 0.00000217 Testing Loss: 0.00000589 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99900/100000] Training Loss: 0.00000315 Testing Loss: 0.00000725 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [99950/100000] Training Loss: 0.00000185 Testing Loss: 0.00000557 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Epoch [100000/100000] Training Loss: 0.00000260 Testing Loss: 0.00000629 lr: [7.775999999999999e-05]\n",
      "**************************************************\n",
      "Total training time: 11532.45 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "train_loss_all = []\n",
    "best_train_loss = 1.0\n",
    "num_epochs = 100000\n",
    "\n",
    "patience = 30000  \n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    net.train()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss = compute_loss(net, train_g)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    net.eval()\n",
    "    loss_test = compute_loss(net, test_g)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {loss:.8f} Testing Loss: {loss_test:.8f} lr: {scheduler.get_last_lr()}\")\n",
    "        print(\"*\" * 50)\n",
    "    \n",
    "    # Save the best model\n",
    "    if loss_test.item() < best_train_loss:\n",
    "        best_train_loss = loss_test.item()\n",
    "        torch.save(net.state_dict(), \"net.pth\")\n",
    "        early_stopping_counter = 0  # Reset counter\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    # Early stopping check\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f\"Early stopping triggered! Best validation loss: {best_train_loss:.6f}, achieved at epoch {epoch + 1 - patience}.\")\n",
    "        break\n",
    "\n",
    "T = time.time() - t0\n",
    "print(\"Total training time: {:.2f} seconds\".format(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2884/2391093717.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"net.pth\", map_location = device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net.pth\", map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8984e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2815e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9383e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7646e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6883e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6759e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6809e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6943e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7031e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6982e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6821e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6615e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6411e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6244e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6212e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6379e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6821e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7602e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8795e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0449e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2651e-06, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(taus)):\n",
    "    print(criterion(predict_u_bd(net, taus[i], train_g), train_g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
