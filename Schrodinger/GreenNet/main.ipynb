{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file, we consider the PDE system $\\mathcal{L} \\mathbf{u} = \\mathbf{f}$ with zero Dirichlet boundary condition, where\n",
    "$$\n",
    "\\mathcal{L}=\\left[\\begin{array}{cc}\n",
    "1 & -\\lambda \\Delta \\\\\n",
    "\\lambda \\Delta & 1\n",
    "\\end{array}\\right],\n",
    "\\quad\n",
    "\\mathbf{u}=\\left[\\begin{array}{c} u_1 \\\\ u_2 \\end{array}\\right]\n",
    "\\quad\n",
    "\\mathbf{f}=\\left[\\begin{array}{c} f_1 \\\\ f_2 \\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU memory: 24217.31 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu_info.name}\")\n",
    "    print(f\"GPU memory: {gpu_info.total_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.05\n",
    "N = 41\n",
    "\n",
    "n = N - 1\n",
    "h = 1 / n \n",
    "m = n - 1\n",
    "m2 = m * m\n",
    "id_m = np.identity(m)\n",
    "d1= np.identity(m)\n",
    "d1[0][1] = 1\n",
    "d1[m - 1][m - 2] = 1\n",
    "for i in range(0, m):\n",
    "    d1[i][i] = -2\n",
    "    if (i >= 1) and (i <= m - 2):\n",
    "        d1[i][i - 1] = 1\n",
    "        d1[i][i + 1] = 1\n",
    "D = np.kron(id_m, d1) + np.kron(d1, id_m)\n",
    "D = D / (h ** 2)\n",
    "\n",
    "L = np.zeros((2 * m2, 2 * m2))\n",
    "L[0: m2, 0: m2] = np.identity(m2)\n",
    "L[0: m2, m2: 2 * m2] = - lambda_ * D\n",
    "L[m2: 2 * m2, 0: m2] = lambda_ * D\n",
    "L[m2: 2 * m2, m2: 2 * m2] = np.identity(m2)\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "L_sparse = sparse.csr_matrix(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.random.rand(5000, 2 * m2)\n",
    "u = np.zeros_like(f)\n",
    "for i in range(5000):\n",
    "    u[i, :] = spsolve(L_sparse, f[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 3042]), torch.Size([5000, 3042]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f, u = torch.tensor(f, dtype=torch.float32), torch.tensor(u, dtype=torch.float32)\n",
    "f.shape, u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GreenFun(nn.Module):\n",
    "    def __init__(self, N):     \n",
    "        super(GreenFun, self).__init__()\n",
    "        self.N = N\n",
    "        self.G_layer = nn.Sequential(nn.Linear(self.N, self.N, bias = False))\n",
    "\n",
    "    def forward(self, f):   \n",
    "        return self.G_layer(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "dataset = TensorDataset(f, u)\n",
    "\n",
    "# 定义训练集和测试集的大小\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# 将数据集按比例分成训练集和测试集\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 获取训练集的数据\n",
    "train_f = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "train_u = torch.stack([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "\n",
    "# 获取测试集的数据\n",
    "test_f = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])\n",
    "test_u = torch.stack([test_dataset[i][1] for i in range(len(test_dataset))])\n",
    "\n",
    "train_f = train_f.to(device)\n",
    "train_u = train_u.to(device)\n",
    "test_f = test_f.to(device)\n",
    "test_u = test_u.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3042]), torch.Size([1, 3042]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(0, 1, N)\n",
    "y = np.linspace(0, 1, N)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "val_U1_np = X * (1 - X) * np.sin(np.pi * Y)\n",
    "val_U2_np = np.sin(np.pi * X) * Y * (1 - Y)\n",
    "laplace_u1 = -2 * np.sin(np.pi * Y) - np.pi ** 2 * val_U1_np\n",
    "laplace_u2 = -2 * np.sin(np.pi * X) - np.pi ** 2 * val_U2_np\n",
    "val_F1_np = val_U1_np - lambda_ * laplace_u2\n",
    "val_F2_np = lambda_ * laplace_u1 + val_U2_np\n",
    "\n",
    "val_f = np.concatenate([val_F1_np[1:-1, 1:-1].flatten(), val_F2_np[1:-1, 1:-1].flatten()])\n",
    "val_u = np.concatenate([val_U1_np[1:-1, 1:-1].flatten(), val_U2_np[1:-1, 1:-1].flatten()])\n",
    "\n",
    "val_f = torch.tensor(val_f, dtype=torch.float32).view(1, -1).to(device)\n",
    "val_u = torch.tensor(val_u, dtype=torch.float32).view(1, -1).to(device)\n",
    "\n",
    "val_f.shape, val_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose, delta, path):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.8f} --> {val_loss:.8f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100000] Training Loss: 0.02363700 Testing Loss: 0.02536865 Validation loss: 0.01578795\n",
      "Epoch [200/100000] Training Loss: 0.01778465 Testing Loss: 0.02176935 Validation loss: 0.01356084\n",
      "Epoch [300/100000] Training Loss: 0.01265628 Testing Loss: 0.01830567 Validation loss: 0.01131018\n",
      "Epoch [400/100000] Training Loss: 0.00881489 Testing Loss: 0.01536019 Validation loss: 0.00931465\n",
      "Epoch [500/100000] Training Loss: 0.00615272 Testing Loss: 0.01298216 Validation loss: 0.00765400\n",
      "Epoch [600/100000] Training Loss: 0.00436561 Testing Loss: 0.01109002 Validation loss: 0.00631454\n",
      "Epoch [700/100000] Training Loss: 0.00316938 Testing Loss: 0.00957761 Validation loss: 0.00524814\n",
      "Epoch [800/100000] Training Loss: 0.00235715 Testing Loss: 0.00835288 Validation loss: 0.00440090\n",
      "Epoch [900/100000] Training Loss: 0.00180822 Testing Loss: 0.00735081 Validation loss: 0.00372230\n",
      "Epoch [1000/100000] Training Loss: 0.00139300 Testing Loss: 0.00650545 Validation loss: 0.00318047\n",
      "Epoch [1100/100000] Training Loss: 0.00109837 Testing Loss: 0.00579388 Validation loss: 0.00273892\n",
      "Epoch [1200/100000] Training Loss: 0.00087977 Testing Loss: 0.00518861 Validation loss: 0.00237542\n",
      "Epoch [1300/100000] Training Loss: 0.00169136 Testing Loss: 0.00567992 Validation loss: 0.00215030\n",
      "Epoch [1400/100000] Training Loss: 0.00123773 Testing Loss: 0.00479025 Validation loss: 0.00185853\n",
      "Epoch [1500/100000] Training Loss: 0.00111269 Testing Loss: 0.00443304 Validation loss: 0.00164542\n",
      "Epoch [1600/100000] Training Loss: 0.00201594 Testing Loss: 0.00512815 Validation loss: 0.00152158\n",
      "Epoch [1700/100000] Training Loss: 0.00154934 Testing Loss: 0.00439375 Validation loss: 0.00134395\n",
      "Epoch [1800/100000] Training Loss: 0.00153757 Testing Loss: 0.00420518 Validation loss: 0.00124412\n",
      "Epoch [1900/100000] Training Loss: 0.00211908 Testing Loss: 0.00453987 Validation loss: 0.00115450\n",
      "Epoch [2000/100000] Training Loss: 0.00246231 Testing Loss: 0.00488658 Validation loss: 0.00107742\n",
      "Epoch [2100/100000] Training Loss: 0.00168178 Testing Loss: 0.00372961 Validation loss: 0.00096898\n",
      "Epoch [2200/100000] Training Loss: 0.00150867 Testing Loss: 0.00346573 Validation loss: 0.00088892\n",
      "Epoch [2300/100000] Training Loss: 0.00251617 Testing Loss: 0.00437627 Validation loss: 0.00085886\n",
      "Epoch [2400/100000] Training Loss: 0.00226358 Testing Loss: 0.00407197 Validation loss: 0.00081100\n",
      "Epoch [2500/100000] Training Loss: 0.00139804 Testing Loss: 0.00297048 Validation loss: 0.00071906\n",
      "Epoch [2600/100000] Training Loss: 0.00160914 Testing Loss: 0.00328322 Validation loss: 0.00068568\n",
      "Epoch [2700/100000] Training Loss: 0.00228665 Testing Loss: 0.00395654 Validation loss: 0.00068716\n",
      "Epoch [2800/100000] Training Loss: 0.00201426 Testing Loss: 0.00337108 Validation loss: 0.00063379\n",
      "Epoch [2900/100000] Training Loss: 0.00167565 Testing Loss: 0.00319937 Validation loss: 0.00059138\n",
      "Epoch [3000/100000] Training Loss: 0.00204297 Testing Loss: 0.00335245 Validation loss: 0.00056913\n",
      "Epoch [3100/100000] Training Loss: 0.00239330 Testing Loss: 0.00374784 Validation loss: 0.00057184\n",
      "Epoch [3200/100000] Training Loss: 0.00165968 Testing Loss: 0.00280188 Validation loss: 0.00051039\n",
      "Epoch [3300/100000] Training Loss: 0.00203321 Testing Loss: 0.00314956 Validation loss: 0.00049872\n",
      "Epoch [3400/100000] Training Loss: 0.00198176 Testing Loss: 0.00316368 Validation loss: 0.00049081\n",
      "Epoch [3500/100000] Training Loss: 0.00206596 Testing Loss: 0.00312577 Validation loss: 0.00046073\n",
      "Epoch [3600/100000] Training Loss: 0.00179299 Testing Loss: 0.00278900 Validation loss: 0.00044239\n",
      "Epoch [3700/100000] Training Loss: 0.00180590 Testing Loss: 0.00265294 Validation loss: 0.00042636\n",
      "Epoch [3800/100000] Training Loss: 0.00183171 Testing Loss: 0.00269984 Validation loss: 0.00039627\n",
      "Epoch [3900/100000] Training Loss: 0.00194980 Testing Loss: 0.00281158 Validation loss: 0.00039436\n",
      "Epoch [4000/100000] Training Loss: 0.00173796 Testing Loss: 0.00262481 Validation loss: 0.00038188\n",
      "Epoch [4100/100000] Training Loss: 0.00186409 Testing Loss: 0.00262620 Validation loss: 0.00036501\n",
      "Epoch [4200/100000] Training Loss: 0.00186809 Testing Loss: 0.00260235 Validation loss: 0.00035588\n",
      "Epoch [4300/100000] Training Loss: 0.00188649 Testing Loss: 0.00270353 Validation loss: 0.00034609\n",
      "Epoch [4400/100000] Training Loss: 0.00169129 Testing Loss: 0.00247953 Validation loss: 0.00031856\n",
      "Epoch [4500/100000] Training Loss: 0.00173863 Testing Loss: 0.00239423 Validation loss: 0.00032495\n",
      "Epoch [4600/100000] Training Loss: 0.00188369 Testing Loss: 0.00254950 Validation loss: 0.00030799\n",
      "Epoch [4700/100000] Training Loss: 0.00205302 Testing Loss: 0.00263517 Validation loss: 0.00031740\n",
      "Epoch [4800/100000] Training Loss: 0.00183642 Testing Loss: 0.00255463 Validation loss: 0.00031045\n",
      "Epoch [4900/100000] Training Loss: 0.00180510 Testing Loss: 0.00245311 Validation loss: 0.00028717\n",
      "Epoch [5000/100000] Training Loss: 0.00171370 Testing Loss: 0.00230089 Validation loss: 0.00027308\n",
      "Epoch [5100/100000] Training Loss: 0.00181331 Testing Loss: 0.00238631 Validation loss: 0.00028573\n",
      "Epoch [5200/100000] Training Loss: 0.00157730 Testing Loss: 0.00216420 Validation loss: 0.00026001\n",
      "Epoch [5300/100000] Training Loss: 0.00160279 Testing Loss: 0.00215102 Validation loss: 0.00025278\n",
      "Epoch [5400/100000] Training Loss: 0.00202402 Testing Loss: 0.00255692 Validation loss: 0.00026556\n",
      "Epoch [5500/100000] Training Loss: 0.00207536 Testing Loss: 0.00250826 Validation loss: 0.00026372\n",
      "Epoch [5600/100000] Training Loss: 0.00154538 Testing Loss: 0.00196423 Validation loss: 0.00022788\n",
      "Epoch [5700/100000] Training Loss: 0.00188603 Testing Loss: 0.00231813 Validation loss: 0.00024149\n",
      "Epoch [5800/100000] Training Loss: 0.00170397 Testing Loss: 0.00221304 Validation loss: 0.00023395\n",
      "Epoch [5900/100000] Training Loss: 0.00164038 Testing Loss: 0.00219234 Validation loss: 0.00022578\n",
      "Epoch [6000/100000] Training Loss: 0.00173285 Testing Loss: 0.00227596 Validation loss: 0.00022382\n",
      "Epoch [6100/100000] Training Loss: 0.00176007 Testing Loss: 0.00212407 Validation loss: 0.00021461\n",
      "Epoch [6200/100000] Training Loss: 0.00155948 Testing Loss: 0.00212359 Validation loss: 0.00020843\n",
      "Epoch [6300/100000] Training Loss: 0.00180831 Testing Loss: 0.00211921 Validation loss: 0.00021035\n",
      "Epoch [6400/100000] Training Loss: 0.00172848 Testing Loss: 0.00216974 Validation loss: 0.00020710\n",
      "Epoch [6500/100000] Training Loss: 0.00149130 Testing Loss: 0.00189249 Validation loss: 0.00018768\n",
      "Epoch [6600/100000] Training Loss: 0.00203170 Testing Loss: 0.00233947 Validation loss: 0.00020928\n",
      "Epoch [6700/100000] Training Loss: 0.00171899 Testing Loss: 0.00198973 Validation loss: 0.00019058\n",
      "Epoch [6800/100000] Training Loss: 0.00158202 Testing Loss: 0.00183381 Validation loss: 0.00017869\n",
      "Epoch [6900/100000] Training Loss: 0.00192495 Testing Loss: 0.00212152 Validation loss: 0.00018455\n",
      "Epoch [7000/100000] Training Loss: 0.00164436 Testing Loss: 0.00177754 Validation loss: 0.00016737\n",
      "Epoch [7100/100000] Training Loss: 0.00179136 Testing Loss: 0.00214316 Validation loss: 0.00018061\n",
      "Epoch [7200/100000] Training Loss: 0.00174491 Testing Loss: 0.00209862 Validation loss: 0.00018134\n",
      "Epoch [7300/100000] Training Loss: 0.00184629 Testing Loss: 0.00216235 Validation loss: 0.00018654\n",
      "Epoch [7400/100000] Training Loss: 0.00187373 Testing Loss: 0.00201585 Validation loss: 0.00017453\n",
      "Epoch [7500/100000] Training Loss: 0.00202040 Testing Loss: 0.00225672 Validation loss: 0.00018026\n",
      "Epoch [7600/100000] Training Loss: 0.00206055 Testing Loss: 0.00247277 Validation loss: 0.00019083\n",
      "Epoch [7700/100000] Training Loss: 0.00161495 Testing Loss: 0.00172802 Validation loss: 0.00015341\n",
      "Epoch [7800/100000] Training Loss: 0.00182799 Testing Loss: 0.00209233 Validation loss: 0.00016265\n",
      "Epoch [7900/100000] Training Loss: 0.00175456 Testing Loss: 0.00196568 Validation loss: 0.00015706\n",
      "Epoch [8000/100000] Training Loss: 0.00190801 Testing Loss: 0.00205167 Validation loss: 0.00015914\n",
      "Epoch [8100/100000] Training Loss: 0.00156910 Testing Loss: 0.00196661 Validation loss: 0.00015168\n",
      "Epoch [8200/100000] Training Loss: 0.00186749 Testing Loss: 0.00209938 Validation loss: 0.00016159\n",
      "Epoch [8300/100000] Training Loss: 0.00182948 Testing Loss: 0.00207889 Validation loss: 0.00015962\n",
      "Epoch [8400/100000] Training Loss: 0.00194701 Testing Loss: 0.00217151 Validation loss: 0.00015642\n",
      "Epoch [8500/100000] Training Loss: 0.00186029 Testing Loss: 0.00191351 Validation loss: 0.00014658\n",
      "Epoch [8600/100000] Training Loss: 0.00184631 Testing Loss: 0.00204892 Validation loss: 0.00015323\n",
      "Epoch [8700/100000] Training Loss: 0.00192481 Testing Loss: 0.00208902 Validation loss: 0.00014997\n",
      "Epoch [8800/100000] Training Loss: 0.00190628 Testing Loss: 0.00209897 Validation loss: 0.00015202\n",
      "Epoch 08866: reducing learning rate of group 0 to 7.0000e-04.\n",
      "Epoch [8900/100000] Training Loss: 0.00004253 Testing Loss: 0.00020893 Validation loss: 0.00005598\n",
      "Epoch [9000/100000] Training Loss: 0.00000616 Testing Loss: 0.00018810 Validation loss: 0.00005433\n",
      "Epoch [9100/100000] Training Loss: 0.00000602 Testing Loss: 0.00018443 Validation loss: 0.00005320\n",
      "Epoch [9200/100000] Training Loss: 0.00000587 Testing Loss: 0.00018066 Validation loss: 0.00005205\n",
      "Epoch [9300/100000] Training Loss: 0.00000572 Testing Loss: 0.00017680 Validation loss: 0.00005087\n",
      "Epoch [9400/100000] Training Loss: 0.00000557 Testing Loss: 0.00017285 Validation loss: 0.00004966\n",
      "Epoch [9500/100000] Training Loss: 0.00000541 Testing Loss: 0.00016881 Validation loss: 0.00004843\n",
      "Epoch [9600/100000] Training Loss: 0.00079399 Testing Loss: 0.00102093 Validation loss: 0.00008798\n",
      "Epoch [9700/100000] Training Loss: 0.00089546 Testing Loss: 0.00102655 Validation loss: 0.00008485\n",
      "Epoch [9800/100000] Training Loss: 0.00092749 Testing Loss: 0.00105709 Validation loss: 0.00008823\n",
      "Epoch [9900/100000] Training Loss: 0.00088865 Testing Loss: 0.00103762 Validation loss: 0.00008804\n",
      "Epoch [10000/100000] Training Loss: 0.00087711 Testing Loss: 0.00101065 Validation loss: 0.00008804\n",
      "Epoch [10100/100000] Training Loss: 0.00066578 Testing Loss: 0.00086127 Validation loss: 0.00007707\n",
      "Epoch [10200/100000] Training Loss: 0.00093514 Testing Loss: 0.00108005 Validation loss: 0.00008914\n",
      "Epoch [10300/100000] Training Loss: 0.00077396 Testing Loss: 0.00088666 Validation loss: 0.00007633\n",
      "Epoch [10400/100000] Training Loss: 0.00101217 Testing Loss: 0.00113432 Validation loss: 0.00008776\n",
      "Epoch [10500/100000] Training Loss: 0.00090423 Testing Loss: 0.00098063 Validation loss: 0.00008032\n",
      "Epoch [10600/100000] Training Loss: 0.00103916 Testing Loss: 0.00116222 Validation loss: 0.00008523\n",
      "Epoch [10700/100000] Training Loss: 0.00083064 Testing Loss: 0.00092878 Validation loss: 0.00007605\n",
      "Epoch [10800/100000] Training Loss: 0.00084678 Testing Loss: 0.00089638 Validation loss: 0.00007311\n",
      "Epoch [10900/100000] Training Loss: 0.00088677 Testing Loss: 0.00103069 Validation loss: 0.00007997\n",
      "Epoch [11000/100000] Training Loss: 0.00085997 Testing Loss: 0.00094929 Validation loss: 0.00007448\n",
      "Epoch 11069: reducing learning rate of group 0 to 4.9000e-04.\n",
      "Epoch [11100/100000] Training Loss: 0.00002049 Testing Loss: 0.00013513 Validation loss: 0.00003319\n",
      "Epoch [11200/100000] Training Loss: 0.00000336 Testing Loss: 0.00011257 Validation loss: 0.00003152\n",
      "Epoch [11300/100000] Training Loss: 0.00000329 Testing Loss: 0.00011056 Validation loss: 0.00003093\n",
      "Epoch [11400/100000] Training Loss: 0.00000322 Testing Loss: 0.00010850 Validation loss: 0.00003032\n",
      "Epoch [11500/100000] Training Loss: 0.00000315 Testing Loss: 0.00010637 Validation loss: 0.00002969\n",
      "Epoch [11600/100000] Training Loss: 0.00000307 Testing Loss: 0.00010419 Validation loss: 0.00002905\n",
      "Epoch [11700/100000] Training Loss: 0.00000300 Testing Loss: 0.00010196 Validation loss: 0.00002839\n",
      "Epoch [11800/100000] Training Loss: 0.00035039 Testing Loss: 0.00047141 Validation loss: 0.00004643\n",
      "Epoch [11900/100000] Training Loss: 0.00052120 Testing Loss: 0.00058007 Validation loss: 0.00005017\n",
      "Epoch [12000/100000] Training Loss: 0.00043356 Testing Loss: 0.00051021 Validation loss: 0.00004723\n",
      "Epoch [12100/100000] Training Loss: 0.00053142 Testing Loss: 0.00062116 Validation loss: 0.00005254\n",
      "Epoch [12200/100000] Training Loss: 0.00039247 Testing Loss: 0.00048877 Validation loss: 0.00004493\n",
      "Epoch [12300/100000] Training Loss: 0.00042560 Testing Loss: 0.00052763 Validation loss: 0.00004763\n",
      "Epoch [12400/100000] Training Loss: 0.00042591 Testing Loss: 0.00054751 Validation loss: 0.00004616\n",
      "Epoch [12500/100000] Training Loss: 0.00042293 Testing Loss: 0.00051031 Validation loss: 0.00004441\n",
      "Epoch [12600/100000] Training Loss: 0.00042823 Testing Loss: 0.00054126 Validation loss: 0.00004476\n",
      "Epoch [12700/100000] Training Loss: 0.00042326 Testing Loss: 0.00048795 Validation loss: 0.00004267\n",
      "Epoch [12800/100000] Training Loss: 0.00045924 Testing Loss: 0.00050164 Validation loss: 0.00004298\n",
      "Epoch [12900/100000] Training Loss: 0.00045801 Testing Loss: 0.00049539 Validation loss: 0.00004171\n",
      "Epoch [13000/100000] Training Loss: 0.00040195 Testing Loss: 0.00045811 Validation loss: 0.00003836\n",
      "Epoch [13100/100000] Training Loss: 0.00050929 Testing Loss: 0.00056393 Validation loss: 0.00004313\n",
      "Epoch [13200/100000] Training Loss: 0.00038723 Testing Loss: 0.00045686 Validation loss: 0.00003947\n",
      "Epoch 13265: reducing learning rate of group 0 to 3.4300e-04.\n",
      "Epoch [13300/100000] Training Loss: 0.00000808 Testing Loss: 0.00007726 Validation loss: 0.00001976\n",
      "Epoch [13400/100000] Training Loss: 0.00000195 Testing Loss: 0.00007004 Validation loss: 0.00001912\n",
      "Epoch [13500/100000] Training Loss: 0.00000191 Testing Loss: 0.00006887 Validation loss: 0.00001878\n",
      "Epoch [13600/100000] Training Loss: 0.00000188 Testing Loss: 0.00006766 Validation loss: 0.00001844\n",
      "Epoch [13700/100000] Training Loss: 0.00000184 Testing Loss: 0.00006642 Validation loss: 0.00001808\n",
      "Epoch [13800/100000] Training Loss: 0.00000180 Testing Loss: 0.00006514 Validation loss: 0.00001771\n",
      "Epoch [13900/100000] Training Loss: 0.00000176 Testing Loss: 0.00006383 Validation loss: 0.00001734\n",
      "Epoch [14000/100000] Training Loss: 0.00018420 Testing Loss: 0.00023944 Validation loss: 0.00002621\n",
      "Epoch [14100/100000] Training Loss: 0.00020540 Testing Loss: 0.00027342 Validation loss: 0.00002712\n",
      "Epoch [14200/100000] Training Loss: 0.00019248 Testing Loss: 0.00025876 Validation loss: 0.00002530\n",
      "Epoch [14300/100000] Training Loss: 0.00023740 Testing Loss: 0.00030165 Validation loss: 0.00002827\n",
      "Epoch [14400/100000] Training Loss: 0.00023282 Testing Loss: 0.00028709 Validation loss: 0.00002649\n",
      "Epoch [14500/100000] Training Loss: 0.00020884 Testing Loss: 0.00027502 Validation loss: 0.00002535\n",
      "Epoch [14600/100000] Training Loss: 0.00022565 Testing Loss: 0.00026393 Validation loss: 0.00002497\n",
      "Epoch [14700/100000] Training Loss: 0.00022395 Testing Loss: 0.00027590 Validation loss: 0.00002545\n",
      "Epoch [14800/100000] Training Loss: 0.00025685 Testing Loss: 0.00030876 Validation loss: 0.00002737\n",
      "Epoch [14900/100000] Training Loss: 0.00021569 Testing Loss: 0.00026967 Validation loss: 0.00002411\n",
      "Epoch [15000/100000] Training Loss: 0.00015934 Testing Loss: 0.00019678 Validation loss: 0.00002042\n",
      "Epoch [15100/100000] Training Loss: 0.00021838 Testing Loss: 0.00027349 Validation loss: 0.00002474\n",
      "Epoch [15200/100000] Training Loss: 0.00020062 Testing Loss: 0.00025925 Validation loss: 0.00002291\n",
      "Epoch [15300/100000] Training Loss: 0.00020599 Testing Loss: 0.00024754 Validation loss: 0.00002298\n",
      "Epoch [15400/100000] Training Loss: 0.00017114 Testing Loss: 0.00022642 Validation loss: 0.00002106\n",
      "Epoch 15471: reducing learning rate of group 0 to 2.4010e-04.\n",
      "Epoch [15500/100000] Training Loss: 0.00000714 Testing Loss: 0.00005191 Validation loss: 0.00001256\n",
      "Epoch [15600/100000] Training Loss: 0.00000118 Testing Loss: 0.00004476 Validation loss: 0.00001196\n",
      "Epoch [15700/100000] Training Loss: 0.00000116 Testing Loss: 0.00004406 Validation loss: 0.00001176\n",
      "Epoch [15800/100000] Training Loss: 0.00000114 Testing Loss: 0.00004332 Validation loss: 0.00001156\n",
      "Epoch [15900/100000] Training Loss: 0.00000112 Testing Loss: 0.00004257 Validation loss: 0.00001135\n",
      "Epoch [16000/100000] Training Loss: 0.00000109 Testing Loss: 0.00004179 Validation loss: 0.00001113\n",
      "Epoch [16100/100000] Training Loss: 0.00000107 Testing Loss: 0.00004099 Validation loss: 0.00001091\n",
      "Epoch [16200/100000] Training Loss: 0.00006482 Testing Loss: 0.00010805 Validation loss: 0.00001379\n",
      "Epoch [16300/100000] Training Loss: 0.00010769 Testing Loss: 0.00014298 Validation loss: 0.00001521\n",
      "Epoch [16400/100000] Training Loss: 0.00009852 Testing Loss: 0.00013705 Validation loss: 0.00001520\n",
      "Epoch [16500/100000] Training Loss: 0.00012691 Testing Loss: 0.00015216 Validation loss: 0.00001604\n",
      "Epoch [16600/100000] Training Loss: 0.00010039 Testing Loss: 0.00013902 Validation loss: 0.00001497\n",
      "Epoch [16700/100000] Training Loss: 0.00010179 Testing Loss: 0.00013537 Validation loss: 0.00001406\n",
      "Epoch [16800/100000] Training Loss: 0.00009941 Testing Loss: 0.00013619 Validation loss: 0.00001402\n",
      "Epoch [16900/100000] Training Loss: 0.00010555 Testing Loss: 0.00013610 Validation loss: 0.00001420\n",
      "Epoch [17000/100000] Training Loss: 0.00009490 Testing Loss: 0.00012842 Validation loss: 0.00001415\n",
      "Epoch [17100/100000] Training Loss: 0.00010803 Testing Loss: 0.00013835 Validation loss: 0.00001424\n",
      "Epoch [17200/100000] Training Loss: 0.00011765 Testing Loss: 0.00014685 Validation loss: 0.00001436\n",
      "Epoch [17300/100000] Training Loss: 0.00010441 Testing Loss: 0.00012941 Validation loss: 0.00001359\n",
      "Epoch [17400/100000] Training Loss: 0.00010575 Testing Loss: 0.00013582 Validation loss: 0.00001319\n",
      "Epoch [17500/100000] Training Loss: 0.00010045 Testing Loss: 0.00013437 Validation loss: 0.00001365\n",
      "Epoch [17600/100000] Training Loss: 0.00011844 Testing Loss: 0.00014744 Validation loss: 0.00001370\n",
      "Epoch 17676: reducing learning rate of group 0 to 1.6807e-04.\n",
      "Epoch [17700/100000] Training Loss: 0.00000484 Testing Loss: 0.00003542 Validation loss: 0.00000808\n",
      "Epoch [17800/100000] Training Loss: 0.00000074 Testing Loss: 0.00002920 Validation loss: 0.00000766\n",
      "Epoch [17900/100000] Training Loss: 0.00000073 Testing Loss: 0.00002876 Validation loss: 0.00000754\n",
      "Epoch [18000/100000] Training Loss: 0.00000071 Testing Loss: 0.00002830 Validation loss: 0.00000742\n",
      "Epoch [18100/100000] Training Loss: 0.00000070 Testing Loss: 0.00002783 Validation loss: 0.00000729\n",
      "Epoch [18200/100000] Training Loss: 0.00000069 Testing Loss: 0.00002735 Validation loss: 0.00000715\n",
      "Epoch [18300/100000] Training Loss: 0.00000067 Testing Loss: 0.00002684 Validation loss: 0.00000702\n",
      "Epoch [18400/100000] Training Loss: 0.00000991 Testing Loss: 0.00003834 Validation loss: 0.00000746\n",
      "Epoch [18500/100000] Training Loss: 0.00004814 Testing Loss: 0.00007357 Validation loss: 0.00000935\n",
      "Epoch [18600/100000] Training Loss: 0.00004926 Testing Loss: 0.00007540 Validation loss: 0.00000905\n",
      "Epoch [18700/100000] Training Loss: 0.00005153 Testing Loss: 0.00007634 Validation loss: 0.00000876\n",
      "Epoch [18800/100000] Training Loss: 0.00004578 Testing Loss: 0.00006769 Validation loss: 0.00000842\n",
      "Epoch [18900/100000] Training Loss: 0.00004902 Testing Loss: 0.00006850 Validation loss: 0.00000844\n",
      "Epoch [19000/100000] Training Loss: 0.00005207 Testing Loss: 0.00007293 Validation loss: 0.00000875\n",
      "Epoch [19100/100000] Training Loss: 0.00004775 Testing Loss: 0.00007014 Validation loss: 0.00000840\n",
      "Epoch [19200/100000] Training Loss: 0.00004718 Testing Loss: 0.00007343 Validation loss: 0.00000843\n",
      "Epoch [19300/100000] Training Loss: 0.00005108 Testing Loss: 0.00007237 Validation loss: 0.00000817\n",
      "Epoch [19400/100000] Training Loss: 0.00005594 Testing Loss: 0.00007284 Validation loss: 0.00000792\n",
      "Epoch [19500/100000] Training Loss: 0.00005122 Testing Loss: 0.00007050 Validation loss: 0.00000816\n",
      "Epoch [19600/100000] Training Loss: 0.00004817 Testing Loss: 0.00006811 Validation loss: 0.00000771\n",
      "Epoch [19700/100000] Training Loss: 0.00005039 Testing Loss: 0.00007099 Validation loss: 0.00000784\n",
      "Epoch [19800/100000] Training Loss: 0.00005102 Testing Loss: 0.00007319 Validation loss: 0.00000779\n",
      "Epoch 19879: reducing learning rate of group 0 to 1.1765e-04.\n",
      "Epoch [19900/100000] Training Loss: 0.00000303 Testing Loss: 0.00002240 Validation loss: 0.00000518\n",
      "Epoch [20000/100000] Training Loss: 0.00000047 Testing Loss: 0.00001936 Validation loss: 0.00000500\n",
      "Epoch [20100/100000] Training Loss: 0.00000046 Testing Loss: 0.00001908 Validation loss: 0.00000492\n",
      "Epoch [20200/100000] Training Loss: 0.00000046 Testing Loss: 0.00001878 Validation loss: 0.00000485\n",
      "Epoch [20300/100000] Training Loss: 0.00000045 Testing Loss: 0.00001848 Validation loss: 0.00000476\n",
      "Epoch [20400/100000] Training Loss: 0.00000044 Testing Loss: 0.00001817 Validation loss: 0.00000468\n",
      "Epoch [20500/100000] Training Loss: 0.00000043 Testing Loss: 0.00001785 Validation loss: 0.00000460\n",
      "Epoch [20600/100000] Training Loss: 0.00000307 Testing Loss: 0.00002080 Validation loss: 0.00000466\n",
      "Epoch [20700/100000] Training Loss: 0.00002551 Testing Loss: 0.00004383 Validation loss: 0.00000586\n",
      "Epoch [20800/100000] Training Loss: 0.00002488 Testing Loss: 0.00004044 Validation loss: 0.00000557\n",
      "Epoch [20900/100000] Training Loss: 0.00002828 Testing Loss: 0.00004152 Validation loss: 0.00000545\n",
      "Epoch [21000/100000] Training Loss: 0.00002545 Testing Loss: 0.00003994 Validation loss: 0.00000523\n",
      "Epoch [21100/100000] Training Loss: 0.00002436 Testing Loss: 0.00004061 Validation loss: 0.00000527\n",
      "Epoch [21200/100000] Training Loss: 0.00002550 Testing Loss: 0.00004046 Validation loss: 0.00000535\n",
      "Epoch [21300/100000] Training Loss: 0.00002616 Testing Loss: 0.00004040 Validation loss: 0.00000515\n",
      "Epoch [21400/100000] Training Loss: 0.00002821 Testing Loss: 0.00004169 Validation loss: 0.00000516\n",
      "Epoch [21500/100000] Training Loss: 0.00002800 Testing Loss: 0.00004139 Validation loss: 0.00000518\n",
      "Epoch [21600/100000] Training Loss: 0.00002196 Testing Loss: 0.00003816 Validation loss: 0.00000490\n",
      "Epoch [21700/100000] Training Loss: 0.00002549 Testing Loss: 0.00004033 Validation loss: 0.00000508\n",
      "Epoch [21800/100000] Training Loss: 0.00002315 Testing Loss: 0.00003705 Validation loss: 0.00000479\n",
      "Epoch [21900/100000] Training Loss: 0.00002292 Testing Loss: 0.00003521 Validation loss: 0.00000461\n",
      "Epoch [22000/100000] Training Loss: 0.00002843 Testing Loss: 0.00004123 Validation loss: 0.00000476\n",
      "Epoch 22078: reducing learning rate of group 0 to 8.2354e-05.\n",
      "Epoch [22100/100000] Training Loss: 0.00000165 Testing Loss: 0.00001474 Validation loss: 0.00000344\n",
      "Epoch [22200/100000] Training Loss: 0.00000031 Testing Loss: 0.00001300 Validation loss: 0.00000331\n",
      "Epoch [22300/100000] Training Loss: 0.00000030 Testing Loss: 0.00001282 Validation loss: 0.00000326\n",
      "Epoch [22400/100000] Training Loss: 0.00000030 Testing Loss: 0.00001263 Validation loss: 0.00000321\n",
      "Epoch [22500/100000] Training Loss: 0.00000029 Testing Loss: 0.00001243 Validation loss: 0.00000316\n",
      "Epoch [22600/100000] Training Loss: 0.00000029 Testing Loss: 0.00001222 Validation loss: 0.00000311\n",
      "Epoch [22700/100000] Training Loss: 0.00000028 Testing Loss: 0.00001201 Validation loss: 0.00000305\n",
      "Epoch [22800/100000] Training Loss: 0.00000173 Testing Loss: 0.00001351 Validation loss: 0.00000308\n",
      "Epoch [22900/100000] Training Loss: 0.00001115 Testing Loss: 0.00002313 Validation loss: 0.00000356\n",
      "Epoch [23000/100000] Training Loss: 0.00001161 Testing Loss: 0.00002298 Validation loss: 0.00000342\n",
      "Epoch [23100/100000] Training Loss: 0.00001198 Testing Loss: 0.00002251 Validation loss: 0.00000338\n",
      "Epoch [23200/100000] Training Loss: 0.00001119 Testing Loss: 0.00002139 Validation loss: 0.00000327\n",
      "Epoch [23300/100000] Training Loss: 0.00000864 Testing Loss: 0.00001958 Validation loss: 0.00000322\n",
      "Epoch [23400/100000] Training Loss: 0.00001364 Testing Loss: 0.00002432 Validation loss: 0.00000345\n",
      "Epoch [23500/100000] Training Loss: 0.00001166 Testing Loss: 0.00002135 Validation loss: 0.00000316\n",
      "Epoch [23600/100000] Training Loss: 0.00001072 Testing Loss: 0.00002100 Validation loss: 0.00000311\n",
      "Epoch [23700/100000] Training Loss: 0.00001189 Testing Loss: 0.00002153 Validation loss: 0.00000310\n",
      "Epoch [23800/100000] Training Loss: 0.00000990 Testing Loss: 0.00001980 Validation loss: 0.00000289\n",
      "Epoch [23900/100000] Training Loss: 0.00001147 Testing Loss: 0.00002060 Validation loss: 0.00000296\n",
      "Epoch [24000/100000] Training Loss: 0.00001196 Testing Loss: 0.00002099 Validation loss: 0.00000292\n",
      "Epoch [24100/100000] Training Loss: 0.00001083 Testing Loss: 0.00002021 Validation loss: 0.00000286\n",
      "Epoch [24200/100000] Training Loss: 0.00001204 Testing Loss: 0.00002110 Validation loss: 0.00000286\n",
      "Epoch 24284: reducing learning rate of group 0 to 5.7648e-05.\n",
      "Epoch [24300/100000] Training Loss: 0.00000176 Testing Loss: 0.00000980 Validation loss: 0.00000230\n",
      "Epoch [24400/100000] Training Loss: 0.00000020 Testing Loss: 0.00000882 Validation loss: 0.00000222\n",
      "Epoch [24500/100000] Training Loss: 0.00000020 Testing Loss: 0.00000870 Validation loss: 0.00000219\n",
      "Epoch [24600/100000] Training Loss: 0.00000020 Testing Loss: 0.00000857 Validation loss: 0.00000215\n",
      "Epoch [24700/100000] Training Loss: 0.00000019 Testing Loss: 0.00000844 Validation loss: 0.00000212\n",
      "Epoch [24800/100000] Training Loss: 0.00000019 Testing Loss: 0.00000831 Validation loss: 0.00000209\n",
      "Epoch [24900/100000] Training Loss: 0.00000019 Testing Loss: 0.00000817 Validation loss: 0.00000205\n",
      "Epoch [25000/100000] Training Loss: 0.00000026 Testing Loss: 0.00000810 Validation loss: 0.00000201\n",
      "Epoch [25100/100000] Training Loss: 0.00000599 Testing Loss: 0.00001341 Validation loss: 0.00000225\n",
      "Epoch [25200/100000] Training Loss: 0.00000564 Testing Loss: 0.00001316 Validation loss: 0.00000220\n",
      "Epoch [25300/100000] Training Loss: 0.00000656 Testing Loss: 0.00001377 Validation loss: 0.00000226\n",
      "Epoch [25400/100000] Training Loss: 0.00000591 Testing Loss: 0.00001304 Validation loss: 0.00000211\n",
      "Epoch [25500/100000] Training Loss: 0.00000638 Testing Loss: 0.00001361 Validation loss: 0.00000213\n",
      "Epoch [25600/100000] Training Loss: 0.00000653 Testing Loss: 0.00001348 Validation loss: 0.00000210\n",
      "Epoch [25700/100000] Training Loss: 0.00000569 Testing Loss: 0.00001270 Validation loss: 0.00000205\n",
      "Epoch [25800/100000] Training Loss: 0.00000641 Testing Loss: 0.00001323 Validation loss: 0.00000204\n",
      "Epoch [25900/100000] Training Loss: 0.00000573 Testing Loss: 0.00001282 Validation loss: 0.00000203\n",
      "Epoch [26000/100000] Training Loss: 0.00000602 Testing Loss: 0.00001273 Validation loss: 0.00000194\n",
      "Epoch [26100/100000] Training Loss: 0.00000547 Testing Loss: 0.00001154 Validation loss: 0.00000189\n",
      "Epoch [26200/100000] Training Loss: 0.00000531 Testing Loss: 0.00001166 Validation loss: 0.00000188\n",
      "Epoch [26300/100000] Training Loss: 0.00000544 Testing Loss: 0.00001163 Validation loss: 0.00000186\n",
      "Epoch [26400/100000] Training Loss: 0.00000652 Testing Loss: 0.00001200 Validation loss: 0.00000185\n",
      "Epoch 26491: reducing learning rate of group 0 to 4.0354e-05.\n",
      "Epoch [26500/100000] Training Loss: 0.00000175 Testing Loss: 0.00000704 Validation loss: 0.00000157\n",
      "Epoch [26600/100000] Training Loss: 0.00000014 Testing Loss: 0.00000604 Validation loss: 0.00000150\n",
      "Epoch [26700/100000] Training Loss: 0.00000013 Testing Loss: 0.00000595 Validation loss: 0.00000148\n",
      "Epoch [26800/100000] Training Loss: 0.00000013 Testing Loss: 0.00000587 Validation loss: 0.00000146\n",
      "Epoch [26900/100000] Training Loss: 0.00000013 Testing Loss: 0.00000578 Validation loss: 0.00000144\n",
      "Epoch [27000/100000] Training Loss: 0.00000013 Testing Loss: 0.00000569 Validation loss: 0.00000141\n",
      "Epoch [27100/100000] Training Loss: 0.00000013 Testing Loss: 0.00000560 Validation loss: 0.00000139\n",
      "Epoch [27200/100000] Training Loss: 0.00000012 Testing Loss: 0.00000550 Validation loss: 0.00000136\n",
      "Epoch [27300/100000] Training Loss: 0.00000349 Testing Loss: 0.00000862 Validation loss: 0.00000150\n",
      "Epoch [27400/100000] Training Loss: 0.00000282 Testing Loss: 0.00000803 Validation loss: 0.00000144\n",
      "Epoch [27500/100000] Training Loss: 0.00000304 Testing Loss: 0.00000819 Validation loss: 0.00000148\n",
      "Epoch [27600/100000] Training Loss: 0.00000291 Testing Loss: 0.00000807 Validation loss: 0.00000145\n",
      "Epoch [27700/100000] Training Loss: 0.00000283 Testing Loss: 0.00000764 Validation loss: 0.00000139\n",
      "Epoch [27800/100000] Training Loss: 0.00000274 Testing Loss: 0.00000767 Validation loss: 0.00000137\n",
      "Epoch [27900/100000] Training Loss: 0.00000241 Testing Loss: 0.00000704 Validation loss: 0.00000131\n",
      "Epoch [28000/100000] Training Loss: 0.00000303 Testing Loss: 0.00000760 Validation loss: 0.00000131\n",
      "Epoch [28100/100000] Training Loss: 0.00000314 Testing Loss: 0.00000765 Validation loss: 0.00000131\n",
      "Epoch [28200/100000] Training Loss: 0.00000250 Testing Loss: 0.00000704 Validation loss: 0.00000124\n",
      "Epoch [28300/100000] Training Loss: 0.00000287 Testing Loss: 0.00000734 Validation loss: 0.00000126\n",
      "Epoch [28400/100000] Training Loss: 0.00000294 Testing Loss: 0.00000726 Validation loss: 0.00000123\n",
      "Epoch [28500/100000] Training Loss: 0.00000294 Testing Loss: 0.00000715 Validation loss: 0.00000122\n",
      "Epoch [28600/100000] Training Loss: 0.00000284 Testing Loss: 0.00000693 Validation loss: 0.00000120\n",
      "Epoch [28700/100000] Training Loss: 0.00000328 Testing Loss: 0.00000738 Validation loss: 0.00000119\n",
      "Epoch 28703: reducing learning rate of group 0 to 2.8248e-05.\n",
      "Epoch [28800/100000] Training Loss: 0.00000009 Testing Loss: 0.00000416 Validation loss: 0.00000102\n",
      "Epoch [28900/100000] Training Loss: 0.00000009 Testing Loss: 0.00000411 Validation loss: 0.00000101\n",
      "Epoch [29000/100000] Training Loss: 0.00000009 Testing Loss: 0.00000405 Validation loss: 0.00000099\n",
      "Epoch [29100/100000] Training Loss: 0.00000009 Testing Loss: 0.00000399 Validation loss: 0.00000098\n",
      "Epoch [29200/100000] Training Loss: 0.00000009 Testing Loss: 0.00000393 Validation loss: 0.00000096\n",
      "Epoch [29300/100000] Training Loss: 0.00000008 Testing Loss: 0.00000387 Validation loss: 0.00000095\n",
      "Epoch [29400/100000] Training Loss: 0.00000008 Testing Loss: 0.00000380 Validation loss: 0.00000093\n",
      "Epoch [29500/100000] Training Loss: 0.00000127 Testing Loss: 0.00000496 Validation loss: 0.00000098\n",
      "Epoch [29600/100000] Training Loss: 0.00000138 Testing Loss: 0.00000497 Validation loss: 0.00000096\n",
      "Epoch [29700/100000] Training Loss: 0.00000164 Testing Loss: 0.00000510 Validation loss: 0.00000097\n",
      "Epoch [29800/100000] Training Loss: 0.00000140 Testing Loss: 0.00000492 Validation loss: 0.00000093\n",
      "Epoch [29900/100000] Training Loss: 0.00000155 Testing Loss: 0.00000492 Validation loss: 0.00000090\n",
      "Epoch [30000/100000] Training Loss: 0.00000143 Testing Loss: 0.00000476 Validation loss: 0.00000090\n",
      "Epoch [30100/100000] Training Loss: 0.00000130 Testing Loss: 0.00000469 Validation loss: 0.00000089\n",
      "Epoch [30200/100000] Training Loss: 0.00000147 Testing Loss: 0.00000461 Validation loss: 0.00000086\n",
      "Epoch [30300/100000] Training Loss: 0.00000144 Testing Loss: 0.00000462 Validation loss: 0.00000085\n",
      "Epoch [30400/100000] Training Loss: 0.00000155 Testing Loss: 0.00000460 Validation loss: 0.00000086\n",
      "Epoch [30500/100000] Training Loss: 0.00000135 Testing Loss: 0.00000443 Validation loss: 0.00000083\n",
      "Epoch [30600/100000] Training Loss: 0.00000145 Testing Loss: 0.00000438 Validation loss: 0.00000082\n",
      "Epoch [30700/100000] Training Loss: 0.00000141 Testing Loss: 0.00000440 Validation loss: 0.00000079\n",
      "Epoch [30800/100000] Training Loss: 0.00000132 Testing Loss: 0.00000426 Validation loss: 0.00000078\n",
      "Epoch [30900/100000] Training Loss: 0.00000133 Testing Loss: 0.00000417 Validation loss: 0.00000078\n",
      "Epoch 30928: reducing learning rate of group 0 to 1.9773e-05.\n",
      "Epoch [31000/100000] Training Loss: 0.00000006 Testing Loss: 0.00000288 Validation loss: 0.00000070\n",
      "Epoch [31100/100000] Training Loss: 0.00000006 Testing Loss: 0.00000285 Validation loss: 0.00000069\n",
      "Epoch [31200/100000] Training Loss: 0.00000006 Testing Loss: 0.00000281 Validation loss: 0.00000068\n",
      "Epoch [31300/100000] Training Loss: 0.00000006 Testing Loss: 0.00000277 Validation loss: 0.00000067\n",
      "Epoch [31400/100000] Training Loss: 0.00000006 Testing Loss: 0.00000273 Validation loss: 0.00000066\n",
      "Epoch [31500/100000] Training Loss: 0.00000006 Testing Loss: 0.00000269 Validation loss: 0.00000065\n",
      "Epoch [31600/100000] Training Loss: 0.00000006 Testing Loss: 0.00000264 Validation loss: 0.00000064\n",
      "Epoch [31700/100000] Training Loss: 0.00000071 Testing Loss: 0.00000324 Validation loss: 0.00000066\n",
      "Epoch [31800/100000] Training Loss: 0.00000063 Testing Loss: 0.00000311 Validation loss: 0.00000065\n",
      "Epoch [31900/100000] Training Loss: 0.00000065 Testing Loss: 0.00000311 Validation loss: 0.00000064\n",
      "Epoch [32000/100000] Training Loss: 0.00000059 Testing Loss: 0.00000299 Validation loss: 0.00000063\n",
      "Epoch [32100/100000] Training Loss: 0.00000055 Testing Loss: 0.00000293 Validation loss: 0.00000061\n",
      "Epoch [32200/100000] Training Loss: 0.00000081 Testing Loss: 0.00000314 Validation loss: 0.00000061\n",
      "Epoch [32300/100000] Training Loss: 0.00000077 Testing Loss: 0.00000303 Validation loss: 0.00000060\n",
      "Epoch [32400/100000] Training Loss: 0.00000066 Testing Loss: 0.00000293 Validation loss: 0.00000058\n",
      "Epoch [32500/100000] Training Loss: 0.00000063 Testing Loss: 0.00000286 Validation loss: 0.00000058\n",
      "Epoch [32600/100000] Training Loss: 0.00000069 Testing Loss: 0.00000285 Validation loss: 0.00000057\n",
      "Epoch [32700/100000] Training Loss: 0.00000059 Testing Loss: 0.00000274 Validation loss: 0.00000056\n",
      "Epoch [32800/100000] Training Loss: 0.00000072 Testing Loss: 0.00000277 Validation loss: 0.00000054\n",
      "Epoch [32900/100000] Training Loss: 0.00000074 Testing Loss: 0.00000286 Validation loss: 0.00000055\n",
      "Epoch [33000/100000] Training Loss: 0.00000063 Testing Loss: 0.00000269 Validation loss: 0.00000053\n",
      "Epoch [33100/100000] Training Loss: 0.00000063 Testing Loss: 0.00000264 Validation loss: 0.00000051\n",
      "Epoch [33200/100000] Training Loss: 0.00000073 Testing Loss: 0.00000271 Validation loss: 0.00000052\n",
      "Epoch [33300/100000] Training Loss: 0.00000069 Testing Loss: 0.00000260 Validation loss: 0.00000050\n",
      "Epoch [33400/100000] Training Loss: 0.00000063 Testing Loss: 0.00000254 Validation loss: 0.00000050\n",
      "Epoch [33500/100000] Training Loss: 0.00000075 Testing Loss: 0.00000260 Validation loss: 0.00000049\n",
      "Epoch [33600/100000] Training Loss: 0.00000067 Testing Loss: 0.00000249 Validation loss: 0.00000048\n",
      "Epoch [33700/100000] Training Loss: 0.00000058 Testing Loss: 0.00000237 Validation loss: 0.00000047\n",
      "Epoch [33800/100000] Training Loss: 0.00000071 Testing Loss: 0.00000250 Validation loss: 0.00000047\n",
      "Epoch [33900/100000] Training Loss: 0.00000068 Testing Loss: 0.00000242 Validation loss: 0.00000046\n",
      "Epoch [34000/100000] Training Loss: 0.00000064 Testing Loss: 0.00000234 Validation loss: 0.00000045\n",
      "Epoch [34100/100000] Training Loss: 0.00000073 Testing Loss: 0.00000245 Validation loss: 0.00000045\n",
      "Epoch [34200/100000] Training Loss: 0.00000068 Testing Loss: 0.00000233 Validation loss: 0.00000044\n",
      "Epoch [34300/100000] Training Loss: 0.00000066 Testing Loss: 0.00000229 Validation loss: 0.00000043\n",
      "Epoch [34400/100000] Training Loss: 0.00000072 Testing Loss: 0.00000234 Validation loss: 0.00000043\n",
      "Epoch [34500/100000] Training Loss: 0.00000065 Testing Loss: 0.00000221 Validation loss: 0.00000042\n",
      "Epoch [34600/100000] Training Loss: 0.00000074 Testing Loss: 0.00000230 Validation loss: 0.00000042\n",
      "Epoch [34700/100000] Training Loss: 0.00000075 Testing Loss: 0.00000224 Validation loss: 0.00000041\n",
      "Epoch [34800/100000] Training Loss: 0.00000065 Testing Loss: 0.00000213 Validation loss: 0.00000039\n",
      "Epoch [34900/100000] Training Loss: 0.00000070 Testing Loss: 0.00000213 Validation loss: 0.00000039\n",
      "Epoch [35000/100000] Training Loss: 0.00000065 Testing Loss: 0.00000211 Validation loss: 0.00000038\n",
      "Epoch [35100/100000] Training Loss: 0.00000061 Testing Loss: 0.00000202 Validation loss: 0.00000037\n",
      "Epoch [35200/100000] Training Loss: 0.00000062 Testing Loss: 0.00000202 Validation loss: 0.00000036\n",
      "Epoch [35300/100000] Training Loss: 0.00000072 Testing Loss: 0.00000210 Validation loss: 0.00000037\n",
      "Epoch [35400/100000] Training Loss: 0.00000066 Testing Loss: 0.00000201 Validation loss: 0.00000036\n",
      "Epoch [35500/100000] Training Loss: 0.00000057 Testing Loss: 0.00000190 Validation loss: 0.00000035\n",
      "Epoch [35600/100000] Training Loss: 0.00000067 Testing Loss: 0.00000198 Validation loss: 0.00000035\n",
      "Epoch [35700/100000] Training Loss: 0.00000059 Testing Loss: 0.00000186 Validation loss: 0.00000035\n",
      "Epoch [35800/100000] Training Loss: 0.00000068 Testing Loss: 0.00000194 Validation loss: 0.00000034\n",
      "Epoch [35900/100000] Training Loss: 0.00000069 Testing Loss: 0.00000193 Validation loss: 0.00000034\n",
      "Epoch [36000/100000] Training Loss: 0.00000067 Testing Loss: 0.00000190 Validation loss: 0.00000033\n",
      "Epoch [36100/100000] Training Loss: 0.00000069 Testing Loss: 0.00000187 Validation loss: 0.00000032\n",
      "Epoch [36200/100000] Training Loss: 0.00000051 Testing Loss: 0.00000173 Validation loss: 0.00000031\n",
      "Epoch [36300/100000] Training Loss: 0.00000063 Testing Loss: 0.00000180 Validation loss: 0.00000031\n",
      "Epoch [36400/100000] Training Loss: 0.00000062 Testing Loss: 0.00000174 Validation loss: 0.00000031\n",
      "Epoch [36500/100000] Training Loss: 0.00000063 Testing Loss: 0.00000175 Validation loss: 0.00000030\n",
      "Epoch [36600/100000] Training Loss: 0.00000061 Testing Loss: 0.00000173 Validation loss: 0.00000030\n",
      "Epoch [36700/100000] Training Loss: 0.00000070 Testing Loss: 0.00000180 Validation loss: 0.00000030\n",
      "Epoch [36800/100000] Training Loss: 0.00000059 Testing Loss: 0.00000166 Validation loss: 0.00000028\n",
      "Epoch [36900/100000] Training Loss: 0.00000067 Testing Loss: 0.00000165 Validation loss: 0.00000028\n",
      "Epoch [37000/100000] Training Loss: 0.00000070 Testing Loss: 0.00000170 Validation loss: 0.00000028\n",
      "Epoch [37100/100000] Training Loss: 0.00000068 Testing Loss: 0.00000166 Validation loss: 0.00000027\n",
      "Epoch [37200/100000] Training Loss: 0.00000064 Testing Loss: 0.00000169 Validation loss: 0.00000027\n",
      "Epoch [37300/100000] Training Loss: 0.00000068 Testing Loss: 0.00000164 Validation loss: 0.00000027\n",
      "Epoch [37400/100000] Training Loss: 0.00000064 Testing Loss: 0.00000163 Validation loss: 0.00000026\n",
      "Epoch [37500/100000] Training Loss: 0.00000062 Testing Loss: 0.00000157 Validation loss: 0.00000026\n",
      "Epoch [37600/100000] Training Loss: 0.00000074 Testing Loss: 0.00000165 Validation loss: 0.00000026\n",
      "Epoch [37700/100000] Training Loss: 0.00000068 Testing Loss: 0.00000164 Validation loss: 0.00000026\n",
      "Epoch [37800/100000] Training Loss: 0.00000062 Testing Loss: 0.00000152 Validation loss: 0.00000025\n",
      "Epoch [37900/100000] Training Loss: 0.00000065 Testing Loss: 0.00000157 Validation loss: 0.00000024\n",
      "Epoch [38000/100000] Training Loss: 0.00000076 Testing Loss: 0.00000162 Validation loss: 0.00000025\n",
      "Epoch [38100/100000] Training Loss: 0.00000052 Testing Loss: 0.00000140 Validation loss: 0.00000023\n",
      "Epoch [38200/100000] Training Loss: 0.00000064 Testing Loss: 0.00000146 Validation loss: 0.00000023\n",
      "Epoch [38300/100000] Training Loss: 0.00000061 Testing Loss: 0.00000143 Validation loss: 0.00000023\n",
      "Epoch [38400/100000] Training Loss: 0.00000053 Testing Loss: 0.00000134 Validation loss: 0.00000023\n",
      "Epoch [38500/100000] Training Loss: 0.00000064 Testing Loss: 0.00000143 Validation loss: 0.00000022\n",
      "Epoch [38600/100000] Training Loss: 0.00000062 Testing Loss: 0.00000139 Validation loss: 0.00000022\n",
      "Epoch [38700/100000] Training Loss: 0.00000061 Testing Loss: 0.00000143 Validation loss: 0.00000022\n",
      "Epoch [38800/100000] Training Loss: 0.00000068 Testing Loss: 0.00000145 Validation loss: 0.00000022\n",
      "Epoch [38900/100000] Training Loss: 0.00000068 Testing Loss: 0.00000142 Validation loss: 0.00000021\n",
      "Epoch [39000/100000] Training Loss: 0.00000061 Testing Loss: 0.00000135 Validation loss: 0.00000020\n",
      "Epoch [39100/100000] Training Loss: 0.00000060 Testing Loss: 0.00000129 Validation loss: 0.00000020\n",
      "Epoch [39200/100000] Training Loss: 0.00000069 Testing Loss: 0.00000138 Validation loss: 0.00000021\n",
      "Epoch [39300/100000] Training Loss: 0.00000067 Testing Loss: 0.00000140 Validation loss: 0.00000020\n",
      "Epoch [39400/100000] Training Loss: 0.00000058 Testing Loss: 0.00000132 Validation loss: 0.00000019\n",
      "Epoch [39500/100000] Training Loss: 0.00000064 Testing Loss: 0.00000132 Validation loss: 0.00000019\n",
      "Epoch [39600/100000] Training Loss: 0.00000064 Testing Loss: 0.00000130 Validation loss: 0.00000019\n",
      "Epoch [39700/100000] Training Loss: 0.00000058 Testing Loss: 0.00000123 Validation loss: 0.00000019\n",
      "Epoch [39800/100000] Training Loss: 0.00000082 Testing Loss: 0.00000142 Validation loss: 0.00000019\n",
      "Epoch [39900/100000] Training Loss: 0.00000061 Testing Loss: 0.00000126 Validation loss: 0.00000019\n",
      "Epoch [40000/100000] Training Loss: 0.00000057 Testing Loss: 0.00000121 Validation loss: 0.00000018\n",
      "Epoch [40100/100000] Training Loss: 0.00000067 Testing Loss: 0.00000134 Validation loss: 0.00000018\n",
      "Epoch [40200/100000] Training Loss: 0.00000070 Testing Loss: 0.00000127 Validation loss: 0.00000017\n",
      "Epoch [40300/100000] Training Loss: 0.00000056 Testing Loss: 0.00000116 Validation loss: 0.00000017\n",
      "Epoch [40400/100000] Training Loss: 0.00000059 Testing Loss: 0.00000116 Validation loss: 0.00000017\n",
      "Epoch [40500/100000] Training Loss: 0.00000064 Testing Loss: 0.00000124 Validation loss: 0.00000017\n",
      "Epoch [40600/100000] Training Loss: 0.00000067 Testing Loss: 0.00000125 Validation loss: 0.00000016\n",
      "Epoch [40700/100000] Training Loss: 0.00000058 Testing Loss: 0.00000114 Validation loss: 0.00000016\n",
      "Epoch [40800/100000] Training Loss: 0.00000066 Testing Loss: 0.00000124 Validation loss: 0.00000017\n",
      "Epoch [40900/100000] Training Loss: 0.00000064 Testing Loss: 0.00000117 Validation loss: 0.00000016\n",
      "Epoch [41000/100000] Training Loss: 0.00000073 Testing Loss: 0.00000129 Validation loss: 0.00000017\n",
      "Epoch [41100/100000] Training Loss: 0.00000061 Testing Loss: 0.00000115 Validation loss: 0.00000015\n",
      "Epoch [41200/100000] Training Loss: 0.00000061 Testing Loss: 0.00000112 Validation loss: 0.00000015\n",
      "Epoch [41300/100000] Training Loss: 0.00000074 Testing Loss: 0.00000126 Validation loss: 0.00000016\n",
      "Epoch [41400/100000] Training Loss: 0.00000063 Testing Loss: 0.00000113 Validation loss: 0.00000015\n",
      "Epoch [41500/100000] Training Loss: 0.00000073 Testing Loss: 0.00000119 Validation loss: 0.00000015\n",
      "Epoch [41600/100000] Training Loss: 0.00000072 Testing Loss: 0.00000120 Validation loss: 0.00000015\n",
      "Epoch [41700/100000] Training Loss: 0.00000066 Testing Loss: 0.00000114 Validation loss: 0.00000014\n",
      "Epoch [41800/100000] Training Loss: 0.00000073 Testing Loss: 0.00000121 Validation loss: 0.00000014\n",
      "Epoch [41900/100000] Training Loss: 0.00000061 Testing Loss: 0.00000107 Validation loss: 0.00000014\n",
      "Epoch [42000/100000] Training Loss: 0.00000069 Testing Loss: 0.00000119 Validation loss: 0.00000014\n",
      "Epoch [42100/100000] Training Loss: 0.00000072 Testing Loss: 0.00000116 Validation loss: 0.00000014\n",
      "Epoch [42200/100000] Training Loss: 0.00000067 Testing Loss: 0.00000110 Validation loss: 0.00000013\n",
      "Epoch [42300/100000] Training Loss: 0.00000074 Testing Loss: 0.00000118 Validation loss: 0.00000014\n",
      "Epoch [42400/100000] Training Loss: 0.00000079 Testing Loss: 0.00000122 Validation loss: 0.00000014\n",
      "Epoch [42500/100000] Training Loss: 0.00000069 Testing Loss: 0.00000109 Validation loss: 0.00000013\n",
      "Epoch [42600/100000] Training Loss: 0.00000066 Testing Loss: 0.00000108 Validation loss: 0.00000013\n",
      "Epoch [42700/100000] Training Loss: 0.00000061 Testing Loss: 0.00000103 Validation loss: 0.00000012\n",
      "Epoch [42800/100000] Training Loss: 0.00000065 Testing Loss: 0.00000103 Validation loss: 0.00000012\n",
      "Epoch [42900/100000] Training Loss: 0.00000069 Testing Loss: 0.00000106 Validation loss: 0.00000012\n",
      "Epoch [43000/100000] Training Loss: 0.00000064 Testing Loss: 0.00000100 Validation loss: 0.00000012\n",
      "Epoch [43100/100000] Training Loss: 0.00000066 Testing Loss: 0.00000103 Validation loss: 0.00000012\n",
      "Epoch [43200/100000] Training Loss: 0.00000067 Testing Loss: 0.00000106 Validation loss: 0.00000012\n",
      "Epoch [43300/100000] Training Loss: 0.00000069 Testing Loss: 0.00000104 Validation loss: 0.00000012\n",
      "Epoch [43400/100000] Training Loss: 0.00000067 Testing Loss: 0.00000100 Validation loss: 0.00000011\n",
      "Epoch [43500/100000] Training Loss: 0.00000065 Testing Loss: 0.00000101 Validation loss: 0.00000012\n",
      "Epoch [43600/100000] Training Loss: 0.00000076 Testing Loss: 0.00000117 Validation loss: 0.00000012\n",
      "Epoch [43700/100000] Training Loss: 0.00000069 Testing Loss: 0.00000105 Validation loss: 0.00000011\n",
      "Epoch [43800/100000] Training Loss: 0.00000053 Testing Loss: 0.00000088 Validation loss: 0.00000011\n",
      "Epoch [43900/100000] Training Loss: 0.00000068 Testing Loss: 0.00000101 Validation loss: 0.00000011\n",
      "Epoch [44000/100000] Training Loss: 0.00000071 Testing Loss: 0.00000102 Validation loss: 0.00000011\n",
      "Epoch [44100/100000] Training Loss: 0.00000064 Testing Loss: 0.00000094 Validation loss: 0.00000011\n",
      "Epoch [44200/100000] Training Loss: 0.00000070 Testing Loss: 0.00000102 Validation loss: 0.00000011\n",
      "Epoch [44300/100000] Training Loss: 0.00000073 Testing Loss: 0.00000105 Validation loss: 0.00000011\n",
      "Epoch [44400/100000] Training Loss: 0.00000067 Testing Loss: 0.00000100 Validation loss: 0.00000011\n",
      "Epoch [44500/100000] Training Loss: 0.00000058 Testing Loss: 0.00000091 Validation loss: 0.00000010\n",
      "Epoch [44600/100000] Training Loss: 0.00000066 Testing Loss: 0.00000096 Validation loss: 0.00000010\n",
      "Epoch [44700/100000] Training Loss: 0.00000067 Testing Loss: 0.00000094 Validation loss: 0.00000010\n",
      "Epoch [44800/100000] Training Loss: 0.00000071 Testing Loss: 0.00000095 Validation loss: 0.00000010\n",
      "Epoch [44900/100000] Training Loss: 0.00000058 Testing Loss: 0.00000086 Validation loss: 0.00000009\n",
      "Epoch [45000/100000] Training Loss: 0.00000052 Testing Loss: 0.00000083 Validation loss: 0.00000010\n",
      "Epoch [45100/100000] Training Loss: 0.00000072 Testing Loss: 0.00000096 Validation loss: 0.00000010\n",
      "Epoch [45200/100000] Training Loss: 0.00000067 Testing Loss: 0.00000094 Validation loss: 0.00000010\n",
      "Epoch [45300/100000] Training Loss: 0.00000062 Testing Loss: 0.00000087 Validation loss: 0.00000009\n",
      "Epoch [45400/100000] Training Loss: 0.00000068 Testing Loss: 0.00000090 Validation loss: 0.00000009\n",
      "Epoch [45500/100000] Training Loss: 0.00000066 Testing Loss: 0.00000091 Validation loss: 0.00000009\n",
      "Epoch [45600/100000] Training Loss: 0.00000061 Testing Loss: 0.00000085 Validation loss: 0.00000009\n",
      "Epoch [45700/100000] Training Loss: 0.00000065 Testing Loss: 0.00000092 Validation loss: 0.00000009\n",
      "Epoch [45800/100000] Training Loss: 0.00000067 Testing Loss: 0.00000092 Validation loss: 0.00000009\n",
      "Epoch [45900/100000] Training Loss: 0.00000058 Testing Loss: 0.00000077 Validation loss: 0.00000008\n",
      "Epoch [46000/100000] Training Loss: 0.00000070 Testing Loss: 0.00000095 Validation loss: 0.00000009\n",
      "Epoch [46100/100000] Training Loss: 0.00000075 Testing Loss: 0.00000097 Validation loss: 0.00000009\n",
      "Epoch [46200/100000] Training Loss: 0.00000063 Testing Loss: 0.00000083 Validation loss: 0.00000008\n",
      "Epoch [46300/100000] Training Loss: 0.00000055 Testing Loss: 0.00000077 Validation loss: 0.00000008\n",
      "Epoch [46400/100000] Training Loss: 0.00000067 Testing Loss: 0.00000092 Validation loss: 0.00000008\n",
      "Epoch [46500/100000] Training Loss: 0.00000061 Testing Loss: 0.00000081 Validation loss: 0.00000008\n",
      "Epoch [46600/100000] Training Loss: 0.00000063 Testing Loss: 0.00000082 Validation loss: 0.00000008\n",
      "Epoch [46700/100000] Training Loss: 0.00000061 Testing Loss: 0.00000083 Validation loss: 0.00000008\n",
      "Epoch [46800/100000] Training Loss: 0.00000060 Testing Loss: 0.00000080 Validation loss: 0.00000008\n",
      "Epoch [46900/100000] Training Loss: 0.00000065 Testing Loss: 0.00000084 Validation loss: 0.00000008\n",
      "Epoch [47000/100000] Training Loss: 0.00000066 Testing Loss: 0.00000084 Validation loss: 0.00000008\n",
      "Epoch [47100/100000] Training Loss: 0.00000068 Testing Loss: 0.00000090 Validation loss: 0.00000008\n",
      "Epoch [47200/100000] Training Loss: 0.00000066 Testing Loss: 0.00000084 Validation loss: 0.00000008\n",
      "Epoch [47300/100000] Training Loss: 0.00000067 Testing Loss: 0.00000087 Validation loss: 0.00000008\n",
      "Epoch [47400/100000] Training Loss: 0.00000067 Testing Loss: 0.00000087 Validation loss: 0.00000008\n",
      "Epoch [47500/100000] Training Loss: 0.00000062 Testing Loss: 0.00000081 Validation loss: 0.00000007\n",
      "Epoch [47600/100000] Training Loss: 0.00000068 Testing Loss: 0.00000085 Validation loss: 0.00000008\n",
      "Epoch [47700/100000] Training Loss: 0.00000060 Testing Loss: 0.00000079 Validation loss: 0.00000007\n",
      "Epoch [47800/100000] Training Loss: 0.00000067 Testing Loss: 0.00000084 Validation loss: 0.00000007\n",
      "Epoch [47900/100000] Training Loss: 0.00000067 Testing Loss: 0.00000086 Validation loss: 0.00000008\n",
      "Epoch [48000/100000] Training Loss: 0.00000065 Testing Loss: 0.00000084 Validation loss: 0.00000007\n",
      "Epoch [48100/100000] Training Loss: 0.00000068 Testing Loss: 0.00000085 Validation loss: 0.00000007\n",
      "Epoch [48200/100000] Training Loss: 0.00000071 Testing Loss: 0.00000087 Validation loss: 0.00000007\n",
      "Epoch [48300/100000] Training Loss: 0.00000067 Testing Loss: 0.00000083 Validation loss: 0.00000007\n",
      "Epoch [48400/100000] Training Loss: 0.00000069 Testing Loss: 0.00000081 Validation loss: 0.00000007\n",
      "Epoch [48500/100000] Training Loss: 0.00000067 Testing Loss: 0.00000083 Validation loss: 0.00000007\n",
      "Epoch [48600/100000] Training Loss: 0.00000060 Testing Loss: 0.00000078 Validation loss: 0.00000007\n",
      "Epoch [48700/100000] Training Loss: 0.00000058 Testing Loss: 0.00000075 Validation loss: 0.00000006\n",
      "Epoch [48800/100000] Training Loss: 0.00000071 Testing Loss: 0.00000086 Validation loss: 0.00000007\n",
      "Epoch [48900/100000] Training Loss: 0.00000064 Testing Loss: 0.00000083 Validation loss: 0.00000007\n",
      "Epoch [49000/100000] Training Loss: 0.00000060 Testing Loss: 0.00000076 Validation loss: 0.00000007\n",
      "Epoch [49100/100000] Training Loss: 0.00000066 Testing Loss: 0.00000078 Validation loss: 0.00000006\n",
      "Epoch [49200/100000] Training Loss: 0.00000062 Testing Loss: 0.00000080 Validation loss: 0.00000006\n",
      "Epoch 49254: reducing learning rate of group 0 to 1.3841e-05.\n",
      "Epoch [49300/100000] Training Loss: 0.00000001 Testing Loss: 0.00000015 Validation loss: 0.00000003\n",
      "Epoch [49400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000014 Validation loss: 0.00000003\n",
      "Epoch [49500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000014 Validation loss: 0.00000003\n",
      "Epoch [49600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000014 Validation loss: 0.00000003\n",
      "Epoch [49700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000014 Validation loss: 0.00000003\n",
      "Epoch [49800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000013 Validation loss: 0.00000003\n",
      "Epoch [49900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000013 Validation loss: 0.00000003\n",
      "Epoch [50000/100000] Training Loss: 0.00000001 Testing Loss: 0.00000014 Validation loss: 0.00000003\n",
      "Epoch [50100/100000] Training Loss: 0.00000032 Testing Loss: 0.00000044 Validation loss: 0.00000004\n",
      "Epoch [50200/100000] Training Loss: 0.00000028 Testing Loss: 0.00000041 Validation loss: 0.00000004\n",
      "Epoch [50300/100000] Training Loss: 0.00000024 Testing Loss: 0.00000036 Validation loss: 0.00000004\n",
      "Epoch [50400/100000] Training Loss: 0.00000034 Testing Loss: 0.00000045 Validation loss: 0.00000004\n",
      "Epoch [50500/100000] Training Loss: 0.00000034 Testing Loss: 0.00000046 Validation loss: 0.00000004\n",
      "Epoch [50600/100000] Training Loss: 0.00000031 Testing Loss: 0.00000042 Validation loss: 0.00000004\n",
      "Epoch [50700/100000] Training Loss: 0.00000034 Testing Loss: 0.00000047 Validation loss: 0.00000004\n",
      "Epoch [50800/100000] Training Loss: 0.00000028 Testing Loss: 0.00000040 Validation loss: 0.00000004\n",
      "Epoch [50900/100000] Training Loss: 0.00000030 Testing Loss: 0.00000041 Validation loss: 0.00000004\n",
      "Epoch [51000/100000] Training Loss: 0.00000029 Testing Loss: 0.00000041 Validation loss: 0.00000004\n",
      "Epoch [51100/100000] Training Loss: 0.00000033 Testing Loss: 0.00000043 Validation loss: 0.00000004\n",
      "Epoch [51200/100000] Training Loss: 0.00000030 Testing Loss: 0.00000040 Validation loss: 0.00000004\n",
      "Epoch [51300/100000] Training Loss: 0.00000030 Testing Loss: 0.00000040 Validation loss: 0.00000004\n",
      "Epoch [51400/100000] Training Loss: 0.00000028 Testing Loss: 0.00000040 Validation loss: 0.00000004\n",
      "Epoch 51491: reducing learning rate of group 0 to 9.6889e-06.\n",
      "Epoch [51500/100000] Training Loss: 0.00000008 Testing Loss: 0.00000014 Validation loss: 0.00000002\n",
      "Epoch [51600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000010 Validation loss: 0.00000002\n",
      "Epoch [51700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000010 Validation loss: 0.00000002\n",
      "Epoch [51800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000010 Validation loss: 0.00000002\n",
      "Epoch [51900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000010 Validation loss: 0.00000002\n",
      "Epoch [52000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000010 Validation loss: 0.00000002\n",
      "Epoch [52100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000009 Validation loss: 0.00000002\n",
      "Epoch [52200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000009 Validation loss: 0.00000002\n",
      "Epoch [52300/100000] Training Loss: 0.00000015 Testing Loss: 0.00000023 Validation loss: 0.00000003\n",
      "Epoch [52400/100000] Training Loss: 0.00000014 Testing Loss: 0.00000023 Validation loss: 0.00000003\n",
      "Epoch [52500/100000] Training Loss: 0.00000015 Testing Loss: 0.00000023 Validation loss: 0.00000003\n",
      "Epoch [52600/100000] Training Loss: 0.00000015 Testing Loss: 0.00000023 Validation loss: 0.00000003\n",
      "Epoch [52700/100000] Training Loss: 0.00000015 Testing Loss: 0.00000024 Validation loss: 0.00000003\n",
      "Epoch [52800/100000] Training Loss: 0.00000016 Testing Loss: 0.00000024 Validation loss: 0.00000003\n",
      "Epoch [52900/100000] Training Loss: 0.00000014 Testing Loss: 0.00000022 Validation loss: 0.00000003\n",
      "Epoch [53000/100000] Training Loss: 0.00000016 Testing Loss: 0.00000024 Validation loss: 0.00000003\n",
      "Epoch [53100/100000] Training Loss: 0.00000014 Testing Loss: 0.00000022 Validation loss: 0.00000003\n",
      "Epoch [53200/100000] Training Loss: 0.00000013 Testing Loss: 0.00000022 Validation loss: 0.00000002\n",
      "Epoch [53300/100000] Training Loss: 0.00000013 Testing Loss: 0.00000021 Validation loss: 0.00000002\n",
      "Epoch [53400/100000] Training Loss: 0.00000015 Testing Loss: 0.00000023 Validation loss: 0.00000003\n",
      "Epoch [53500/100000] Training Loss: 0.00000013 Testing Loss: 0.00000021 Validation loss: 0.00000002\n",
      "Epoch [53600/100000] Training Loss: 0.00000015 Testing Loss: 0.00000022 Validation loss: 0.00000002\n",
      "Epoch [53700/100000] Training Loss: 0.00000015 Testing Loss: 0.00000022 Validation loss: 0.00000002\n",
      "Epoch 53737: reducing learning rate of group 0 to 6.7822e-06.\n",
      "Epoch [53800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000007 Validation loss: 0.00000002\n",
      "Epoch [53900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000007 Validation loss: 0.00000002\n",
      "Epoch [54000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000007 Validation loss: 0.00000002\n",
      "Epoch [54100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000007 Validation loss: 0.00000002\n",
      "Epoch [54200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000007 Validation loss: 0.00000002\n",
      "Epoch [54300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000007 Validation loss: 0.00000002\n",
      "Epoch [54400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [54500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [54600/100000] Training Loss: 0.00000006 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [54700/100000] Training Loss: 0.00000006 Testing Loss: 0.00000013 Validation loss: 0.00000002\n",
      "Epoch [54800/100000] Training Loss: 0.00000006 Testing Loss: 0.00000013 Validation loss: 0.00000002\n",
      "Epoch [54900/100000] Training Loss: 0.00000006 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [55000/100000] Training Loss: 0.00000006 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [55100/100000] Training Loss: 0.00000006 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [55200/100000] Training Loss: 0.00000006 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [55300/100000] Training Loss: 0.00000007 Testing Loss: 0.00000013 Validation loss: 0.00000002\n",
      "Epoch [55400/100000] Training Loss: 0.00000007 Testing Loss: 0.00000013 Validation loss: 0.00000002\n",
      "Epoch [55500/100000] Training Loss: 0.00000006 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [55600/100000] Training Loss: 0.00000007 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [55700/100000] Training Loss: 0.00000007 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [55800/100000] Training Loss: 0.00000006 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [55900/100000] Training Loss: 0.00000007 Testing Loss: 0.00000012 Validation loss: 0.00000002\n",
      "Epoch [56000/100000] Training Loss: 0.00000006 Testing Loss: 0.00000011 Validation loss: 0.00000001\n",
      "Epoch 56014: reducing learning rate of group 0 to 4.7476e-06.\n",
      "Epoch [56100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000005 Validation loss: 0.00000001\n",
      "Epoch [56200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000005 Validation loss: 0.00000001\n",
      "Epoch [56300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000005 Validation loss: 0.00000001\n",
      "Epoch [56400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000005 Validation loss: 0.00000001\n",
      "Epoch [56500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000005 Validation loss: 0.00000001\n",
      "Epoch [56600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000005 Validation loss: 0.00000001\n",
      "Epoch [56700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000005 Validation loss: 0.00000001\n",
      "Epoch [56800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000005 Validation loss: 0.00000001\n",
      "Epoch [56900/100000] Training Loss: 0.00000002 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [57000/100000] Training Loss: 0.00000003 Testing Loss: 0.00000008 Validation loss: 0.00000001\n",
      "Epoch [57100/100000] Training Loss: 0.00000003 Testing Loss: 0.00000008 Validation loss: 0.00000001\n",
      "Epoch [57200/100000] Training Loss: 0.00000002 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [57300/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [57400/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [57500/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [57600/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [57700/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [57800/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [57900/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [58000/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [58100/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [58200/100000] Training Loss: 0.00000003 Testing Loss: 0.00000007 Validation loss: 0.00000001\n",
      "Epoch [58300/100000] Training Loss: 0.00000003 Testing Loss: 0.00000006 Validation loss: 0.00000001\n",
      "Epoch 58328: reducing learning rate of group 0 to 3.3233e-06.\n",
      "Epoch [58400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [58500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [58600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [58700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [58800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [58900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [59000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [59100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [59200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [59300/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [59400/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [59500/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [59600/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [59700/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [59800/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [59900/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [60000/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [60100/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [60200/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [60300/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [60400/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [60500/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [60600/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch [60700/100000] Training Loss: 0.00000001 Testing Loss: 0.00000004 Validation loss: 0.00000001\n",
      "Epoch 60703: reducing learning rate of group 0 to 2.3263e-06.\n",
      "Epoch [60800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [60900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [61000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [61100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [61200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000001\n",
      "Epoch [61300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000001\n",
      "Epoch [61400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000001\n",
      "Epoch [61500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000001\n",
      "Epoch [61600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000001\n",
      "Epoch [61700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [61800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [61900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [62000/100000] Training Loss: 0.00000001 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [62100/100000] Training Loss: 0.00000001 Testing Loss: 0.00000003 Validation loss: 0.00000001\n",
      "Epoch [62200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000000\n",
      "Epoch [62300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000003 Validation loss: 0.00000000\n",
      "Epoch [62400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [62500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [62600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [62700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [62800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [62900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63400/100000] Training Loss: 0.00000001 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63800/100000] Training Loss: 0.00000001 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [63900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64000/100000] Training Loss: 0.00000001 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [64900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [65900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [66000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [66100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [66200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [66300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [66400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [66500/100000] Training Loss: 0.00000001 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [66600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000002 Validation loss: 0.00000000\n",
      "Epoch [66700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [66800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [66900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [67900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68400/100000] Training Loss: 0.00000001 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68600/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68700/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68800/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [68900/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [69000/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [69100/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [69200/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [69300/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [69400/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Epoch [69500/100000] Training Loss: 0.00000000 Testing Loss: 0.00000001 Validation loss: 0.00000000\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "net = GreenFun(2 * m2).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 1500, factor=0.7, verbose=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 5000, verbose=False, delta=1e-8, path='net.pth')\n",
    "num_epochs = 100000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = net(train_f)\n",
    "    \n",
    "    loss = criterion(output, train_u)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_test = net(test_f)\n",
    "        loss_test = criterion(output_test, test_u)\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            output_val = net(val_f)\n",
    "            loss_val = criterion(output_val, val_u)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {loss.item():.8f} Testing Loss: {loss_test.item():.8f} Validation loss: {loss_val.item():.8f}\")\n",
    "\n",
    "    # 调整学习率\n",
    "    scheduler.step(loss_test)\n",
    "\n",
    "    early_stopping(loss_test, net)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net.pth\", map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_exact = np.sin(np.pi * X) * Y * (1 - Y)\n",
    "u2_exact = X * (1 - X) * np.sin(np.pi * Y)\n",
    "laplace_u1 = -2 * np.sin(np.pi * X) - np.pi ** 2 * u1_exact\n",
    "laplace_u2 = -2 * np.sin(np.pi * Y) - np.pi ** 2 * u2_exact\n",
    "f1 = u1_exact - lambda_ * laplace_u2\n",
    "f2 = lambda_ * laplace_u1 + u2_exact\n",
    "\n",
    "f = np.concatenate([f1[1:-1, 1:-1].flatten(), f2[1:-1, 1:-1].flatten()])\n",
    "\n",
    "f_torch = torch.tensor(f, dtype=torch.float32).view(1, -1).to(device)\n",
    "\n",
    "u_numerical_torch = net(f_torch)\n",
    "u_numerical = u_numerical_torch.cpu().detach().numpy().flatten()\n",
    "\n",
    "u1_numerical = np.zeros_like(u1_exact)\n",
    "u2_numerical = np.zeros_like(u2_exact)\n",
    "u1_numerical[1:-1, 1:-1] = u_numerical[0: m2].reshape((m, m))\n",
    "u2_numerical[1:-1, 1:-1] = u_numerical[m2: 2 * m2].reshape((m, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeErrors(u_exact, u_pre, printOrNot):\n",
    "    error = u_exact - u_pre\n",
    "    l2_norm_abs = np.linalg.norm(error, ord=2) / np.sqrt(error.size)\n",
    "    max_norm_abs = np.max(np.abs(error))\n",
    "    l2_norm_rel = np.linalg.norm(error, ord=2) / np.linalg.norm(u_exact, ord=2)\n",
    "    max_norm_rel = np.max(np.abs(error)) / np.max(np.abs(u_exact))  \n",
    "    \n",
    "    l2_norm_rel_percent = l2_norm_rel * 100\n",
    "    max_norm_rel_percent = max_norm_rel * 100\n",
    "    \n",
    "    if printOrNot == True:\n",
    "        print(f\"Absolute L2 Norm Error: {l2_norm_abs:.8f}\")\n",
    "        print(f\"Absolute Max Norm Error: {max_norm_abs:.8f}\")\n",
    "        print(f\"Relative L2 Norm Error: {l2_norm_rel_percent:.6f}%\")\n",
    "        print(f\"Relative Max Norm Error: {max_norm_rel_percent:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute L2 Norm Error: 0.00002744\n",
      "Absolute Max Norm Error: 0.00022687\n",
      "Relative L2 Norm Error: 0.021787%\n",
      "Relative Max Norm Error: 0.090747%\n"
     ]
    }
   ],
   "source": [
    "computeErrors(u1_exact, u1_numerical, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute L2 Norm Error: 0.00001731\n",
      "Absolute Max Norm Error: 0.00021974\n",
      "Relative L2 Norm Error: 0.013743%\n",
      "Relative Max Norm Error: 0.087896%\n"
     ]
    }
   ],
   "source": [
    "computeErrors(u2_exact, u2_numerical, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
