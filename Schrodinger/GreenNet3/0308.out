GPU: NVIDIA GeForce RTX 3080 Ti
GPU memory: 12038.19 MB
Epoch [100/200000] Training Loss: 0.00246562 Testing Loss: 0.00202393 || lr: [0.001]
Epoch [200/200000] Training Loss: 0.00006148 Testing Loss: 0.00007204 || lr: [0.001]
Epoch [300/200000] Training Loss: 0.00003540 Testing Loss: 0.00003528 || lr: [0.001]
Epoch [400/200000] Training Loss: 0.00003881 Testing Loss: 0.00003561 || lr: [0.001]
Epoch [500/200000] Training Loss: 0.00003775 Testing Loss: 0.00004760 || lr: [0.001]
Epoch [600/200000] Training Loss: 0.00007807 Testing Loss: 0.00004806 || lr: [0.001]
Epoch [700/200000] Training Loss: 0.00003513 Testing Loss: 0.00003340 || lr: [0.001]
Epoch [800/200000] Training Loss: 0.00004256 Testing Loss: 0.00004585 || lr: [0.001]
Epoch [900/200000] Training Loss: 0.00006534 Testing Loss: 0.00003876 || lr: [0.001]
Epoch [1000/200000] Training Loss: 0.00003120 Testing Loss: 0.00003274 || lr: [0.001]
Epoch [1100/200000] Training Loss: 0.00003406 Testing Loss: 0.00003393 || lr: [0.001]
Epoch [1200/200000] Training Loss: 0.00005739 Testing Loss: 0.00007221 || lr: [0.001]
Epoch [1300/200000] Training Loss: 0.00002819 Testing Loss: 0.00002831 || lr: [0.001]
Epoch [1400/200000] Training Loss: 0.00004911 Testing Loss: 0.00003072 || lr: [0.001]
Epoch [1500/200000] Training Loss: 0.00002569 Testing Loss: 0.00002586 || lr: [0.001]
Epoch [1600/200000] Training Loss: 0.00002598 Testing Loss: 0.00002702 || lr: [0.001]
Epoch [1700/200000] Training Loss: 0.00002375 Testing Loss: 0.00002391 || lr: [0.001]
Epoch [1800/200000] Training Loss: 0.00002526 Testing Loss: 0.00002553 || lr: [0.001]
Epoch [1900/200000] Training Loss: 0.00002301 Testing Loss: 0.00002314 || lr: [0.001]
Epoch [2000/200000] Training Loss: 0.00010026 Testing Loss: 0.00023761 || lr: [0.001]
Epoch [2100/200000] Training Loss: 0.00002292 Testing Loss: 0.00002307 || lr: [0.001]
Epoch [2200/200000] Training Loss: 0.00002197 Testing Loss: 0.00002205 || lr: [0.001]
Epoch [2300/200000] Training Loss: 0.00002393 Testing Loss: 0.00002296 || lr: [0.001]
Epoch [2400/200000] Training Loss: 0.00004520 Testing Loss: 0.00007035 || lr: [0.001]
Epoch [2500/200000] Training Loss: 0.00002122 Testing Loss: 0.00002141 || lr: [0.001]
Epoch [2600/200000] Training Loss: 0.00002219 Testing Loss: 0.00002683 || lr: [0.001]
Epoch [2700/200000] Training Loss: 0.00001943 Testing Loss: 0.00001953 || lr: [0.001]
Epoch [2800/200000] Training Loss: 0.00001968 Testing Loss: 0.00001961 || lr: [0.001]
Epoch [2900/200000] Training Loss: 0.00003308 Testing Loss: 0.00004728 || lr: [0.001]
Epoch [3000/200000] Training Loss: 0.00001763 Testing Loss: 0.00001821 || lr: [0.001]
Epoch [3100/200000] Training Loss: 0.00015774 Testing Loss: 0.00009159 || lr: [0.001]
Epoch [3200/200000] Training Loss: 0.00001597 Testing Loss: 0.00001616 || lr: [0.001]
Epoch [3300/200000] Training Loss: 0.00002021 Testing Loss: 0.00001834 || lr: [0.001]
Epoch [3400/200000] Training Loss: 0.00001421 Testing Loss: 0.00001439 || lr: [0.001]
Epoch [3500/200000] Training Loss: 0.00002405 Testing Loss: 0.00001974 || lr: [0.001]
Epoch [3600/200000] Training Loss: 0.00001286 Testing Loss: 0.00001309 || lr: [0.001]
Epoch [3700/200000] Training Loss: 0.00001246 Testing Loss: 0.00001251 || lr: [0.001]
Epoch [3800/200000] Training Loss: 0.00001300 Testing Loss: 0.00001375 || lr: [0.001]
Epoch [3900/200000] Training Loss: 0.00001141 Testing Loss: 0.00001153 || lr: [0.001]
Epoch [4000/200000] Training Loss: 0.00076588 Testing Loss: 0.00084338 || lr: [0.001]
Epoch [4100/200000] Training Loss: 0.00001348 Testing Loss: 0.00001367 || lr: [0.001]
Epoch [4200/200000] Training Loss: 0.00001140 Testing Loss: 0.00001154 || lr: [0.001]
Epoch [4300/200000] Training Loss: 0.00001041 Testing Loss: 0.00001054 || lr: [0.001]
Epoch [4400/200000] Training Loss: 0.00001066 Testing Loss: 0.00001056 || lr: [0.001]
Epoch [4500/200000] Training Loss: 0.00000922 Testing Loss: 0.00000934 || lr: [0.001]
Epoch [4600/200000] Training Loss: 0.00001094 Testing Loss: 0.00001078 || lr: [0.001]
Epoch [4700/200000] Training Loss: 0.00003963 Testing Loss: 0.00001831 || lr: [0.001]
Epoch [4800/200000] Training Loss: 0.00000817 Testing Loss: 0.00000831 || lr: [0.001]
Epoch [4900/200000] Training Loss: 0.00000760 Testing Loss: 0.00000773 || lr: [0.001]
Epoch [5000/200000] Training Loss: 0.00000877 Testing Loss: 0.00000872 || lr: [0.001]
Epoch [5100/200000] Training Loss: 0.00000735 Testing Loss: 0.00000756 || lr: [0.001]
Epoch [5200/200000] Training Loss: 0.00000686 Testing Loss: 0.00000704 || lr: [0.001]
Epoch [5300/200000] Training Loss: 0.00000827 Testing Loss: 0.00000797 || lr: [0.001]
Epoch [5400/200000] Training Loss: 0.00000667 Testing Loss: 0.00000679 || lr: [0.001]
Epoch [5500/200000] Training Loss: 0.00000632 Testing Loss: 0.00000643 || lr: [0.001]
Epoch [5600/200000] Training Loss: 0.00001303 Testing Loss: 0.00001055 || lr: [0.001]
Epoch [5700/200000] Training Loss: 0.00000621 Testing Loss: 0.00000633 || lr: [0.001]
Epoch [5800/200000] Training Loss: 0.00000589 Testing Loss: 0.00000600 || lr: [0.001]
Epoch [5900/200000] Training Loss: 0.00000637 Testing Loss: 0.00000686 || lr: [0.001]
Epoch [6000/200000] Training Loss: 0.00000566 Testing Loss: 0.00000576 || lr: [0.001]
Epoch [6100/200000] Training Loss: 0.00000542 Testing Loss: 0.00000552 || lr: [0.001]
Epoch [6200/200000] Training Loss: 0.00001574 Testing Loss: 0.00001151 || lr: [0.001]
Epoch [6300/200000] Training Loss: 0.00000535 Testing Loss: 0.00000545 || lr: [0.001]
Epoch [6400/200000] Training Loss: 0.00000507 Testing Loss: 0.00000516 || lr: [0.001]
Epoch [6500/200000] Training Loss: 0.00000565 Testing Loss: 0.00000557 || lr: [0.001]
Epoch [6600/200000] Training Loss: 0.00000486 Testing Loss: 0.00000496 || lr: [0.001]
Epoch [6700/200000] Training Loss: 0.00003262 Testing Loss: 0.00004759 || lr: [0.001]
Epoch [6800/200000] Training Loss: 0.00000479 Testing Loss: 0.00000488 || lr: [0.001]
Epoch [6900/200000] Training Loss: 0.00000452 Testing Loss: 0.00000461 || lr: [0.001]
Epoch [7000/200000] Training Loss: 0.00014463 Testing Loss: 0.00006496 || lr: [0.001]
Epoch [7100/200000] Training Loss: 0.00000460 Testing Loss: 0.00000470 || lr: [0.001]
Epoch [7200/200000] Training Loss: 0.00000430 Testing Loss: 0.00000439 || lr: [0.001]
Epoch [7300/200000] Training Loss: 0.00001541 Testing Loss: 0.00001492 || lr: [0.001]
Epoch [7400/200000] Training Loss: 0.00000419 Testing Loss: 0.00000428 || lr: [0.001]
Epoch [7500/200000] Training Loss: 0.00000398 Testing Loss: 0.00000407 || lr: [0.001]
Epoch [7600/200000] Training Loss: 0.00005977 Testing Loss: 0.00005136 || lr: [0.001]
Epoch [7700/200000] Training Loss: 0.00000397 Testing Loss: 0.00000406 || lr: [0.001]
Epoch [7800/200000] Training Loss: 0.00000373 Testing Loss: 0.00000381 || lr: [0.001]
Epoch [7900/200000] Training Loss: 0.00006959 Testing Loss: 0.00015971 || lr: [0.001]
Epoch [8000/200000] Training Loss: 0.00000373 Testing Loss: 0.00000382 || lr: [0.001]
Epoch [8100/200000] Training Loss: 0.00000348 Testing Loss: 0.00000356 || lr: [0.001]
Epoch [8200/200000] Training Loss: 0.00000796 Testing Loss: 0.00001091 || lr: [0.001]
Epoch [8300/200000] Training Loss: 0.00000338 Testing Loss: 0.00000346 || lr: [0.001]
Epoch [8400/200000] Training Loss: 0.00000324 Testing Loss: 0.00000332 || lr: [0.001]
Epoch [8500/200000] Training Loss: 0.00001867 Testing Loss: 0.00001018 || lr: [0.001]
Epoch [8600/200000] Training Loss: 0.00000323 Testing Loss: 0.00000331 || lr: [0.001]
Epoch [8700/200000] Training Loss: 0.00000305 Testing Loss: 0.00000313 || lr: [0.001]
Epoch [8800/200000] Training Loss: 0.00000358 Testing Loss: 0.00000392 || lr: [0.001]
Epoch [8900/200000] Training Loss: 0.00000294 Testing Loss: 0.00000301 || lr: [0.001]
Epoch [9000/200000] Training Loss: 0.00000283 Testing Loss: 0.00000290 || lr: [0.001]
Epoch [9100/200000] Training Loss: 0.00000340 Testing Loss: 0.00000349 || lr: [0.001]
Epoch [9200/200000] Training Loss: 0.00000283 Testing Loss: 0.00000291 || lr: [0.001]
Epoch [9300/200000] Training Loss: 0.00027499 Testing Loss: 0.00056008 || lr: [0.001]
Epoch [9400/200000] Training Loss: 0.00000359 Testing Loss: 0.00000366 || lr: [0.001]
Epoch [9500/200000] Training Loss: 0.00000293 Testing Loss: 0.00000301 || lr: [0.001]
Epoch [9600/200000] Training Loss: 0.00000273 Testing Loss: 0.00000281 || lr: [0.001]
Epoch [9700/200000] Training Loss: 0.00000261 Testing Loss: 0.00000269 || lr: [0.001]
Epoch [9800/200000] Training Loss: 0.00000255 Testing Loss: 0.00000263 || lr: [0.001]
Epoch [9900/200000] Training Loss: 0.00000245 Testing Loss: 0.00000253 || lr: [0.001]
Epoch [10000/200000] Training Loss: 0.00001642 Testing Loss: 0.00002527 || lr: [0.001]
Epoch [10100/200000] Training Loss: 0.00000268 Testing Loss: 0.00000275 || lr: [0.001]
Epoch [10200/200000] Training Loss: 0.00000248 Testing Loss: 0.00000256 || lr: [0.001]
Epoch [10300/200000] Training Loss: 0.00000267 Testing Loss: 0.00000277 || lr: [0.001]
Epoch [10400/200000] Training Loss: 0.00000234 Testing Loss: 0.00000242 || lr: [0.001]
Epoch [10500/200000] Training Loss: 0.00004482 Testing Loss: 0.00006119 || lr: [0.001]
Epoch [10600/200000] Training Loss: 0.00000240 Testing Loss: 0.00000248 || lr: [0.001]
Epoch [10700/200000] Training Loss: 0.00002450 Testing Loss: 0.00000788 || lr: [0.001]
Epoch [10800/200000] Training Loss: 0.00000224 Testing Loss: 0.00000231 || lr: [0.001]
Epoch [10900/200000] Training Loss: 0.00000214 Testing Loss: 0.00000221 || lr: [0.001]
Epoch [11000/200000] Training Loss: 0.00000608 Testing Loss: 0.00000930 || lr: [0.001]
Epoch [11100/200000] Training Loss: 0.00000212 Testing Loss: 0.00000219 || lr: [0.001]
Epoch [11200/200000] Training Loss: 0.00005125 Testing Loss: 0.00001135 || lr: [0.001]
Epoch [11300/200000] Training Loss: 0.00000208 Testing Loss: 0.00000215 || lr: [0.001]
Epoch [11400/200000] Training Loss: 0.00001622 Testing Loss: 0.00008739 || lr: [0.001]
Epoch [11500/200000] Training Loss: 0.00000199 Testing Loss: 0.00000206 || lr: [0.001]
Epoch [11600/200000] Training Loss: 0.00000189 Testing Loss: 0.00000196 || lr: [0.001]
Epoch [11700/200000] Training Loss: 0.00000183 Testing Loss: 0.00000189 || lr: [0.001]
Epoch [11800/200000] Training Loss: 0.00000330 Testing Loss: 0.00000315 || lr: [0.001]
Epoch [11900/200000] Training Loss: 0.00000186 Testing Loss: 0.00000194 || lr: [0.001]
Epoch [12000/200000] Training Loss: 0.00000180 Testing Loss: 0.00000186 || lr: [0.001]
Epoch [12100/200000] Training Loss: 0.00000171 Testing Loss: 0.00000178 || lr: [0.001]
Epoch [12200/200000] Training Loss: 0.00000795 Testing Loss: 0.00000798 || lr: [0.001]
Epoch [12300/200000] Training Loss: 0.00000183 Testing Loss: 0.00000189 || lr: [0.001]
Epoch [12400/200000] Training Loss: 0.00000208 Testing Loss: 0.00000220 || lr: [0.001]
Epoch [12500/200000] Training Loss: 0.00000237 Testing Loss: 0.00000237 || lr: [0.001]
Epoch [12600/200000] Training Loss: 0.00000161 Testing Loss: 0.00000167 || lr: [0.001]
Epoch [12700/200000] Training Loss: 0.00000195 Testing Loss: 0.00000202 || lr: [0.001]
Epoch [12800/200000] Training Loss: 0.00000153 Testing Loss: 0.00000159 || lr: [0.001]
Epoch [12900/200000] Training Loss: 0.00000166 Testing Loss: 0.00000173 || lr: [0.001]
Epoch [13000/200000] Training Loss: 0.00001072 Testing Loss: 0.00001017 || lr: [0.001]
Epoch [13100/200000] Training Loss: 0.00000210 Testing Loss: 0.00000216 || lr: [0.001]
Epoch [13200/200000] Training Loss: 0.00000297 Testing Loss: 0.00000389 || lr: [0.001]
Epoch [13300/200000] Training Loss: 0.00000172 Testing Loss: 0.00000178 || lr: [0.001]
Epoch [13400/200000] Training Loss: 0.00000159 Testing Loss: 0.00000166 || lr: [0.001]
Epoch [13500/200000] Training Loss: 0.00000430 Testing Loss: 0.00000677 || lr: [0.001]
Epoch [13600/200000] Training Loss: 0.00000149 Testing Loss: 0.00000155 || lr: [0.001]
Epoch [13700/200000] Training Loss: 0.00000141 Testing Loss: 0.00000147 || lr: [0.001]
Epoch [13800/200000] Training Loss: 0.00000147 Testing Loss: 0.00000155 || lr: [0.001]
Epoch [13900/200000] Training Loss: 0.00000135 Testing Loss: 0.00000140 || lr: [0.001]
Epoch [14000/200000] Training Loss: 0.00000547 Testing Loss: 0.00000332 || lr: [0.001]
Epoch [14100/200000] Training Loss: 0.00000128 Testing Loss: 0.00000133 || lr: [0.001]
Epoch [14200/200000] Training Loss: 0.00000123 Testing Loss: 0.00000129 || lr: [0.001]
Epoch [14300/200000] Training Loss: 0.00000161 Testing Loss: 0.00000166 || lr: [0.001]
Epoch [14400/200000] Training Loss: 0.00000163 Testing Loss: 0.00000187 || lr: [0.001]
Epoch [14500/200000] Training Loss: 0.00000122 Testing Loss: 0.00000128 || lr: [0.001]
Epoch [14600/200000] Training Loss: 0.00000142 Testing Loss: 0.00000161 || lr: [0.001]
Epoch [14700/200000] Training Loss: 0.00000135 Testing Loss: 0.00000139 || lr: [0.001]
Epoch [14800/200000] Training Loss: 0.00000116 Testing Loss: 0.00000121 || lr: [0.001]
Epoch [14900/200000] Training Loss: 0.00000111 Testing Loss: 0.00000116 || lr: [0.001]
Epoch [15000/200000] Training Loss: 0.00000108 Testing Loss: 0.00000113 || lr: [0.001]
Epoch [15100/200000] Training Loss: 0.00000106 Testing Loss: 0.00000110 || lr: [0.001]
Epoch [15200/200000] Training Loss: 0.00000104 Testing Loss: 0.00000108 || lr: [0.001]
Epoch [15300/200000] Training Loss: 0.00000191 Testing Loss: 0.00000193 || lr: [0.001]
Epoch [15400/200000] Training Loss: 0.00000128 Testing Loss: 0.00000132 || lr: [0.001]
Epoch [15500/200000] Training Loss: 0.00000121 Testing Loss: 0.00000126 || lr: [0.001]
Epoch [15600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000118 || lr: [0.001]
Epoch [15700/200000] Training Loss: 0.00000108 Testing Loss: 0.00000112 || lr: [0.001]
Epoch [15800/200000] Training Loss: 0.00001634 Testing Loss: 0.00001086 || lr: [0.001]
Epoch [15900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119 || lr: [0.001]
Epoch [16000/200000] Training Loss: 0.00000104 Testing Loss: 0.00000109 || lr: [0.001]
Epoch [16100/200000] Training Loss: 0.00000100 Testing Loss: 0.00000104 || lr: [0.001]
Epoch [16200/200000] Training Loss: 0.00000095 Testing Loss: 0.00000099 || lr: [0.001]
Epoch [16300/200000] Training Loss: 0.00000320 Testing Loss: 0.00000301 || lr: [0.001]
Epoch [16400/200000] Training Loss: 0.00000094 Testing Loss: 0.00000098 || lr: [0.001]
Epoch [16500/200000] Training Loss: 0.00000747 Testing Loss: 0.00001012 || lr: [0.001]
Epoch [16600/200000] Training Loss: 0.00000098 Testing Loss: 0.00000101 || lr: [0.001]
Epoch [16700/200000] Training Loss: 0.00000112 Testing Loss: 0.00000122 || lr: [0.001]
Epoch [16800/200000] Training Loss: 0.00003284 Testing Loss: 0.00002990 || lr: [0.001]
Epoch [16900/200000] Training Loss: 0.00000100 Testing Loss: 0.00000104 || lr: [0.001]
Epoch [17000/200000] Training Loss: 0.00000091 Testing Loss: 0.00000095 || lr: [0.001]
Epoch [17100/200000] Training Loss: 0.00000087 Testing Loss: 0.00000090 || lr: [0.001]
Epoch [17200/200000] Training Loss: 0.00000085 Testing Loss: 0.00000089 || lr: [0.001]
Epoch [17300/200000] Training Loss: 0.00000135 Testing Loss: 0.00000194 || lr: [0.001]
Epoch [17400/200000] Training Loss: 0.00000084 Testing Loss: 0.00000088 || lr: [0.001]
Epoch [17500/200000] Training Loss: 0.00000089 Testing Loss: 0.00000097 || lr: [0.001]
Epoch [17600/200000] Training Loss: 0.00000223 Testing Loss: 0.00000179 || lr: [0.001]
Epoch [17700/200000] Training Loss: 0.00001128 Testing Loss: 0.00002433 || lr: [0.001]
Epoch [17800/200000] Training Loss: 0.00000083 Testing Loss: 0.00000086 || lr: [0.001]
Epoch [17900/200000] Training Loss: 0.00000081 Testing Loss: 0.00000087 || lr: [0.001]
Epoch [18000/200000] Training Loss: 0.00000122 Testing Loss: 0.00000134 || lr: [0.001]
Epoch [18100/200000] Training Loss: 0.00000404 Testing Loss: 0.00000496 || lr: [0.001]
Epoch [18200/200000] Training Loss: 0.00000280 Testing Loss: 0.00000352 || lr: [0.001]
Epoch [18300/200000] Training Loss: 0.00000121 Testing Loss: 0.00000122 || lr: [0.001]
Epoch [18400/200000] Training Loss: 0.00000086 Testing Loss: 0.00000090 || lr: [0.001]
Epoch [18500/200000] Training Loss: 0.00000085 Testing Loss: 0.00000092 || lr: [0.001]
Epoch [18600/200000] Training Loss: 0.00000079 Testing Loss: 0.00000084 || lr: [0.001]
Epoch [18700/200000] Training Loss: 0.00002970 Testing Loss: 0.00002458 || lr: [0.001]
Epoch [18800/200000] Training Loss: 0.00000084 Testing Loss: 0.00000088 || lr: [0.001]
Epoch [18900/200000] Training Loss: 0.00000078 Testing Loss: 0.00000082 || lr: [0.001]
Epoch [19000/200000] Training Loss: 0.00000076 Testing Loss: 0.00000081 || lr: [0.001]
Epoch [19100/200000] Training Loss: 0.00000116 Testing Loss: 0.00000169 || lr: [0.001]
Epoch [19200/200000] Training Loss: 0.00000072 Testing Loss: 0.00000075 || lr: [0.001]
Epoch [19300/200000] Training Loss: 0.00000091 Testing Loss: 0.00000083 || lr: [0.001]
Epoch [19400/200000] Training Loss: 0.00000348 Testing Loss: 0.00000476 || lr: [0.001]
Epoch [19500/200000] Training Loss: 0.00000072 Testing Loss: 0.00000076 || lr: [0.001]
Epoch [19600/200000] Training Loss: 0.00000180 Testing Loss: 0.00000203 || lr: [0.001]
Epoch [19700/200000] Training Loss: 0.00000069 Testing Loss: 0.00000073 || lr: [0.001]
Epoch [19800/200000] Training Loss: 0.00000069 Testing Loss: 0.00000073 || lr: [0.001]
Epoch [19900/200000] Training Loss: 0.00000080 Testing Loss: 0.00000082 || lr: [0.001]
Epoch [20000/200000] Training Loss: 0.00000068 Testing Loss: 0.00000071 || lr: [0.001]
Epoch [20100/200000] Training Loss: 0.00000067 Testing Loss: 0.00000070 || lr: [0.001]
Epoch [20200/200000] Training Loss: 0.00000152 Testing Loss: 0.00000150 || lr: [0.001]
Epoch [20300/200000] Training Loss: 0.00002946 Testing Loss: 0.00005613 || lr: [0.001]
Epoch [20400/200000] Training Loss: 0.00000071 Testing Loss: 0.00000074 || lr: [0.001]
Epoch [20500/200000] Training Loss: 0.00000065 Testing Loss: 0.00000068 || lr: [0.001]
Epoch [20600/200000] Training Loss: 0.00000063 Testing Loss: 0.00000066 || lr: [0.001]
Epoch [20700/200000] Training Loss: 0.00000063 Testing Loss: 0.00000066 || lr: [0.001]
Epoch [20800/200000] Training Loss: 0.00000076 Testing Loss: 0.00000081 || lr: [0.001]
Epoch [20900/200000] Training Loss: 0.00003172 Testing Loss: 0.00003583 || lr: [0.001]
Epoch [21000/200000] Training Loss: 0.00000071 Testing Loss: 0.00000074 || lr: [0.001]
Epoch [21100/200000] Training Loss: 0.00000064 Testing Loss: 0.00000067 || lr: [0.001]
Epoch [21200/200000] Training Loss: 0.00000062 Testing Loss: 0.00000065 || lr: [0.001]
Epoch [21300/200000] Training Loss: 0.00000061 Testing Loss: 0.00000064 || lr: [0.001]
Epoch [21400/200000] Training Loss: 0.00000595 Testing Loss: 0.00000561 || lr: [0.001]
Epoch [21500/200000] Training Loss: 0.00000066 Testing Loss: 0.00000069 || lr: [0.001]
Epoch [21600/200000] Training Loss: 0.00000061 Testing Loss: 0.00000064 || lr: [0.001]
Epoch [21700/200000] Training Loss: 0.00000062 Testing Loss: 0.00000066 || lr: [0.001]
Epoch [21800/200000] Training Loss: 0.00000059 Testing Loss: 0.00000061 || lr: [0.001]
Epoch [21900/200000] Training Loss: 0.00000058 Testing Loss: 0.00000061 || lr: [0.001]
Epoch [22000/200000] Training Loss: 0.00000314 Testing Loss: 0.00000284 || lr: [0.001]
Epoch [22100/200000] Training Loss: 0.00000101 Testing Loss: 0.00000104 || lr: [0.001]
Epoch [22200/200000] Training Loss: 0.00000080 Testing Loss: 0.00000083 || lr: [0.001]
Epoch [22300/200000] Training Loss: 0.00000072 Testing Loss: 0.00000078 || lr: [0.001]
Epoch [22400/200000] Training Loss: 0.00000069 Testing Loss: 0.00000076 || lr: [0.001]
Epoch [22500/200000] Training Loss: 0.00000061 Testing Loss: 0.00000064 || lr: [0.001]
Epoch [22600/200000] Training Loss: 0.00000060 Testing Loss: 0.00000062 || lr: [0.001]
Epoch [22700/200000] Training Loss: 0.00000060 Testing Loss: 0.00000064 || lr: [0.001]
Epoch [22800/200000] Training Loss: 0.00000106 Testing Loss: 0.00000101 || lr: [0.001]
Epoch [22900/200000] Training Loss: 0.00000290 Testing Loss: 0.00000412 || lr: [0.001]
Epoch [23000/200000] Training Loss: 0.00000094 Testing Loss: 0.00000098 || lr: [0.001]
Epoch [23100/200000] Training Loss: 0.00000067 Testing Loss: 0.00000070 || lr: [0.001]
Epoch [23200/200000] Training Loss: 0.00000060 Testing Loss: 0.00000063 || lr: [0.001]
Epoch [23300/200000] Training Loss: 0.00000065 Testing Loss: 0.00000067 || lr: [0.001]
Epoch [23400/200000] Training Loss: 0.00000057 Testing Loss: 0.00000060 || lr: [0.001]
Epoch [23500/200000] Training Loss: 0.00000078 Testing Loss: 0.00000079 || lr: [0.001]
Epoch [23600/200000] Training Loss: 0.00000059 Testing Loss: 0.00000061 || lr: [0.001]
Epoch [23700/200000] Training Loss: 0.00000056 Testing Loss: 0.00000059 || lr: [0.001]
Epoch [23800/200000] Training Loss: 0.00000055 Testing Loss: 0.00000058 || lr: [0.001]
Epoch [23900/200000] Training Loss: 0.00000126 Testing Loss: 0.00000138 || lr: [0.001]
Epoch [24000/200000] Training Loss: 0.00000057 Testing Loss: 0.00000060 || lr: [0.001]
Epoch [24100/200000] Training Loss: 0.00000055 Testing Loss: 0.00000057 || lr: [0.001]
Epoch [24200/200000] Training Loss: 0.00000054 Testing Loss: 0.00000057 || lr: [0.001]
Epoch [24300/200000] Training Loss: 0.00000053 Testing Loss: 0.00000056 || lr: [0.001]
Epoch [24400/200000] Training Loss: 0.00000055 Testing Loss: 0.00000057 || lr: [0.001]
Epoch [24500/200000] Training Loss: 0.00000126 Testing Loss: 0.00000147 || lr: [0.001]
Epoch [24600/200000] Training Loss: 0.00010211 Testing Loss: 0.00006184 || lr: [0.001]
Epoch [24700/200000] Training Loss: 0.00000119 Testing Loss: 0.00000101 || lr: [0.001]
Epoch [24800/200000] Training Loss: 0.00000183 Testing Loss: 0.00000109 || lr: [0.001]
Epoch [24900/200000] Training Loss: 0.00000060 Testing Loss: 0.00000063 || lr: [0.001]
Epoch [25000/200000] Training Loss: 0.00000074 Testing Loss: 0.00000087 || lr: [0.001]
Epoch [25100/200000] Training Loss: 0.00000094 Testing Loss: 0.00000110 || lr: [0.001]
Epoch [25200/200000] Training Loss: 0.00000053 Testing Loss: 0.00000056 || lr: [0.001]
Epoch [25300/200000] Training Loss: 0.00000383 Testing Loss: 0.00000292 || lr: [0.001]
Epoch [25400/200000] Training Loss: 0.00000237 Testing Loss: 0.00000326 || lr: [0.001]
Epoch [25500/200000] Training Loss: 0.00000083 Testing Loss: 0.00000085 || lr: [0.001]
Epoch [25600/200000] Training Loss: 0.00000058 Testing Loss: 0.00000061 || lr: [0.001]
Epoch [25700/200000] Training Loss: 0.00000055 Testing Loss: 0.00000058 || lr: [0.001]
Epoch [25800/200000] Training Loss: 0.00000725 Testing Loss: 0.00001078 || lr: [0.001]
Epoch [25900/200000] Training Loss: 0.00000053 Testing Loss: 0.00000055 || lr: [0.001]
Epoch [26000/200000] Training Loss: 0.00000051 Testing Loss: 0.00000054 || lr: [0.001]
Epoch [26100/200000] Training Loss: 0.00000053 Testing Loss: 0.00000056 || lr: [0.001]
Epoch [26200/200000] Training Loss: 0.00004621 Testing Loss: 0.00004700 || lr: [0.001]
Epoch [26300/200000] Training Loss: 0.00000091 Testing Loss: 0.00000098 || lr: [0.001]
Epoch [26400/200000] Training Loss: 0.00000066 Testing Loss: 0.00000069 || lr: [0.001]
Epoch [26500/200000] Training Loss: 0.00000055 Testing Loss: 0.00000058 || lr: [0.001]
Epoch [26600/200000] Training Loss: 0.00000053 Testing Loss: 0.00000056 || lr: [0.001]
Epoch [26700/200000] Training Loss: 0.00000053 Testing Loss: 0.00000055 || lr: [0.001]
Epoch [26800/200000] Training Loss: 0.00000050 Testing Loss: 0.00000053 || lr: [0.001]
Epoch [26900/200000] Training Loss: 0.00000142 Testing Loss: 0.00000117 || lr: [0.001]
Epoch [27000/200000] Training Loss: 0.00000050 Testing Loss: 0.00000052 || lr: [0.001]
Epoch [27100/200000] Training Loss: 0.00000049 Testing Loss: 0.00000051 || lr: [0.001]
Epoch [27200/200000] Training Loss: 0.00000049 Testing Loss: 0.00000051 || lr: [0.001]
Epoch [27300/200000] Training Loss: 0.00000048 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [27400/200000] Training Loss: 0.00000071 Testing Loss: 0.00000081 || lr: [0.001]
Epoch [27500/200000] Training Loss: 0.00000048 Testing Loss: 0.00000051 || lr: [0.001]
Epoch [27600/200000] Training Loss: 0.00000047 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [27700/200000] Training Loss: 0.00000063 Testing Loss: 0.00000076 || lr: [0.001]
Epoch [27800/200000] Training Loss: 0.00000057 Testing Loss: 0.00000059 || lr: [0.001]
Epoch [27900/200000] Training Loss: 0.00000233 Testing Loss: 0.00000236 || lr: [0.001]
Epoch [28000/200000] Training Loss: 0.00000048 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [28100/200000] Training Loss: 0.00000048 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [28200/200000] Training Loss: 0.00000049 Testing Loss: 0.00000052 || lr: [0.001]
Epoch [28300/200000] Training Loss: 0.00000048 Testing Loss: 0.00000051 || lr: [0.001]
Epoch [28400/200000] Training Loss: 0.00000046 Testing Loss: 0.00000049 || lr: [0.001]
Epoch [28500/200000] Training Loss: 0.00000190 Testing Loss: 0.00000188 || lr: [0.001]
Epoch [28600/200000] Training Loss: 0.00000050 Testing Loss: 0.00000053 || lr: [0.001]
Epoch [28700/200000] Training Loss: 0.00000047 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [28800/200000] Training Loss: 0.00000053 Testing Loss: 0.00000057 || lr: [0.001]
Epoch [28900/200000] Training Loss: 0.00000046 Testing Loss: 0.00000048 || lr: [0.001]
Epoch [29000/200000] Training Loss: 0.00000045 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [29100/200000] Training Loss: 0.00000206 Testing Loss: 0.00000191 || lr: [0.001]
Epoch [29200/200000] Training Loss: 0.00000049 Testing Loss: 0.00000052 || lr: [0.001]
Epoch [29300/200000] Training Loss: 0.00000046 Testing Loss: 0.00000049 || lr: [0.001]
Epoch [29400/200000] Training Loss: 0.00000194 Testing Loss: 0.00000235 || lr: [0.001]
Epoch [29500/200000] Training Loss: 0.00000044 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [29600/200000] Training Loss: 0.00000052 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [29700/200000] Training Loss: 0.00000069 Testing Loss: 0.00000107 || lr: [0.001]
Epoch [29800/200000] Training Loss: 0.00000068 Testing Loss: 0.00000062 || lr: [0.001]
Epoch [29900/200000] Training Loss: 0.00000101 Testing Loss: 0.00000099 || lr: [0.001]
Epoch [30000/200000] Training Loss: 0.00000060 Testing Loss: 0.00000062 || lr: [0.001]
Epoch [30100/200000] Training Loss: 0.00000092 Testing Loss: 0.00000077 || lr: [0.001]
Epoch [30200/200000] Training Loss: 0.00000046 Testing Loss: 0.00000049 || lr: [0.001]
Epoch [30300/200000] Training Loss: 0.00000045 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [30400/200000] Training Loss: 0.00000047 Testing Loss: 0.00000049 || lr: [0.001]
Epoch [30500/200000] Training Loss: 0.00000225 Testing Loss: 0.00000222 || lr: [0.001]
Epoch [30600/200000] Training Loss: 0.00000052 Testing Loss: 0.00000055 || lr: [0.001]
Epoch [30700/200000] Training Loss: 0.00000046 Testing Loss: 0.00000048 || lr: [0.001]
Epoch [30800/200000] Training Loss: 0.00000045 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [30900/200000] Training Loss: 0.00000044 Testing Loss: 0.00000046 || lr: [0.001]
Epoch [31000/200000] Training Loss: 0.00000044 Testing Loss: 0.00000046 || lr: [0.001]
Epoch [31100/200000] Training Loss: 0.00000043 Testing Loss: 0.00000045 || lr: [0.001]
Epoch [31200/200000] Training Loss: 0.00000200 Testing Loss: 0.00000249 || lr: [0.001]
Epoch [31300/200000] Training Loss: 0.00000047 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [31400/200000] Training Loss: 0.00000043 Testing Loss: 0.00000046 || lr: [0.001]
Epoch [31500/200000] Training Loss: 0.00000043 Testing Loss: 0.00000045 || lr: [0.001]
Epoch [31600/200000] Training Loss: 0.00000042 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [31700/200000] Training Loss: 0.00000180 Testing Loss: 0.00000192 || lr: [0.001]
Epoch [31800/200000] Training Loss: 0.00000042 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [31900/200000] Training Loss: 0.00000041 Testing Loss: 0.00000043 || lr: [0.001]
Epoch [32000/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.001]
Epoch [32100/200000] Training Loss: 0.00000043 Testing Loss: 0.00000045 || lr: [0.001]
Epoch [32200/200000] Training Loss: 0.00000048 Testing Loss: 0.00000049 || lr: [0.001]
Epoch [32300/200000] Training Loss: 0.00000402 Testing Loss: 0.00000560 || lr: [0.001]
Epoch [32400/200000] Training Loss: 0.00000098 Testing Loss: 0.00000100 || lr: [0.001]
Epoch [32500/200000] Training Loss: 0.00000060 Testing Loss: 0.00000064 || lr: [0.001]
Epoch [32600/200000] Training Loss: 0.00000053 Testing Loss: 0.00000056 || lr: [0.001]
Epoch [32700/200000] Training Loss: 0.00000070 Testing Loss: 0.00000078 || lr: [0.001]
Epoch [32800/200000] Training Loss: 0.00000050 Testing Loss: 0.00000053 || lr: [0.001]
Epoch [32900/200000] Training Loss: 0.00000051 Testing Loss: 0.00000053 || lr: [0.001]
Epoch [33000/200000] Training Loss: 0.00000055 Testing Loss: 0.00000060 || lr: [0.001]
Epoch [33100/200000] Training Loss: 0.00000050 Testing Loss: 0.00000058 || lr: [0.001]
Epoch [33200/200000] Training Loss: 0.00000308 Testing Loss: 0.00000291 || lr: [0.001]
Epoch [33300/200000] Training Loss: 0.00000050 Testing Loss: 0.00000053 || lr: [0.001]
Epoch [33400/200000] Training Loss: 0.00000061 Testing Loss: 0.00000130 || lr: [0.001]
Epoch [33500/200000] Training Loss: 0.00000043 Testing Loss: 0.00000045 || lr: [0.001]
Epoch [33600/200000] Training Loss: 0.00000042 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [33700/200000] Training Loss: 0.00000070 Testing Loss: 0.00000069 || lr: [0.001]
Epoch [33800/200000] Training Loss: 0.00000042 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [33900/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [34000/200000] Training Loss: 0.00000204 Testing Loss: 0.00000155 || lr: [0.001]
Epoch [34100/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [34200/200000] Training Loss: 0.00000041 Testing Loss: 0.00000043 || lr: [0.001]
Epoch [34300/200000] Training Loss: 0.00001543 Testing Loss: 0.00001157 || lr: [0.001]
Epoch [34400/200000] Training Loss: 0.00000047 Testing Loss: 0.00000049 || lr: [0.001]
Epoch [34500/200000] Training Loss: 0.00000043 Testing Loss: 0.00000045 || lr: [0.001]
Epoch [34600/200000] Training Loss: 0.00000042 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [34700/200000] Training Loss: 0.00000079 Testing Loss: 0.00000228 || lr: [0.001]
Epoch [34800/200000] Training Loss: 0.00000041 Testing Loss: 0.00000043 || lr: [0.001]
Epoch [34900/200000] Training Loss: 0.00000042 Testing Loss: 0.00000046 || lr: [0.001]
Epoch [35000/200000] Training Loss: 0.00000385 Testing Loss: 0.00000382 || lr: [0.001]
Epoch [35100/200000] Training Loss: 0.00000044 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [35200/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [35300/200000] Training Loss: 0.00000040 Testing Loss: 0.00000043 || lr: [0.001]
Epoch [35400/200000] Training Loss: 0.00000040 Testing Loss: 0.00000042 || lr: [0.001]
Epoch [35500/200000] Training Loss: 0.00000282 Testing Loss: 0.00000304 || lr: [0.001]
Epoch [35600/200000] Training Loss: 0.00000072 Testing Loss: 0.00000087 || lr: [0.001]
Epoch [35700/200000] Training Loss: 0.00000058 Testing Loss: 0.00000058 || lr: [0.001]
Epoch [35800/200000] Training Loss: 0.00000156 Testing Loss: 0.00000155 || lr: [0.001]
Epoch [35900/200000] Training Loss: 0.00000062 Testing Loss: 0.00000065 || lr: [0.001]
Epoch [36000/200000] Training Loss: 0.00000052 Testing Loss: 0.00000055 || lr: [0.001]
Epoch [36100/200000] Training Loss: 0.00000094 Testing Loss: 0.00000068 || lr: [0.001]
Epoch [36200/200000] Training Loss: 0.00000043 Testing Loss: 0.00000046 || lr: [0.001]
Epoch [36300/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.001]
Epoch [36400/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [36500/200000] Training Loss: 0.00000052 Testing Loss: 0.00000060 || lr: [0.001]
Epoch [36600/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.001]
Epoch [36700/200000] Training Loss: 0.00000040 Testing Loss: 0.00000043 || lr: [0.001]
Epoch [36800/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.001]
Epoch [36900/200000] Training Loss: 0.00000123 Testing Loss: 0.00000111 || lr: [0.001]
Epoch [37000/200000] Training Loss: 0.00000051 Testing Loss: 0.00000055 || lr: [0.001]
Epoch [37100/200000] Training Loss: 0.00000055 Testing Loss: 0.00000060 || lr: [0.001]
Epoch [37200/200000] Training Loss: 0.00000047 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [37300/200000] Training Loss: 0.00000087 Testing Loss: 0.00000088 || lr: [0.001]
Epoch [37400/200000] Training Loss: 0.00000085 Testing Loss: 0.00000099 || lr: [0.001]
Epoch [37500/200000] Training Loss: 0.00000045 Testing Loss: 0.00000048 || lr: [0.001]
Epoch [37600/200000] Training Loss: 0.00000050 Testing Loss: 0.00000053 || lr: [0.001]
Epoch [37700/200000] Training Loss: 0.00002002 Testing Loss: 0.00001475 || lr: [0.001]
Epoch [37800/200000] Training Loss: 0.00000053 Testing Loss: 0.00000056 || lr: [0.001]
Epoch [37900/200000] Training Loss: 0.00000046 Testing Loss: 0.00000050 || lr: [0.001]
Epoch [38000/200000] Training Loss: 0.00000045 Testing Loss: 0.00000048 || lr: [0.001]
Epoch [38100/200000] Training Loss: 0.00000044 Testing Loss: 0.00000048 || lr: [0.001]
Epoch [38200/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [38300/200000] Training Loss: 0.00000117 Testing Loss: 0.00000111 || lr: [0.001]
Epoch [38400/200000] Training Loss: 0.00000054 Testing Loss: 0.00000063 || lr: [0.001]
Epoch [38500/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [38600/200000] Training Loss: 0.00000153 Testing Loss: 0.00000150 || lr: [0.001]
Epoch [38700/200000] Training Loss: 0.00000047 Testing Loss: 0.00000051 || lr: [0.001]
Epoch [38800/200000] Training Loss: 0.00000044 Testing Loss: 0.00000048 || lr: [0.001]
Epoch [38900/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [39000/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [39100/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [39200/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [39300/200000] Training Loss: 0.00000434 Testing Loss: 0.00000386 || lr: [0.001]
Epoch [39400/200000] Training Loss: 0.00000051 Testing Loss: 0.00000056 || lr: [0.001]
Epoch [39500/200000] Training Loss: 0.00000045 Testing Loss: 0.00000049 || lr: [0.001]
Epoch [39600/200000] Training Loss: 0.00000044 Testing Loss: 0.00000048 || lr: [0.001]
Epoch [39700/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [39800/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [39900/200000] Training Loss: 0.00000042 Testing Loss: 0.00000046 || lr: [0.001]
Epoch [40000/200000] Training Loss: 0.00000043 Testing Loss: 0.00000047 || lr: [0.001]
Epoch [40100/200000] Training Loss: 0.00000043 Testing Loss: 0.00000046 || lr: [0.0007]
Epoch [40200/200000] Training Loss: 0.00000038 Testing Loss: 0.00000040 || lr: [0.0007]
Epoch [40300/200000] Training Loss: 0.00000037 Testing Loss: 0.00000039 || lr: [0.0007]
Epoch [40400/200000] Training Loss: 0.00000037 Testing Loss: 0.00000039 || lr: [0.0007]
Epoch [40500/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [40600/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.0007]
Epoch [40700/200000] Training Loss: 0.00000038 Testing Loss: 0.00000039 || lr: [0.0007]
Epoch [40800/200000] Training Loss: 0.00000037 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [40900/200000] Training Loss: 0.00001018 Testing Loss: 0.00002915 || lr: [0.0007]
Epoch [41000/200000] Training Loss: 0.00000039 Testing Loss: 0.00000041 || lr: [0.0007]
Epoch [41100/200000] Training Loss: 0.00000037 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [41200/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [41300/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [41400/200000] Training Loss: 0.00000037 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [41500/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.0007]
Epoch [41600/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.0007]
Epoch [41700/200000] Training Loss: 0.00000287 Testing Loss: 0.00000269 || lr: [0.0007]
Epoch [41800/200000] Training Loss: 0.00000046 Testing Loss: 0.00000049 || lr: [0.0007]
Epoch [41900/200000] Training Loss: 0.00000043 Testing Loss: 0.00000046 || lr: [0.0007]
Epoch [42000/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.0007]
Epoch [42100/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.0007]
Epoch [42200/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.0007]
Epoch [42300/200000] Training Loss: 0.00000041 Testing Loss: 0.00000045 || lr: [0.0007]
Epoch [42400/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.0007]
Epoch [42500/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.0007]
Epoch [42600/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.0007]
Epoch [42700/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.0007]
Epoch [42800/200000] Training Loss: 0.00000069 Testing Loss: 0.00000075 || lr: [0.0007]
Epoch [42900/200000] Training Loss: 0.00000046 Testing Loss: 0.00000049 || lr: [0.0007]
Epoch [43000/200000] Training Loss: 0.00000043 Testing Loss: 0.00000046 || lr: [0.0007]
Epoch [43100/200000] Training Loss: 0.00000121 Testing Loss: 0.00000156 || lr: [0.0007]
Epoch [43200/200000] Training Loss: 0.00000046 Testing Loss: 0.00000049 || lr: [0.0007]
Epoch [43300/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.0007]
Epoch [43400/200000] Training Loss: 0.00000041 Testing Loss: 0.00000045 || lr: [0.0007]
Epoch [43500/200000] Training Loss: 0.00000041 Testing Loss: 0.00000045 || lr: [0.0007]
Epoch [43600/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.0007]
Epoch [43700/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.0007]
Epoch [43800/200000] Training Loss: 0.00000040 Testing Loss: 0.00000044 || lr: [0.0007]
Epoch [43900/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.0007]
Epoch [44000/200000] Training Loss: 0.00000044 Testing Loss: 0.00000048 || lr: [0.0007]
Epoch [44100/200000] Training Loss: 0.00000044 Testing Loss: 0.00000046 || lr: [0.0007]
Epoch [44200/200000] Training Loss: 0.00000148 Testing Loss: 0.00000077 || lr: [0.0007]
Epoch [44300/200000] Training Loss: 0.00000051 Testing Loss: 0.00000059 || lr: [0.0007]
Epoch [44400/200000] Training Loss: 0.00000042 Testing Loss: 0.00000046 || lr: [0.0007]
Epoch [44500/200000] Training Loss: 0.00000073 Testing Loss: 0.00000093 || lr: [0.0007]
Epoch [44600/200000] Training Loss: 0.00025846 Testing Loss: 0.00027990 || lr: [0.0007]
Epoch [44700/200000] Training Loss: 0.00000046 Testing Loss: 0.00000049 || lr: [0.0007]
Epoch [44800/200000] Training Loss: 0.00000038 Testing Loss: 0.00000041 || lr: [0.0007]
Epoch [44900/200000] Training Loss: 0.00000037 Testing Loss: 0.00000039 || lr: [0.0007]
Epoch [45000/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.0007]
Epoch [45100/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [45200/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [45300/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [45400/200000] Training Loss: 0.00000035 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [45500/200000] Training Loss: 0.00000193 Testing Loss: 0.00000238 || lr: [0.0007]
Epoch [45600/200000] Training Loss: 0.00000035 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [45700/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [45800/200000] Training Loss: 0.00000035 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [45900/200000] Training Loss: 0.00000035 Testing Loss: 0.00000038 || lr: [0.0007]
Epoch [46000/200000] Training Loss: 0.00000074 Testing Loss: 0.00000050 || lr: [0.0007]
Epoch [46100/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.0007]
Epoch [46200/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [46300/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [46400/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [46500/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [46600/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [46700/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [46800/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [46900/200000] Training Loss: 0.00000169 Testing Loss: 0.00000231 || lr: [0.00049]
Epoch [47000/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [47100/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [47200/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [47300/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [47400/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [47500/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [47600/200000] Training Loss: 0.00000042 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [47700/200000] Training Loss: 0.00000151 Testing Loss: 0.00000201 || lr: [0.00049]
Epoch [47800/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [47900/200000] Training Loss: 0.00000042 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [48000/200000] Training Loss: 0.00000036 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [48100/200000] Training Loss: 0.00000048 Testing Loss: 0.00000052 || lr: [0.00049]
Epoch [48200/200000] Training Loss: 0.00000182 Testing Loss: 0.00000194 || lr: [0.00049]
Epoch [48300/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [48400/200000] Training Loss: 0.00000395 Testing Loss: 0.00000280 || lr: [0.00049]
Epoch [48500/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [48600/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [48700/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [48800/200000] Training Loss: 0.00000093 Testing Loss: 0.00000057 || lr: [0.00049]
Epoch [48900/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [49000/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [49100/200000] Training Loss: 0.00000041 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [49200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [49300/200000] Training Loss: 0.00000237 Testing Loss: 0.00000263 || lr: [0.00049]
Epoch [49400/200000] Training Loss: 0.00000047 Testing Loss: 0.00000052 || lr: [0.00049]
Epoch [49500/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [49600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [49700/200000] Training Loss: 0.00000039 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [49800/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [49900/200000] Training Loss: 0.00000043 Testing Loss: 0.00000053 || lr: [0.00049]
Epoch [50000/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [50100/200000] Training Loss: 0.00000044 Testing Loss: 0.00000044 || lr: [0.00049]
Epoch [50200/200000] Training Loss: 0.00000040 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [50300/200000] Training Loss: 0.00000086 Testing Loss: 0.00000054 || lr: [0.00049]
Epoch [50400/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [50500/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [50600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [50700/200000] Training Loss: 0.00000051 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [50800/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [50900/200000] Training Loss: 0.00000060 Testing Loss: 0.00000072 || lr: [0.00049]
Epoch [51000/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [51100/200000] Training Loss: 0.00000040 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [51200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [51300/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [51400/200000] Training Loss: 0.00000038 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [51500/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [51600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [51700/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [51800/200000] Training Loss: 0.00000056 Testing Loss: 0.00000065 || lr: [0.00049]
Epoch [51900/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [52000/200000] Training Loss: 0.00000035 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [52100/200000] Training Loss: 0.00000040 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [52200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [52300/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [52400/200000] Training Loss: 0.00000037 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [52500/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [52600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [52700/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [52800/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [52900/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [53000/200000] Training Loss: 0.00000047 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [53100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [53200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [53300/200000] Training Loss: 0.00000036 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [53400/200000] Training Loss: 0.00000037 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [53500/200000] Training Loss: 0.00000056 Testing Loss: 0.00000064 || lr: [0.00049]
Epoch [53600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [53700/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [53800/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [53900/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [54000/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [54100/200000] Training Loss: 0.00000045 Testing Loss: 0.00000047 || lr: [0.00049]
Epoch [54200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [54300/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [54400/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [54500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [54600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [54700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [54800/200000] Training Loss: 0.00000036 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [54900/200000] Training Loss: 0.00000036 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [55000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [55100/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [55200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [55300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [55400/200000] Training Loss: 0.00000183 Testing Loss: 0.00000139 || lr: [0.00049]
Epoch [55500/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [55600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [55700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [55800/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [55900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [56000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [56100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [56200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [56300/200000] Training Loss: 0.00000039 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [56400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [56500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [56600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [56700/200000] Training Loss: 0.00000044 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [56800/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [56900/200000] Training Loss: 0.00000045 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [57000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [57100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [57200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [57300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [57400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [57500/200000] Training Loss: 0.00000036 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [57600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [57700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [57800/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [57900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [58000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [58100/200000] Training Loss: 0.00000039 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [58200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [58300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [58400/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [58500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [58600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [58700/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [58800/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [58900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [59000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [59100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [59200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [59300/200000] Training Loss: 0.00000126 Testing Loss: 0.00000165 || lr: [0.00049]
Epoch [59400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [59500/200000] Training Loss: 0.00000059 Testing Loss: 0.00000047 || lr: [0.00049]
Epoch [59600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [59700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [59800/200000] Training Loss: 0.00000037 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [59900/200000] Training Loss: 0.00000035 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [60000/200000] Training Loss: 0.00000040 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [60100/200000] Training Loss: 0.00000038 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [60200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [60300/200000] Training Loss: 0.00000048 Testing Loss: 0.00000054 || lr: [0.00049]
Epoch [60400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [60500/200000] Training Loss: 0.00000039 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [60600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [60700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [60800/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [60900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [61000/200000] Training Loss: 0.00000040 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [61100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [61200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [61300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [61400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [61500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [61600/200000] Training Loss: 0.00000043 Testing Loss: 0.00000054 || lr: [0.00049]
Epoch [61700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [61800/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [61900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [62000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [62100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [62200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [62300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [62400/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [62500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [62600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [62700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [62800/200000] Training Loss: 0.00000044 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [62900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [63000/200000] Training Loss: 0.00001473 Testing Loss: 0.00000438 || lr: [0.00049]
Epoch [63100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [63200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [63300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [63400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [63500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [63600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [63700/200000] Training Loss: 0.00000036 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [63800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [63900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [64000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [64100/200000] Training Loss: 0.00000606 Testing Loss: 0.00000946 || lr: [0.00049]
Epoch [64200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [64300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [64400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [64500/200000] Training Loss: 0.00000036 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [64600/200000] Training Loss: 0.00000338 Testing Loss: 0.00000433 || lr: [0.00049]
Epoch [64700/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [64800/200000] Training Loss: 0.00000085 Testing Loss: 0.00000109 || lr: [0.00049]
Epoch [64900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [65000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [65100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [65200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [65300/200000] Training Loss: 0.00000056 Testing Loss: 0.00000066 || lr: [0.00049]
Epoch [65400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [65500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [65600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [65700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [65800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [65900/200000] Training Loss: 0.00000203 Testing Loss: 0.00000199 || lr: [0.00049]
Epoch [66000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [66100/200000] Training Loss: 0.00000044 Testing Loss: 0.00000051 || lr: [0.00049]
Epoch [66200/200000] Training Loss: 0.00000509 Testing Loss: 0.00000394 || lr: [0.00049]
Epoch [66300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [66400/200000] Training Loss: 0.00000413 Testing Loss: 0.00000707 || lr: [0.00049]
Epoch [66500/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [66600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [66700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [66800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [66900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [67000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [67100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [67200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [67300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [67400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [67500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [67600/200000] Training Loss: 0.00000039 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [67700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [67800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [67900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [68000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [68100/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [68200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [68300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [68400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [68500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [68600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [68700/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [68800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [68900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [69000/200000] Training Loss: 0.00000045 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [69100/200000] Training Loss: 0.00000168 Testing Loss: 0.00000109 || lr: [0.00049]
Epoch [69200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [69300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [69400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [69500/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [69600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [69700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [69800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [69900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [70000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [70100/200000] Training Loss: 0.00000042 Testing Loss: 0.00000047 || lr: [0.00049]
Epoch [70200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [70300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [70400/200000] Training Loss: 0.00002884 Testing Loss: 0.00005216 || lr: [0.00049]
Epoch [70500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [70600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [70700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [70800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [70900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71600/200000] Training Loss: 0.00000044 Testing Loss: 0.00000061 || lr: [0.00049]
Epoch [71700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [71900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [72000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [72100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [72200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [72300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [72400/200000] Training Loss: 0.00000474 Testing Loss: 0.00000498 || lr: [0.00049]
Epoch [72500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [72600/200000] Training Loss: 0.00000162 Testing Loss: 0.00000155 || lr: [0.00049]
Epoch [72700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [72800/200000] Training Loss: 0.00000039 Testing Loss: 0.00000044 || lr: [0.00049]
Epoch [72900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [73000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [73100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [73200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [73300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [73400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [73500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [73600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [73700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [73800/200000] Training Loss: 0.00000097 Testing Loss: 0.00000050 || lr: [0.00049]
Epoch [73900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [74000/200000] Training Loss: 0.00000034 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [74100/200000] Training Loss: 0.00000107 Testing Loss: 0.00000119 || lr: [0.00049]
Epoch [74200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [74300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [74400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [74500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [74600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [74700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [74800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [74900/200000] Training Loss: 0.00000055 Testing Loss: 0.00000049 || lr: [0.00049]
Epoch [75000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [75100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [75200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [75300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [75400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [75500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [75600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [75700/200000] Training Loss: 0.00000039 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [75800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [75900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [76000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [76100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [76200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [76300/200000] Training Loss: 0.00000165 Testing Loss: 0.00000151 || lr: [0.00049]
Epoch [76400/200000] Training Loss: 0.00000126 Testing Loss: 0.00000166 || lr: [0.00049]
Epoch [76500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [76600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [76700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [76800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [76900/200000] Training Loss: 0.00002060 Testing Loss: 0.00001949 || lr: [0.00049]
Epoch [77000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [77900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [78000/200000] Training Loss: 0.00000174 Testing Loss: 0.00000146 || lr: [0.00049]
Epoch [78100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [78200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [78300/200000] Training Loss: 0.00000265 Testing Loss: 0.00000263 || lr: [0.00049]
Epoch [78400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [78500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [78600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [78700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [78800/200000] Training Loss: 0.00000040 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [78900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [79000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [79100/200000] Training Loss: 0.00000044 Testing Loss: 0.00000047 || lr: [0.00049]
Epoch [79200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [79300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [79400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [79500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [79600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [79700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [79800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [79900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [80000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [80100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [80200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [80300/200000] Training Loss: 0.00000053 Testing Loss: 0.00000075 || lr: [0.00049]
Epoch [80400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [80500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [80600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [80700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [80800/200000] Training Loss: 0.00000069 Testing Loss: 0.00000058 || lr: [0.00049]
Epoch [80900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [81000/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [81100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [81200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [81300/200000] Training Loss: 0.00000088 Testing Loss: 0.00000049 || lr: [0.00049]
Epoch [81400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [81500/200000] Training Loss: 0.00000129 Testing Loss: 0.00000118 || lr: [0.00049]
Epoch [81600/200000] Training Loss: 0.00000051 Testing Loss: 0.00000052 || lr: [0.00049]
Epoch [81700/200000] Training Loss: 0.00000082 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [81800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [81900/200000] Training Loss: 0.00000118 Testing Loss: 0.00000146 || lr: [0.00049]
Epoch [82000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [82100/200000] Training Loss: 0.00000041 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [82200/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [82300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [82400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [82500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [82600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [82700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [82800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [82900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83100/200000] Training Loss: 0.00000035 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [83200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [83900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [84000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [84100/200000] Training Loss: 0.00000053 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [84200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [84300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [84400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [84500/200000] Training Loss: 0.00000038 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [84600/200000] Training Loss: 0.00000786 Testing Loss: 0.00001186 || lr: [0.00049]
Epoch [84700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [84800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [84900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [85000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [85100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [85200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [85300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [85400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [85500/200000] Training Loss: 0.00000035 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [85600/200000] Training Loss: 0.00000215 Testing Loss: 0.00000268 || lr: [0.00049]
Epoch [85700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [85800/200000] Training Loss: 0.00000917 Testing Loss: 0.00002043 || lr: [0.00049]
Epoch [85900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86600/200000] Training Loss: 0.00000041 Testing Loss: 0.00000111 || lr: [0.00049]
Epoch [86700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [86900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [87000/200000] Training Loss: 0.00000039 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [87100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [87200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [87300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [87400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [87500/200000] Training Loss: 0.00002295 Testing Loss: 0.00001314 || lr: [0.00049]
Epoch [87600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [87700/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [87800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [87900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [88900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [89000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [89100/200000] Training Loss: 0.00000139 Testing Loss: 0.00000224 || lr: [0.00049]
Epoch [89200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [89300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [89400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [89500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [89600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [89700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [89800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [89900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [90000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [90100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [90200/200000] Training Loss: 0.00000135 Testing Loss: 0.00000147 || lr: [0.00049]
Epoch [90300/200000] Training Loss: 0.00000217 Testing Loss: 0.00000168 || lr: [0.00049]
Epoch [90400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [90500/200000] Training Loss: 0.00000046 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [90600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [90700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [90800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [90900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91600/200000] Training Loss: 0.00000616 Testing Loss: 0.00000486 || lr: [0.00049]
Epoch [91700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [91900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [92000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [92100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [92200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [92300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [92400/200000] Training Loss: 0.00000330 Testing Loss: 0.00000277 || lr: [0.00049]
Epoch [92500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [92600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [92700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [92800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [92900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [93400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [93900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [94000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [94100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [94200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [94300/200000] Training Loss: 0.00000037 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [94400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [94500/200000] Training Loss: 0.00000048 Testing Loss: 0.00000059 || lr: [0.00049]
Epoch [94600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [94700/200000] Training Loss: 0.00000035 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [94800/200000] Training Loss: 0.00001695 Testing Loss: 0.00001739 || lr: [0.00049]
Epoch [94900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [95000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [95100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [95200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [95300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [95400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [95500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [95600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [95700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [95800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [95900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [96000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [96100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [96200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [96300/200000] Training Loss: 0.00000046 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [96400/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [96500/200000] Training Loss: 0.00000034 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [96600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [96700/200000] Training Loss: 0.00000164 Testing Loss: 0.00000263 || lr: [0.00049]
Epoch [96800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [96900/200000] Training Loss: 0.00000043 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [97000/200000] Training Loss: 0.00000056 Testing Loss: 0.00000067 || lr: [0.00049]
Epoch [97100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [97200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [97300/200000] Training Loss: 0.00000117 Testing Loss: 0.00000103 || lr: [0.00049]
Epoch [97400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [97500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [97600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [97700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [97800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [97900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [98800/200000] Training Loss: 0.00000042 Testing Loss: 0.00000049 || lr: [0.00049]
Epoch [98900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [99000/200000] Training Loss: 0.00000042 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [99100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [99200/200000] Training Loss: 0.00000050 Testing Loss: 0.00000052 || lr: [0.00049]
Epoch [99300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [99400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [99500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [99600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [99700/200000] Training Loss: 0.00000066 Testing Loss: 0.00000079 || lr: [0.00049]
Epoch [99800/200000] Training Loss: 0.00000035 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [99900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [100000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [100100/200000] Training Loss: 0.00000646 Testing Loss: 0.00000223 || lr: [0.00049]
Epoch [100200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [100300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [100400/200000] Training Loss: 0.00000037 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [100500/200000] Training Loss: 0.00000037 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [100600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [100700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [100800/200000] Training Loss: 0.00000050 Testing Loss: 0.00000051 || lr: [0.00049]
Epoch [100900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [101000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [101100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [101200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [101300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [101400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [101500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [101600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [101700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [101800/200000] Training Loss: 0.00000039 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [101900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [102000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [102100/200000] Training Loss: 0.00000049 Testing Loss: 0.00000055 || lr: [0.00049]
Epoch [102200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [102300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [102400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [102500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [102600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [102700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [102800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [102900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [103000/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [103100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [103200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [103300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [103400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [103500/200000] Training Loss: 0.00000036 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [103600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [103700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [103800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [103900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [104000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [104100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [104200/200000] Training Loss: 0.00000041 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [104300/200000] Training Loss: 0.00000041 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [104400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [104500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [104600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [104700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [104800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [104900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [105000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [105100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [105200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [105300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [105400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [105500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [105600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [105700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [105800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [105900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [106000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [106100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [106200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [106300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [106400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [106500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [106600/200000] Training Loss: 0.00000142 Testing Loss: 0.00000197 || lr: [0.00049]
Epoch [106700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [106800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [106900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [107000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [107100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [107200/200000] Training Loss: 0.00000043 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [107300/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [107400/200000] Training Loss: 0.00000094 Testing Loss: 0.00000065 || lr: [0.00049]
Epoch [107500/200000] Training Loss: 0.00000105 Testing Loss: 0.00000134 || lr: [0.00049]
Epoch [107600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [107700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [107800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [107900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [108000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [108100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [108200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [108300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [108400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [108500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [108600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [108700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [108800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [108900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [109000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [109100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [109200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [109300/200000] Training Loss: 0.00000042 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [109400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [109500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [109600/200000] Training Loss: 0.00000037 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [109700/200000] Training Loss: 0.00000067 Testing Loss: 0.00000070 || lr: [0.00049]
Epoch [109800/200000] Training Loss: 0.00000160 Testing Loss: 0.00000101 || lr: [0.00049]
Epoch [109900/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [110000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [110100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [110200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [110300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [110400/200000] Training Loss: 0.00000034 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [110500/200000] Training Loss: 0.00000042 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [110600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [110700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [110800/200000] Training Loss: 0.00000047 Testing Loss: 0.00000049 || lr: [0.00049]
Epoch [110900/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [111000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [111100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [111200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [111300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [111400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [111500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [111600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [111700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [111800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [111900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [112800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [112900/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [113000/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [113100/200000] Training Loss: 0.00000036 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [113200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [113300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [113400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [113500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [113600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [113700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [113800/200000] Training Loss: 0.00000050 Testing Loss: 0.00000062 || lr: [0.00049]
Epoch [113900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [114000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [114100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [114200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [114300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [114400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [114500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [114600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [114700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [114800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [114900/200000] Training Loss: 0.00000071 Testing Loss: 0.00000078 || lr: [0.00049]
Epoch [115000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [115100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [115200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [115300/200000] Training Loss: 0.00000057 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [115400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [115500/200000] Training Loss: 0.00000038 Testing Loss: 0.00000050 || lr: [0.00049]
Epoch [115600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [115700/200000] Training Loss: 0.00000047 Testing Loss: 0.00000050 || lr: [0.00049]
Epoch [115800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [115900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [116000/200000] Training Loss: 0.00000044 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [116100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [116200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [116300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [116400/200000] Training Loss: 0.00000135 Testing Loss: 0.00000114 || lr: [0.00049]
Epoch [116500/200000] Training Loss: 0.00000036 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [116600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [116700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [116800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [116900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [117000/200000] Training Loss: 0.00000037 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [117100/200000] Training Loss: 0.00000679 Testing Loss: 0.00000474 || lr: [0.00049]
Epoch [117200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [117300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [117400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [117500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [117600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [117700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [117800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [117900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [118000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [118100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [118200/200000] Training Loss: 0.00000041 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [118300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [118400/200000] Training Loss: 0.00000038 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [118500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [118600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [118700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [118800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [118900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [119000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [119100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [119200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [119300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [119400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [119500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [119600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [119700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [119800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [119900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [120000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [120100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [120200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [120300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [120400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [120500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [120600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [120700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [120800/200000] Training Loss: 0.00000059 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [120900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [121000/200000] Training Loss: 0.00000039 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [121100/200000] Training Loss: 0.00000037 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [121200/200000] Training Loss: 0.00000134 Testing Loss: 0.00000178 || lr: [0.00049]
Epoch [121300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [121400/200000] Training Loss: 0.00000036 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [121500/200000] Training Loss: 0.00000178 Testing Loss: 0.00000117 || lr: [0.00049]
Epoch [121600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [121700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [121800/200000] Training Loss: 0.00000088 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [121900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [122000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [122100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [122200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [122300/200000] Training Loss: 0.00000121 Testing Loss: 0.00000111 || lr: [0.00049]
Epoch [122400/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [122500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [122600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [122700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [122800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [122900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [123900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [124000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [124100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [124200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [124300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [124400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [124500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [124600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [124700/200000] Training Loss: 0.00000042 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [124800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [124900/200000] Training Loss: 0.00000038 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [125000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [125100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [125200/200000] Training Loss: 0.00000038 Testing Loss: 0.00000049 || lr: [0.00049]
Epoch [125300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [125400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [125500/200000] Training Loss: 0.00000043 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [125600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [125700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [125800/200000] Training Loss: 0.00000046 Testing Loss: 0.00000049 || lr: [0.00049]
Epoch [125900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [126000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [126900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [127000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [127100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [127200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [127300/200000] Training Loss: 0.00000039 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [127400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [127500/200000] Training Loss: 0.00000036 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [127600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [127700/200000] Training Loss: 0.00000414 Testing Loss: 0.00000218 || lr: [0.00049]
Epoch [127800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [127900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [128000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [128100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [128200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [128300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [128400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [128500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [128600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [128700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [128800/200000] Training Loss: 0.00000226 Testing Loss: 0.00000177 || lr: [0.00049]
Epoch [128900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [129000/200000] Training Loss: 0.00000109 Testing Loss: 0.00000104 || lr: [0.00049]
Epoch [129100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [129200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [129300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [129400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [129500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [129600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [129700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [129800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [129900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [130800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [130900/200000] Training Loss: 0.00000042 Testing Loss: 0.00000044 || lr: [0.00049]
Epoch [131000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [131100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [131200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [131300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [131400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [131500/200000] Training Loss: 0.00000066 Testing Loss: 0.00000074 || lr: [0.00049]
Epoch [131600/200000] Training Loss: 0.00000043 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [131700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [131800/200000] Training Loss: 0.00000033 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [131900/200000] Training Loss: 0.00000037 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [132000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [132100/200000] Training Loss: 0.00000055 Testing Loss: 0.00000055 || lr: [0.00049]
Epoch [132200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [132300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [132400/200000] Training Loss: 0.00000045 Testing Loss: 0.00000047 || lr: [0.00049]
Epoch [132500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [132600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [132700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [132800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [132900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [133900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [134000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [134100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [134200/200000] Training Loss: 0.00000037 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [134300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [134400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [134500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [134600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [134700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [134800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [134900/200000] Training Loss: 0.00000069 Testing Loss: 0.00000089 || lr: [0.00049]
Epoch [135000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [135100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [135200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [135300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [135400/200000] Training Loss: 0.00000315 Testing Loss: 0.00000462 || lr: [0.00049]
Epoch [135500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [135600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [135700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [135800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [135900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [136500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [136900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [137000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [137100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [137200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [137300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [137400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [137500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [137600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [137700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [137800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [137900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [138000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [138100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [138200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [138300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [138400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [138500/200000] Training Loss: 0.00000036 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [138600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [138700/200000] Training Loss: 0.00000033 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [138800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [138900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [139000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [139100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [139200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [139300/200000] Training Loss: 0.00000043 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [139400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [139500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [139600/200000] Training Loss: 0.00000042 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [139700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [139800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [139900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [140000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [140100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [140200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [140300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [140400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [140500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [140600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [140700/200000] Training Loss: 0.00000178 Testing Loss: 0.00000180 || lr: [0.00049]
Epoch [140800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [140900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [141000/200000] Training Loss: 0.00000039 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [141100/200000] Training Loss: 0.00000094 Testing Loss: 0.00000089 || lr: [0.00049]
Epoch [141200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [141300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [141400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [141500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [141600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [141700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [141800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [141900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [142900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [143000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [143100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [143200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [143300/200000] Training Loss: 0.00000034 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [143400/200000] Training Loss: 0.00000062 Testing Loss: 0.00000047 || lr: [0.00049]
Epoch [143500/200000] Training Loss: 0.00000154 Testing Loss: 0.00000120 || lr: [0.00049]
Epoch [143600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [143700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [143800/200000] Training Loss: 0.00000456 Testing Loss: 0.00000437 || lr: [0.00049]
Epoch [143900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [144000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [144100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [144200/200000] Training Loss: 0.00000040 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [144300/200000] Training Loss: 0.00000034 Testing Loss: 0.00000051 || lr: [0.00049]
Epoch [144400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [144500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [144600/200000] Training Loss: 0.00000040 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [144700/200000] Training Loss: 0.00001345 Testing Loss: 0.00002519 || lr: [0.00049]
Epoch [144800/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [144900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [145900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [146000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [146100/200000] Training Loss: 0.00000043 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [146200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [146300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [146400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [146500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [146600/200000] Training Loss: 0.00000043 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [146700/200000] Training Loss: 0.00000043 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [146800/200000] Training Loss: 0.00000137 Testing Loss: 0.00000186 || lr: [0.00049]
Epoch [146900/200000] Training Loss: 0.00000038 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [147000/200000] Training Loss: 0.00000176 Testing Loss: 0.00000212 || lr: [0.00049]
Epoch [147100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [147200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [147300/200000] Training Loss: 0.00000112 Testing Loss: 0.00000052 || lr: [0.00049]
Epoch [147400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [147500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [147600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [147700/200000] Training Loss: 0.00000038 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [147800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [147900/200000] Training Loss: 0.00000081 Testing Loss: 0.00000095 || lr: [0.00049]
Epoch [148000/200000] Training Loss: 0.00000043 Testing Loss: 0.00000044 || lr: [0.00049]
Epoch [148100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [148200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [148300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [148400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [148500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [148600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [148700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [148800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [148900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [149900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [150000/200000] Training Loss: 0.00000036 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [150100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [150200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [150300/200000] Training Loss: 0.00000037 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [150400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [150500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [150600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [150700/200000] Training Loss: 0.00000071 Testing Loss: 0.00000098 || lr: [0.00049]
Epoch [150800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [150900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [151000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [151100/200000] Training Loss: 0.00000054 Testing Loss: 0.00000060 || lr: [0.00049]
Epoch [151200/200000] Training Loss: 0.00000045 Testing Loss: 0.00000086 || lr: [0.00049]
Epoch [151300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [151400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [151500/200000] Training Loss: 0.00000048 Testing Loss: 0.00000050 || lr: [0.00049]
Epoch [151600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [151700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [151800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [151900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [152000/200000] Training Loss: 0.00000057 Testing Loss: 0.00000074 || lr: [0.00049]
Epoch [152100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [152200/200000] Training Loss: 0.00000099 Testing Loss: 0.00000135 || lr: [0.00049]
Epoch [152300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [152400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [152500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [152600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [152700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [152800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [152900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [153000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [153100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [153200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [153300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [153400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [153500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [153600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [153700/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [153800/200000] Training Loss: 0.00000038 Testing Loss: 0.00000040 || lr: [0.00049]
Epoch [153900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [154000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [154900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [155000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [155100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [155200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [155300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [155400/200000] Training Loss: 0.00000185 Testing Loss: 0.00000227 || lr: [0.00049]
Epoch [155500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [155600/200000] Training Loss: 0.00000256 Testing Loss: 0.00000304 || lr: [0.00049]
Epoch [155700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [155800/200000] Training Loss: 0.00000035 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [155900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [156000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [156100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [156200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [156300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [156400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [156500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [156600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [156700/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [156800/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [156900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [157000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [157100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [157200/200000] Training Loss: 0.00000035 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [157300/200000] Training Loss: 0.00000204 Testing Loss: 0.00000303 || lr: [0.00049]
Epoch [157400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [157500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [157600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [157700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [157800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [157900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [158000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [158100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [158200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [158300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [158400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [158500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [158600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [158700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [158800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [158900/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [159000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [159100/200000] Training Loss: 0.00000044 Testing Loss: 0.00000045 || lr: [0.00049]
Epoch [159200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [159300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [159400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [159500/200000] Training Loss: 0.00000084 Testing Loss: 0.00000106 || lr: [0.00049]
Epoch [159600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [159700/200000] Training Loss: 0.00000295 Testing Loss: 0.00000752 || lr: [0.00049]
Epoch [159800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [159900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [160000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [160100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [160200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [160300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [160400/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [160500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [160600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [160700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [160800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [160900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [161000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [161100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [161200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000047 || lr: [0.00049]
Epoch [161300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [161400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [161500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [161600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [161700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [161800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [161900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [162000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [162100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [162200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [162300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [162400/200000] Training Loss: 0.00000031 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [162500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [162600/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [162700/200000] Training Loss: 0.00000055 Testing Loss: 0.00000049 || lr: [0.00049]
Epoch [162800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [162900/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [163000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [163100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [163200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [163300/200000] Training Loss: 0.00000052 Testing Loss: 0.00000074 || lr: [0.00049]
Epoch [163400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [163500/200000] Training Loss: 0.00000031 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [163600/200000] Training Loss: 0.00000511 Testing Loss: 0.00000557 || lr: [0.00049]
Epoch [163700/200000] Training Loss: 0.00000036 Testing Loss: 0.00000039 || lr: [0.00049]
Epoch [163800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [163900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [164000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [164100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [164200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [164300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [164400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [164500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [164600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [164700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [164800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [164900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [165800/200000] Training Loss: 0.00000040 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [165900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [166000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [166100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [166200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [166300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [166400/200000] Training Loss: 0.00000043 Testing Loss: 0.00000051 || lr: [0.00049]
Epoch [166500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [166600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [166700/200000] Training Loss: 0.00000060 Testing Loss: 0.00000059 || lr: [0.00049]
Epoch [166800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [166900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [167000/200000] Training Loss: 0.00000041 Testing Loss: 0.00000044 || lr: [0.00049]
Epoch [167100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [167200/200000] Training Loss: 0.00000087 Testing Loss: 0.00000107 || lr: [0.00049]
Epoch [167300/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [167400/200000] Training Loss: 0.00000730 Testing Loss: 0.00000788 || lr: [0.00049]
Epoch [167500/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [167600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [167700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [167800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [167900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [168900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [169000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [169100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [169200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [169300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [169400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [169500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [169600/200000] Training Loss: 0.00000033 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [169700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [169800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [169900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [170000/200000] Training Loss: 0.00000046 Testing Loss: 0.00000046 || lr: [0.00049]
Epoch [170100/200000] Training Loss: 0.00000037 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [170200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [170300/200000] Training Loss: 0.00000033 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [170400/200000] Training Loss: 0.00000037 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [170500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [170600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [170700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [170800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [170900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [171000/200000] Training Loss: 0.00000045 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [171100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [171200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [171300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [171400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [171500/200000] Training Loss: 0.00000046 Testing Loss: 0.00000060 || lr: [0.00049]
Epoch [171600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [171700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [171800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [171900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [172000/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [172100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [172200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [172300/200000] Training Loss: 0.00000083 Testing Loss: 0.00000082 || lr: [0.00049]
Epoch [172400/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [172500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [172600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [172700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [172800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [172900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [173900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [174000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [174100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [174200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [174300/200000] Training Loss: 0.00000042 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [174400/200000] Training Loss: 0.00000032 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [174500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [174600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [174700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [174800/200000] Training Loss: 0.00000036 Testing Loss: 0.00000041 || lr: [0.00049]
Epoch [174900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [175000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [175100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [175200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [175300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [175400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [175500/200000] Training Loss: 0.00000042 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [175600/200000] Training Loss: 0.00000048 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [175700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [175800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [175900/200000] Training Loss: 0.00000040 Testing Loss: 0.00000042 || lr: [0.00049]
Epoch [176000/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [176100/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [176200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [176300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [176400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [176500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [176600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [176700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [176800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [176900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [177800/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [177900/200000] Training Loss: 0.00000044 Testing Loss: 0.00000051 || lr: [0.00049]
Epoch [178000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [178100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [178200/200000] Training Loss: 0.00000078 Testing Loss: 0.00000089 || lr: [0.00049]
Epoch [178300/200000] Training Loss: 0.00000142 Testing Loss: 0.00000165 || lr: [0.00049]
Epoch [178400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [178500/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [178600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [178700/200000] Training Loss: 0.00000093 Testing Loss: 0.00000238 || lr: [0.00049]
Epoch [178800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [178900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [179000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [179100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000037 || lr: [0.00049]
Epoch [179200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [179300/200000] Training Loss: 0.00000154 Testing Loss: 0.00000112 || lr: [0.00049]
Epoch [179400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [179500/200000] Training Loss: 0.00000332 Testing Loss: 0.00000325 || lr: [0.00049]
Epoch [179600/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [179700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [179800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [179900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [180000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [180100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [180200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [180300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [180400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [180500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [180600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [180700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [180800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [180900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [181000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [181100/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [181200/200000] Training Loss: 0.00000068 Testing Loss: 0.00000077 || lr: [0.00049]
Epoch [181300/200000] Training Loss: 0.00000217 Testing Loss: 0.00000243 || lr: [0.00049]
Epoch [181400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [181500/200000] Training Loss: 0.00000038 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [181600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [181700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [181800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [181900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [182000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [182100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [182200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [182300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [182400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [182500/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [182600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000048 || lr: [0.00049]
Epoch [182700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [182800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [182900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [183000/200000] Training Loss: 0.00000409 Testing Loss: 0.00000367 || lr: [0.00049]
Epoch [183100/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [183200/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [183300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [183400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [183500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [183600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [183700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [183800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [183900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [184000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [184100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [184200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [184300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [184400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [184500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [184600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [184700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [184800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [184900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [185000/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [185100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [185200/200000] Training Loss: 0.00000032 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [185300/200000] Training Loss: 0.00000032 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [185400/200000] Training Loss: 0.00000090 Testing Loss: 0.00000118 || lr: [0.00049]
Epoch [185500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [185600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [185700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [185800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [185900/200000] Training Loss: 0.00000045 Testing Loss: 0.00000054 || lr: [0.00049]
Epoch [186000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [186100/200000] Training Loss: 0.00000034 Testing Loss: 0.00000035 || lr: [0.00049]
Epoch [186200/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [186300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [186400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [186500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [186600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [186700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [186800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [186900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [187000/200000] Training Loss: 0.00000053 Testing Loss: 0.00000057 || lr: [0.00049]
Epoch [187100/200000] Training Loss: 0.00001648 Testing Loss: 0.00002111 || lr: [0.00049]
Epoch [187200/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [187300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [187400/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [187500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [187600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [187700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [187800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [187900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [188000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [188100/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [188200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [188300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [188400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [188500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [188600/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [188700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000033 || lr: [0.00049]
Epoch [188800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [188900/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [189000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [189100/200000] Training Loss: 0.00000033 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [189200/200000] Training Loss: 0.00000034 Testing Loss: 0.00000036 || lr: [0.00049]
Epoch [189300/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [189400/200000] Training Loss: 0.00000054 Testing Loss: 0.00000055 || lr: [0.00049]
Epoch [189500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [189600/200000] Training Loss: 0.00000034 Testing Loss: 0.00000038 || lr: [0.00049]
Epoch [189700/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [189800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [189900/200000] Training Loss: 0.00000031 Testing Loss: 0.00000034 || lr: [0.00049]
Epoch [190000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [190100/200000] Training Loss: 0.00000051 Testing Loss: 0.00000058 || lr: [0.00049]
Epoch [190200/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [190300/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [190400/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [190500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [190600/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [190700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [190800/200000] Training Loss: 0.00000030 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [190900/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [191000/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [191100/200000] Training Loss: 0.00000046 Testing Loss: 0.00000047 || lr: [0.00049]
Epoch [191200/200000] Training Loss: 0.00000039 Testing Loss: 0.00000043 || lr: [0.00049]
Epoch [191300/200000] Training Loss: 0.00000040 Testing Loss: 0.00000056 || lr: [0.00049]
Epoch [191400/200000] Training Loss: 0.00000046 Testing Loss: 0.00000052 || lr: [0.00049]
Epoch [191500/200000] Training Loss: 0.00000029 Testing Loss: 0.00000031 || lr: [0.00049]
Epoch [191600/200000] Training Loss: 0.00000105 Testing Loss: 0.00000072 || lr: [0.00049]
Epoch [191700/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Epoch [191800/200000] Training Loss: 0.00000029 Testing Loss: 0.00000032 || lr: [0.00049]
Early stopping!
/home/ubuntu/Schrodinger/GreenNet3/main.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  net.load_state_dict(torch.load("net.pth", map_location = device))
numerical result for u_1:
Absolute L2 Norm Error: 0.00027043
Absolute Max Norm Error: 0.00171460
Relative L2 Norm Error: 0.214712%
Relative Max Norm Error: 0.685838%
numerical result for u_2:
Absolute L2 Norm Error: 0.00036826
Absolute Max Norm Error: 0.00179922
Relative L2 Norm Error: 0.292388%
Relative Max Norm Error: 0.719687%
numerical result for u_1:
Absolute L2 Norm Error: 0.00053998
Absolute Max Norm Error: 0.00151578
Relative L2 Norm Error: 0.428721%
Relative Max Norm Error: 0.606312%
numerical result for u_2:
Absolute L2 Norm Error: 0.00036784
Absolute Max Norm Error: 0.00109008
Relative L2 Norm Error: 0.292055%
Relative Max Norm Error: 0.436031%
numerical result for u_1:
Absolute L2 Norm Error: 0.00037180
Absolute Max Norm Error: 0.00161524
Relative L2 Norm Error: 0.295197%
Relative Max Norm Error: 0.646094%
numerical result for u_2:
Absolute L2 Norm Error: 0.00035480
Absolute Max Norm Error: 0.00149200
Relative L2 Norm Error: 0.281696%
Relative Max Norm Error: 0.596801%
