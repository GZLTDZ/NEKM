{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file, we consider the PDE system $\\mathcal{L} \\mathbf{u} = \\mathbf{f}$ with zero Dirichlet boundary condition, where\n",
    "$$\n",
    "\\mathcal{L}=\\left[\\begin{array}{cc}\n",
    "1 & -\\lambda \\Delta \\\\\n",
    "\\lambda \\Delta & 1\n",
    "\\end{array}\\right],\n",
    "\\quad\n",
    "\\mathbf{u}=\\left[\\begin{array}{c} u_1 \\\\ u_2 \\end{array}\\right]\n",
    "\\quad\n",
    "\\mathbf{f}=\\left[\\begin{array}{c} f_1 \\\\ f_2 \\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU memory: 24217.31 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu_info.name}\")\n",
    "    print(f\"GPU memory: {gpu_info.total_memory / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 41\n",
    "n = N - 1\n",
    "h = 1 / n \n",
    "m = n - 1\n",
    "m2 = m * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData1(lambda_):\n",
    "    id_m = np.identity(m)\n",
    "    d1= np.identity(m)\n",
    "    d1[0][1] = 1\n",
    "    d1[m - 1][m - 2] = 1\n",
    "    for i in range(0, m):\n",
    "        d1[i][i] = -2\n",
    "        if (i >= 1) and (i <= m - 2):\n",
    "            d1[i][i - 1] = 1\n",
    "            d1[i][i + 1] = 1\n",
    "    D = np.kron(id_m, d1) + np.kron(d1, id_m)\n",
    "    D = D / (h ** 2)\n",
    "\n",
    "    L = np.zeros((2 * m2, 2 * m2))\n",
    "    L[0: m2, 0: m2] = np.identity(m2)\n",
    "    L[0: m2, m2: 2 * m2] = - lambda_ * D\n",
    "    L[m2: 2 * m2, 0: m2] = lambda_ * D\n",
    "    L[m2: 2 * m2, m2: 2 * m2] = np.identity(m2)\n",
    "\n",
    "    L_sparse = sparse.csr_matrix(L)\n",
    "\n",
    "    f = np.random.rand(5000, 2 * m2)\n",
    "    u = np.zeros_like(f)\n",
    "    for i in range(5000):\n",
    "        u[i, :] = spsolve(L_sparse, f[i, :])\n",
    "\n",
    "    lambda_np = lambda_ + np.zeros_like(f[:, 0].reshape((-1, 1)))\n",
    "\n",
    "    return lambda_np, f, u\n",
    "\n",
    "def generateData2(lambda_values):\n",
    "    lambda_list = []\n",
    "    f_list = []\n",
    "    u_list = []\n",
    "\n",
    "    for lambda_ in lambda_values:\n",
    "        lambda_np, f, u = generateData1(lambda_)\n",
    "        lambda_list.append(lambda_np)\n",
    "        f_list.append(f)\n",
    "        u_list.append(u)\n",
    "    \n",
    "    return np.concatenate(lambda_list, axis=0), np.concatenate(f_list, axis=0), np.concatenate(u_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([55000, 1]), torch.Size([55000, 3042]), torch.Size([55000, 3042]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda__ = np.linspace(0.025, 0.05, 11)\n",
    "lambda_values, f, u = generateData2(lambda__)\n",
    "lambda_values, f, u = torch.tensor(lambda_values, dtype=torch.float32), torch.tensor(f, dtype=torch.float32), torch.tensor(u, dtype=torch.float32)\n",
    "lambda_values.shape, f.shape, u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GreenFun(nn.Module):\n",
    "    def __init__(self, N):     \n",
    "        super(GreenFun, self).__init__()\n",
    "        self.N = N\n",
    "        self.lambda_layer = nn.Sequential(nn.Linear(1, N // 4), nn.ReLU(), nn.Linear(N // 4, N // 4), nn.ReLU(), nn.Linear(N // 4, N // 4), nn.ReLU(), nn.Linear(N // 4, N // 4), nn.ReLU(), nn.Linear(N // 4, N // 4))\n",
    "        self.G_layer1 = nn.Sequential(nn.Linear(N, N // 4, bias = False))\n",
    "        self.G_layer2 = nn.Sequential(nn.Linear(N // 4, N, bias = False))\n",
    "\n",
    "    def forward(self, lambda_values,  f):   \n",
    "        return self.G_layer2(self.lambda_layer(lambda_values) * self.G_layer1(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "dataset = TensorDataset(lambda_values, f, u)\n",
    "\n",
    "# 定义训练集和测试集的大小\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# 将数据集按比例分成训练集和测试集\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 获取训练集的数据\n",
    "train_lambda = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))])\n",
    "train_f = torch.stack([train_dataset[i][1] for i in range(len(train_dataset))])\n",
    "train_u = torch.stack([train_dataset[i][2] for i in range(len(train_dataset))])\n",
    "\n",
    "# 获取测试集的数据\n",
    "test_lambda = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))])\n",
    "test_f = torch.stack([test_dataset[i][1] for i in range(len(test_dataset))])\n",
    "test_u = torch.stack([test_dataset[i][2] for i in range(len(test_dataset))])\n",
    "\n",
    "train_lambda = train_lambda.to(device)\n",
    "train_f = train_f.to(device)\n",
    "train_u = train_u.to(device)\n",
    "test_lambda = test_lambda.to(device)\n",
    "test_f = test_f.to(device)\n",
    "test_u = test_u.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, verbose, delta, path):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.8f} --> {val_loss:.8f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/200000] Training Loss: 0.00202217 Testing Loss: 0.00169732\n",
      "Epoch [200/200000] Training Loss: 0.00167147 Testing Loss: 0.00149630\n",
      "Epoch [300/200000] Training Loss: 0.00026859 Testing Loss: 0.00038907\n",
      "Epoch [400/200000] Training Loss: 0.00010826 Testing Loss: 0.00010216\n",
      "Epoch [500/200000] Training Loss: 0.00012375 Testing Loss: 0.00010724\n",
      "Epoch [600/200000] Training Loss: 0.00014008 Testing Loss: 0.00011561\n",
      "Epoch [700/200000] Training Loss: 0.00011266 Testing Loss: 0.00010829\n",
      "Epoch [800/200000] Training Loss: 0.00012865 Testing Loss: 0.00013815\n",
      "Epoch [900/200000] Training Loss: 0.00009525 Testing Loss: 0.00009590\n",
      "Epoch [1000/200000] Training Loss: 0.00020776 Testing Loss: 0.00011441\n",
      "Epoch [1100/200000] Training Loss: 0.00022312 Testing Loss: 0.00024806\n",
      "Epoch [1200/200000] Training Loss: 0.00010659 Testing Loss: 0.00010679\n",
      "Epoch [1300/200000] Training Loss: 0.00009277 Testing Loss: 0.00009178\n",
      "Epoch [1400/200000] Training Loss: 0.00011107 Testing Loss: 0.00012650\n",
      "Epoch [1500/200000] Training Loss: 0.00010310 Testing Loss: 0.00010227\n",
      "Epoch [1600/200000] Training Loss: 0.00011518 Testing Loss: 0.00013583\n",
      "Epoch [1700/200000] Training Loss: 0.00009433 Testing Loss: 0.00009818\n",
      "Epoch [1800/200000] Training Loss: 0.00008742 Testing Loss: 0.00008677\n",
      "Epoch [1900/200000] Training Loss: 0.00019479 Testing Loss: 0.00017266\n",
      "Epoch [2000/200000] Training Loss: 0.00019282 Testing Loss: 0.00032149\n",
      "Epoch [2100/200000] Training Loss: 0.00007715 Testing Loss: 0.00007503\n",
      "Epoch [2200/200000] Training Loss: 0.00007155 Testing Loss: 0.00006839\n",
      "Epoch [2300/200000] Training Loss: 0.00006538 Testing Loss: 0.00006694\n",
      "Epoch [2400/200000] Training Loss: 0.00029066 Testing Loss: 0.00029051\n",
      "Epoch [2500/200000] Training Loss: 0.00007363 Testing Loss: 0.00009113\n",
      "Epoch [2600/200000] Training Loss: 0.00005258 Testing Loss: 0.00005330\n",
      "Epoch [2700/200000] Training Loss: 0.00004423 Testing Loss: 0.00004533\n",
      "Epoch [2800/200000] Training Loss: 0.00005376 Testing Loss: 0.00005864\n",
      "Epoch [2900/200000] Training Loss: 0.00004733 Testing Loss: 0.00004713\n",
      "Epoch [3000/200000] Training Loss: 0.00004015 Testing Loss: 0.00004184\n",
      "Epoch [3100/200000] Training Loss: 0.00012017 Testing Loss: 0.00011626\n",
      "Epoch [3200/200000] Training Loss: 0.00009285 Testing Loss: 0.00013205\n",
      "Epoch [3300/200000] Training Loss: 0.00006037 Testing Loss: 0.00007451\n",
      "Epoch [3400/200000] Training Loss: 0.00003835 Testing Loss: 0.00003792\n",
      "Epoch [3500/200000] Training Loss: 0.00017927 Testing Loss: 0.00018142\n",
      "Epoch [3600/200000] Training Loss: 0.00003631 Testing Loss: 0.00005011\n",
      "Epoch [3700/200000] Training Loss: 0.00028869 Testing Loss: 0.00054589\n",
      "Epoch [3800/200000] Training Loss: 0.00004650 Testing Loss: 0.00004597\n",
      "Epoch [3900/200000] Training Loss: 0.00002664 Testing Loss: 0.00002801\n",
      "Epoch [4000/200000] Training Loss: 0.00002814 Testing Loss: 0.00002921\n",
      "Epoch [4100/200000] Training Loss: 0.00002569 Testing Loss: 0.00002506\n",
      "Epoch [4200/200000] Training Loss: 0.00002552 Testing Loss: 0.00002748\n",
      "Epoch [4300/200000] Training Loss: 0.00011163 Testing Loss: 0.00015130\n",
      "Epoch [4400/200000] Training Loss: 0.00008528 Testing Loss: 0.00011056\n",
      "Epoch [4500/200000] Training Loss: 0.00006600 Testing Loss: 0.00005328\n",
      "Epoch [4600/200000] Training Loss: 0.00003234 Testing Loss: 0.00004713\n",
      "Epoch [4700/200000] Training Loss: 0.00002006 Testing Loss: 0.00002037\n",
      "Epoch [4800/200000] Training Loss: 0.00002236 Testing Loss: 0.00002366\n",
      "Epoch [4900/200000] Training Loss: 0.00002639 Testing Loss: 0.00002298\n",
      "Epoch [5000/200000] Training Loss: 0.00003082 Testing Loss: 0.00006336\n",
      "Epoch [5100/200000] Training Loss: 0.00001933 Testing Loss: 0.00002500\n",
      "Epoch [5200/200000] Training Loss: 0.00002914 Testing Loss: 0.00003690\n",
      "Epoch [5300/200000] Training Loss: 0.00002220 Testing Loss: 0.00001791\n",
      "Epoch [5400/200000] Training Loss: 0.00009224 Testing Loss: 0.00007667\n",
      "Epoch [5500/200000] Training Loss: 0.00005759 Testing Loss: 0.00009743\n",
      "Epoch [5600/200000] Training Loss: 0.00002704 Testing Loss: 0.00002504\n",
      "Epoch [5700/200000] Training Loss: 0.00001611 Testing Loss: 0.00001636\n",
      "Epoch [5800/200000] Training Loss: 0.00001486 Testing Loss: 0.00001510\n",
      "Epoch [5900/200000] Training Loss: 0.00001456 Testing Loss: 0.00001477\n",
      "Epoch [6000/200000] Training Loss: 0.00022505 Testing Loss: 0.00025024\n",
      "Epoch [6100/200000] Training Loss: 0.00001468 Testing Loss: 0.00001526\n",
      "Epoch [6200/200000] Training Loss: 0.00001645 Testing Loss: 0.00001607\n",
      "Epoch [6300/200000] Training Loss: 0.00002272 Testing Loss: 0.00002175\n",
      "Epoch [6400/200000] Training Loss: 0.00001412 Testing Loss: 0.00001467\n",
      "Epoch [6500/200000] Training Loss: 0.00001477 Testing Loss: 0.00001507\n",
      "Epoch [6600/200000] Training Loss: 0.00001307 Testing Loss: 0.00001332\n",
      "Epoch [6700/200000] Training Loss: 0.00003163 Testing Loss: 0.00002925\n",
      "Epoch [6800/200000] Training Loss: 0.00001248 Testing Loss: 0.00001303\n",
      "Epoch [6900/200000] Training Loss: 0.00011937 Testing Loss: 0.00004673\n",
      "Epoch [7000/200000] Training Loss: 0.00006127 Testing Loss: 0.00009034\n",
      "Epoch [7100/200000] Training Loss: 0.00001146 Testing Loss: 0.00001177\n",
      "Epoch [7200/200000] Training Loss: 0.00006573 Testing Loss: 0.00009565\n",
      "Epoch [7300/200000] Training Loss: 0.00001075 Testing Loss: 0.00001098\n",
      "Epoch [7400/200000] Training Loss: 0.00002633 Testing Loss: 0.00002186\n",
      "Epoch [7500/200000] Training Loss: 0.00001092 Testing Loss: 0.00001104\n",
      "Epoch [7600/200000] Training Loss: 0.00001015 Testing Loss: 0.00001040\n",
      "Epoch [7700/200000] Training Loss: 0.00001006 Testing Loss: 0.00001040\n",
      "Epoch [7800/200000] Training Loss: 0.00004933 Testing Loss: 0.00005000\n",
      "Epoch [7900/200000] Training Loss: 0.00001312 Testing Loss: 0.00001346\n",
      "Epoch [8000/200000] Training Loss: 0.00001087 Testing Loss: 0.00001112\n",
      "Epoch [8100/200000] Training Loss: 0.00016872 Testing Loss: 0.00021034\n",
      "Epoch [8200/200000] Training Loss: 0.00001447 Testing Loss: 0.00001500\n",
      "Epoch [8300/200000] Training Loss: 0.00001010 Testing Loss: 0.00001141\n",
      "Epoch [8400/200000] Training Loss: 0.00001266 Testing Loss: 0.00001018\n",
      "Epoch [8500/200000] Training Loss: 0.00002381 Testing Loss: 0.00002901\n",
      "Epoch [8600/200000] Training Loss: 0.00004808 Testing Loss: 0.00003453\n",
      "Epoch [8700/200000] Training Loss: 0.00011410 Testing Loss: 0.00003763\n",
      "Epoch [8800/200000] Training Loss: 0.00000829 Testing Loss: 0.00000869\n",
      "Epoch [8900/200000] Training Loss: 0.00000797 Testing Loss: 0.00000817\n",
      "Epoch [9000/200000] Training Loss: 0.00000768 Testing Loss: 0.00000790\n",
      "Epoch [9100/200000] Training Loss: 0.00002064 Testing Loss: 0.00002761\n",
      "Epoch [9200/200000] Training Loss: 0.00003886 Testing Loss: 0.00003920\n",
      "Epoch [9300/200000] Training Loss: 0.00002242 Testing Loss: 0.00002851\n",
      "Epoch [9400/200000] Training Loss: 0.00000930 Testing Loss: 0.00001056\n",
      "Epoch [9500/200000] Training Loss: 0.00000714 Testing Loss: 0.00000741\n",
      "Epoch [9600/200000] Training Loss: 0.00000706 Testing Loss: 0.00000732\n",
      "Epoch [9700/200000] Training Loss: 0.00000685 Testing Loss: 0.00000706\n",
      "Epoch [9800/200000] Training Loss: 0.00001274 Testing Loss: 0.00001309\n",
      "Epoch [9900/200000] Training Loss: 0.00000705 Testing Loss: 0.00000745\n",
      "Epoch [10000/200000] Training Loss: 0.00000917 Testing Loss: 0.00001116\n",
      "Epoch [10100/200000] Training Loss: 0.00000754 Testing Loss: 0.00000858\n",
      "Epoch [10200/200000] Training Loss: 0.00001549 Testing Loss: 0.00002104\n",
      "Epoch [10300/200000] Training Loss: 0.00000983 Testing Loss: 0.00000788\n",
      "Epoch [10400/200000] Training Loss: 0.00000881 Testing Loss: 0.00000834\n",
      "Epoch [10500/200000] Training Loss: 0.00000971 Testing Loss: 0.00000817\n",
      "Epoch [10600/200000] Training Loss: 0.00002895 Testing Loss: 0.00003143\n",
      "Epoch [10700/200000] Training Loss: 0.00002883 Testing Loss: 0.00002371\n",
      "Epoch [10800/200000] Training Loss: 0.00000804 Testing Loss: 0.00000908\n",
      "Epoch [10900/200000] Training Loss: 0.00000603 Testing Loss: 0.00000625\n",
      "Epoch [11000/200000] Training Loss: 0.00003643 Testing Loss: 0.00002121\n",
      "Epoch [11100/200000] Training Loss: 0.00000558 Testing Loss: 0.00000580\n",
      "Epoch [11200/200000] Training Loss: 0.00000682 Testing Loss: 0.00000707\n",
      "Epoch [11300/200000] Training Loss: 0.00001069 Testing Loss: 0.00000863\n",
      "Epoch [11400/200000] Training Loss: 0.00014727 Testing Loss: 0.00011192\n",
      "Epoch [11500/200000] Training Loss: 0.00000554 Testing Loss: 0.00000611\n",
      "Epoch [11600/200000] Training Loss: 0.00002711 Testing Loss: 0.00003019\n",
      "Epoch [11700/200000] Training Loss: 0.00000497 Testing Loss: 0.00000518\n",
      "Epoch [11800/200000] Training Loss: 0.00000486 Testing Loss: 0.00000504\n",
      "Epoch [11900/200000] Training Loss: 0.00001140 Testing Loss: 0.00000904\n",
      "Epoch [12000/200000] Training Loss: 0.00001699 Testing Loss: 0.00002404\n",
      "Epoch [12100/200000] Training Loss: 0.00000454 Testing Loss: 0.00000472\n",
      "Epoch [12200/200000] Training Loss: 0.00000542 Testing Loss: 0.00000554\n",
      "Epoch [12300/200000] Training Loss: 0.00000506 Testing Loss: 0.00000521\n",
      "Epoch [12400/200000] Training Loss: 0.00009886 Testing Loss: 0.00007147\n",
      "Epoch [12500/200000] Training Loss: 0.00000668 Testing Loss: 0.00000716\n",
      "Epoch [12600/200000] Training Loss: 0.00000532 Testing Loss: 0.00000549\n",
      "Epoch [12700/200000] Training Loss: 0.00000452 Testing Loss: 0.00000471\n",
      "Epoch [12800/200000] Training Loss: 0.00001597 Testing Loss: 0.00000991\n",
      "Epoch [12900/200000] Training Loss: 0.00000902 Testing Loss: 0.00000743\n",
      "Epoch [13000/200000] Training Loss: 0.00000410 Testing Loss: 0.00000429\n",
      "Epoch [13100/200000] Training Loss: 0.00000467 Testing Loss: 0.00000488\n",
      "Epoch [13200/200000] Training Loss: 0.00000490 Testing Loss: 0.00000520\n",
      "Epoch [13300/200000] Training Loss: 0.00000400 Testing Loss: 0.00000418\n",
      "Epoch [13400/200000] Training Loss: 0.00008323 Testing Loss: 0.00002868\n",
      "Epoch [13500/200000] Training Loss: 0.00000379 Testing Loss: 0.00000397\n",
      "Epoch [13600/200000] Training Loss: 0.00000364 Testing Loss: 0.00000381\n",
      "Epoch [13700/200000] Training Loss: 0.00000452 Testing Loss: 0.00000482\n",
      "Epoch [13800/200000] Training Loss: 0.00003565 Testing Loss: 0.00002805\n",
      "Epoch [13900/200000] Training Loss: 0.00000359 Testing Loss: 0.00000376\n",
      "Epoch [14000/200000] Training Loss: 0.00001290 Testing Loss: 0.00001276\n",
      "Epoch [14100/200000] Training Loss: 0.00000369 Testing Loss: 0.00000387\n",
      "Epoch [14200/200000] Training Loss: 0.00001553 Testing Loss: 0.00001186\n",
      "Epoch [14300/200000] Training Loss: 0.00000347 Testing Loss: 0.00000364\n",
      "Epoch [14400/200000] Training Loss: 0.00002633 Testing Loss: 0.00001769\n",
      "Epoch [14500/200000] Training Loss: 0.00000653 Testing Loss: 0.00000589\n",
      "Epoch [14600/200000] Training Loss: 0.00001396 Testing Loss: 0.00001405\n",
      "Epoch [14700/200000] Training Loss: 0.00003484 Testing Loss: 0.00009125\n",
      "Epoch [14800/200000] Training Loss: 0.00000325 Testing Loss: 0.00000341\n",
      "Epoch [14900/200000] Training Loss: 0.00000322 Testing Loss: 0.00000345\n",
      "Epoch [15000/200000] Training Loss: 0.00000790 Testing Loss: 0.00001228\n",
      "Epoch [15100/200000] Training Loss: 0.00001740 Testing Loss: 0.00001017\n",
      "Epoch [15200/200000] Training Loss: 0.00000316 Testing Loss: 0.00000333\n",
      "Epoch [15300/200000] Training Loss: 0.00000298 Testing Loss: 0.00000314\n",
      "Epoch [15400/200000] Training Loss: 0.00000289 Testing Loss: 0.00000304\n",
      "Epoch [15500/200000] Training Loss: 0.00000286 Testing Loss: 0.00000305\n",
      "Epoch [15600/200000] Training Loss: 0.00002855 Testing Loss: 0.00007821\n",
      "Epoch [15700/200000] Training Loss: 0.00000793 Testing Loss: 0.00001345\n",
      "Epoch [15800/200000] Training Loss: 0.00000294 Testing Loss: 0.00000310\n",
      "Epoch [15900/200000] Training Loss: 0.00000276 Testing Loss: 0.00000292\n",
      "Epoch [16000/200000] Training Loss: 0.00000266 Testing Loss: 0.00000280\n",
      "Epoch [16100/200000] Training Loss: 0.00000259 Testing Loss: 0.00000272\n",
      "Epoch [16200/200000] Training Loss: 0.00000282 Testing Loss: 0.00000298\n",
      "Epoch [16300/200000] Training Loss: 0.00000258 Testing Loss: 0.00000272\n",
      "Epoch [16400/200000] Training Loss: 0.00000249 Testing Loss: 0.00000263\n",
      "Epoch [16500/200000] Training Loss: 0.00000805 Testing Loss: 0.00000856\n",
      "Epoch [16600/200000] Training Loss: 0.00000279 Testing Loss: 0.00000293\n",
      "Epoch [16700/200000] Training Loss: 0.00001288 Testing Loss: 0.00001063\n",
      "Epoch [16800/200000] Training Loss: 0.00000241 Testing Loss: 0.00000254\n",
      "Epoch [16900/200000] Training Loss: 0.00001862 Testing Loss: 0.00002401\n",
      "Epoch [17000/200000] Training Loss: 0.00000264 Testing Loss: 0.00000279\n",
      "Epoch [17100/200000] Training Loss: 0.00000241 Testing Loss: 0.00000254\n",
      "Epoch [17200/200000] Training Loss: 0.00000233 Testing Loss: 0.00000245\n",
      "Epoch [17300/200000] Training Loss: 0.00002292 Testing Loss: 0.00004727\n",
      "Epoch [17400/200000] Training Loss: 0.00000236 Testing Loss: 0.00000249\n",
      "Epoch [17500/200000] Training Loss: 0.00001510 Testing Loss: 0.00002300\n",
      "Epoch [17600/200000] Training Loss: 0.00000238 Testing Loss: 0.00000253\n",
      "Epoch [17700/200000] Training Loss: 0.00000222 Testing Loss: 0.00000234\n",
      "Epoch [17800/200000] Training Loss: 0.00001201 Testing Loss: 0.00000992\n",
      "Epoch [17900/200000] Training Loss: 0.00000220 Testing Loss: 0.00000232\n",
      "Epoch [18000/200000] Training Loss: 0.00001053 Testing Loss: 0.00000956\n",
      "Epoch [18100/200000] Training Loss: 0.00000217 Testing Loss: 0.00000229\n",
      "Epoch [18200/200000] Training Loss: 0.00000208 Testing Loss: 0.00000220\n",
      "Epoch [18300/200000] Training Loss: 0.00001046 Testing Loss: 0.00000927\n",
      "Epoch [18400/200000] Training Loss: 0.00000242 Testing Loss: 0.00000254\n",
      "Epoch [18500/200000] Training Loss: 0.00000217 Testing Loss: 0.00000228\n",
      "Epoch [18600/200000] Training Loss: 0.00000210 Testing Loss: 0.00000222\n",
      "Epoch [18700/200000] Training Loss: 0.00000784 Testing Loss: 0.00000943\n",
      "Epoch [18800/200000] Training Loss: 0.00000198 Testing Loss: 0.00000209\n",
      "Epoch [18900/200000] Training Loss: 0.00001254 Testing Loss: 0.00001094\n",
      "Epoch [19000/200000] Training Loss: 0.00000206 Testing Loss: 0.00000214\n",
      "Epoch [19100/200000] Training Loss: 0.00000194 Testing Loss: 0.00000204\n",
      "Epoch [19200/200000] Training Loss: 0.00000190 Testing Loss: 0.00000200\n",
      "Epoch [19300/200000] Training Loss: 0.00008294 Testing Loss: 0.00004069\n",
      "Epoch [19400/200000] Training Loss: 0.00000248 Testing Loss: 0.00000260\n",
      "Epoch [19500/200000] Training Loss: 0.00000211 Testing Loss: 0.00000222\n",
      "Epoch [19600/200000] Training Loss: 0.00000196 Testing Loss: 0.00000207\n",
      "Epoch [19700/200000] Training Loss: 0.00001122 Testing Loss: 0.00001057\n",
      "Epoch [19800/200000] Training Loss: 0.00000232 Testing Loss: 0.00000248\n",
      "Epoch [19900/200000] Training Loss: 0.00000433 Testing Loss: 0.00000352\n",
      "Epoch [20000/200000] Training Loss: 0.00000186 Testing Loss: 0.00000196\n",
      "Epoch [20100/200000] Training Loss: 0.00000359 Testing Loss: 0.00000511\n",
      "Epoch [20200/200000] Training Loss: 0.00000196 Testing Loss: 0.00000207\n",
      "Epoch [20300/200000] Training Loss: 0.00000183 Testing Loss: 0.00000194\n",
      "Epoch [20400/200000] Training Loss: 0.00000294 Testing Loss: 0.00000313\n",
      "Epoch [20500/200000] Training Loss: 0.00001171 Testing Loss: 0.00001775\n",
      "Epoch [20600/200000] Training Loss: 0.00000180 Testing Loss: 0.00000190\n",
      "Epoch [20700/200000] Training Loss: 0.00000176 Testing Loss: 0.00000186\n",
      "Epoch [20800/200000] Training Loss: 0.00000196 Testing Loss: 0.00000207\n",
      "Epoch [20900/200000] Training Loss: 0.00000178 Testing Loss: 0.00000188\n",
      "Epoch [21000/200000] Training Loss: 0.00000347 Testing Loss: 0.00000398\n",
      "Epoch [21100/200000] Training Loss: 0.00000177 Testing Loss: 0.00000187\n",
      "Epoch [21200/200000] Training Loss: 0.00000172 Testing Loss: 0.00000181\n",
      "Epoch [21300/200000] Training Loss: 0.00000299 Testing Loss: 0.00000297\n",
      "Epoch [21400/200000] Training Loss: 0.00000177 Testing Loss: 0.00000187\n",
      "Epoch [21500/200000] Training Loss: 0.00000267 Testing Loss: 0.00000232\n",
      "Epoch [21600/200000] Training Loss: 0.00000167 Testing Loss: 0.00000177\n",
      "Epoch [21700/200000] Training Loss: 0.00004998 Testing Loss: 0.00003923\n",
      "Epoch [21800/200000] Training Loss: 0.00000329 Testing Loss: 0.00000434\n",
      "Epoch [21900/200000] Training Loss: 0.00000166 Testing Loss: 0.00000175\n",
      "Epoch [22000/200000] Training Loss: 0.00000163 Testing Loss: 0.00000172\n",
      "Epoch [22100/200000] Training Loss: 0.00001035 Testing Loss: 0.00001107\n",
      "Epoch [22200/200000] Training Loss: 0.00000191 Testing Loss: 0.00000202\n",
      "Epoch [22300/200000] Training Loss: 0.00000169 Testing Loss: 0.00000179\n",
      "Epoch [22400/200000] Training Loss: 0.00000163 Testing Loss: 0.00000173\n",
      "Epoch [22500/200000] Training Loss: 0.00000160 Testing Loss: 0.00000169\n",
      "Epoch [22600/200000] Training Loss: 0.00004447 Testing Loss: 0.00002939\n",
      "Epoch [22700/200000] Training Loss: 0.00000429 Testing Loss: 0.00000440\n",
      "Epoch [22800/200000] Training Loss: 0.00000317 Testing Loss: 0.00000329\n",
      "Epoch [22900/200000] Training Loss: 0.00000282 Testing Loss: 0.00000294\n",
      "Epoch [23000/200000] Training Loss: 0.00000252 Testing Loss: 0.00000260\n",
      "Epoch [23100/200000] Training Loss: 0.00000206 Testing Loss: 0.00000219\n",
      "Epoch [23200/200000] Training Loss: 0.00000204 Testing Loss: 0.00000215\n",
      "Epoch [23300/200000] Training Loss: 0.00019302 Testing Loss: 0.00016386\n",
      "Epoch [23400/200000] Training Loss: 0.00000282 Testing Loss: 0.00000293\n",
      "Epoch [23500/200000] Training Loss: 0.00000238 Testing Loss: 0.00000250\n",
      "Epoch [23600/200000] Training Loss: 0.00000218 Testing Loss: 0.00000230\n",
      "Epoch [23700/200000] Training Loss: 0.00000206 Testing Loss: 0.00000219\n",
      "Epoch [23800/200000] Training Loss: 0.00000200 Testing Loss: 0.00000213\n",
      "Epoch [23900/200000] Training Loss: 0.00001411 Testing Loss: 0.00001767\n",
      "Epoch [24000/200000] Training Loss: 0.00000178 Testing Loss: 0.00000190\n",
      "Epoch [24100/200000] Training Loss: 0.00001075 Testing Loss: 0.00001374\n",
      "Epoch [24200/200000] Training Loss: 0.00000198 Testing Loss: 0.00000209\n",
      "Epoch [24300/200000] Training Loss: 0.00000201 Testing Loss: 0.00000223\n",
      "Epoch [24400/200000] Training Loss: 0.00000170 Testing Loss: 0.00000181\n",
      "Epoch [24500/200000] Training Loss: 0.00000163 Testing Loss: 0.00000174\n",
      "Epoch [24600/200000] Training Loss: 0.00000421 Testing Loss: 0.00000438\n",
      "Epoch [24700/200000] Training Loss: 0.00000194 Testing Loss: 0.00000205\n",
      "Epoch [24800/200000] Training Loss: 0.00000625 Testing Loss: 0.00000561\n",
      "Epoch [24900/200000] Training Loss: 0.00000264 Testing Loss: 0.00000269\n",
      "Epoch [25000/200000] Training Loss: 0.00000187 Testing Loss: 0.00000198\n",
      "Epoch [25100/200000] Training Loss: 0.00000253 Testing Loss: 0.00000212\n",
      "Epoch [25200/200000] Training Loss: 0.00000165 Testing Loss: 0.00000175\n",
      "Epoch [25300/200000] Training Loss: 0.00000161 Testing Loss: 0.00000170\n",
      "Epoch [25400/200000] Training Loss: 0.00000198 Testing Loss: 0.00000209\n",
      "Epoch [25500/200000] Training Loss: 0.00001635 Testing Loss: 0.00002130\n",
      "Epoch [25600/200000] Training Loss: 0.00000161 Testing Loss: 0.00000171\n",
      "Epoch [25700/200000] Training Loss: 0.00000159 Testing Loss: 0.00000170\n",
      "Epoch [25800/200000] Training Loss: 0.00000415 Testing Loss: 0.00000378\n",
      "Epoch [25900/200000] Training Loss: 0.00000155 Testing Loss: 0.00000165\n",
      "Epoch [26000/200000] Training Loss: 0.00000153 Testing Loss: 0.00000162\n",
      "Epoch [26100/200000] Training Loss: 0.00002082 Testing Loss: 0.00002452\n",
      "Epoch [26200/200000] Training Loss: 0.00000201 Testing Loss: 0.00000211\n",
      "Epoch [26300/200000] Training Loss: 0.00000173 Testing Loss: 0.00000184\n",
      "Epoch [26400/200000] Training Loss: 0.00000171 Testing Loss: 0.00000180\n",
      "Epoch [26500/200000] Training Loss: 0.00000153 Testing Loss: 0.00000163\n",
      "Epoch [26600/200000] Training Loss: 0.00000150 Testing Loss: 0.00000159\n",
      "Epoch [26700/200000] Training Loss: 0.00000181 Testing Loss: 0.00000191\n",
      "Epoch [26800/200000] Training Loss: 0.00001800 Testing Loss: 0.00002234\n",
      "Epoch [26900/200000] Training Loss: 0.00000151 Testing Loss: 0.00000160\n",
      "Epoch [27000/200000] Training Loss: 0.00000149 Testing Loss: 0.00000158\n",
      "Epoch [27100/200000] Training Loss: 0.00002442 Testing Loss: 0.00002423\n",
      "Epoch [27200/200000] Training Loss: 0.00001742 Testing Loss: 0.00001659\n",
      "Epoch [27300/200000] Training Loss: 0.00000525 Testing Loss: 0.00000536\n",
      "Epoch [27400/200000] Training Loss: 0.00000407 Testing Loss: 0.00000420\n",
      "Epoch [27500/200000] Training Loss: 0.00000508 Testing Loss: 0.00000524\n",
      "Epoch [27600/200000] Training Loss: 0.00000363 Testing Loss: 0.00000375\n",
      "Epoch [27700/200000] Training Loss: 0.00000313 Testing Loss: 0.00000325\n",
      "Epoch [27800/200000] Training Loss: 0.00000281 Testing Loss: 0.00000294\n",
      "Epoch [27900/200000] Training Loss: 0.00000258 Testing Loss: 0.00000271\n",
      "Epoch [28000/200000] Training Loss: 0.00000241 Testing Loss: 0.00000254\n",
      "Epoch [28100/200000] Training Loss: 0.00000230 Testing Loss: 0.00000243\n",
      "Epoch [28200/200000] Training Loss: 0.00000361 Testing Loss: 0.00000365\n",
      "Epoch [28300/200000] Training Loss: 0.00000242 Testing Loss: 0.00000254\n",
      "Epoch [28400/200000] Training Loss: 0.00000221 Testing Loss: 0.00000233\n",
      "Epoch [28500/200000] Training Loss: 0.00000208 Testing Loss: 0.00000220\n",
      "Epoch [28600/200000] Training Loss: 0.00000209 Testing Loss: 0.00000222\n",
      "Epoch [28700/200000] Training Loss: 0.00001517 Testing Loss: 0.00002395\n",
      "Epoch [28800/200000] Training Loss: 0.00000203 Testing Loss: 0.00000214\n",
      "Epoch [28900/200000] Training Loss: 0.00000189 Testing Loss: 0.00000200\n",
      "Epoch [29000/200000] Training Loss: 0.00000658 Testing Loss: 0.00000682\n",
      "Epoch [29100/200000] Training Loss: 0.00000199 Testing Loss: 0.00000210\n",
      "Epoch [29200/200000] Training Loss: 0.00000184 Testing Loss: 0.00000195\n",
      "Epoch [29300/200000] Training Loss: 0.00000550 Testing Loss: 0.00000534\n",
      "Epoch [29400/200000] Training Loss: 0.00000187 Testing Loss: 0.00000198\n",
      "Epoch [29500/200000] Training Loss: 0.00000974 Testing Loss: 0.00002146\n",
      "Epoch [29600/200000] Training Loss: 0.00000179 Testing Loss: 0.00000190\n",
      "Epoch [29700/200000] Training Loss: 0.00000171 Testing Loss: 0.00000182\n",
      "Epoch [29800/200000] Training Loss: 0.00000171 Testing Loss: 0.00000180\n",
      "Epoch [29900/200000] Training Loss: 0.00000182 Testing Loss: 0.00000193\n",
      "Epoch [30000/200000] Training Loss: 0.00000174 Testing Loss: 0.00000187\n",
      "Epoch [30100/200000] Training Loss: 0.00000177 Testing Loss: 0.00000187\n",
      "Epoch [30200/200000] Training Loss: 0.00000163 Testing Loss: 0.00000173\n",
      "Epoch [30300/200000] Training Loss: 0.00000180 Testing Loss: 0.00000188\n",
      "Epoch [30400/200000] Training Loss: 0.00000160 Testing Loss: 0.00000170\n",
      "Epoch [30500/200000] Training Loss: 0.00000189 Testing Loss: 0.00000203\n",
      "Epoch [30600/200000] Training Loss: 0.00000159 Testing Loss: 0.00000169\n",
      "Epoch [30700/200000] Training Loss: 0.00000691 Testing Loss: 0.00000709\n",
      "Epoch [30800/200000] Training Loss: 0.00000159 Testing Loss: 0.00000168\n",
      "Epoch [30900/200000] Training Loss: 0.00000154 Testing Loss: 0.00000163\n",
      "Epoch [31000/200000] Training Loss: 0.00000294 Testing Loss: 0.00000291\n",
      "Epoch [31100/200000] Training Loss: 0.00000159 Testing Loss: 0.00000168\n",
      "Epoch [31200/200000] Training Loss: 0.00000153 Testing Loss: 0.00000163\n",
      "Epoch [31300/200000] Training Loss: 0.00000155 Testing Loss: 0.00000165\n",
      "Epoch [31400/200000] Training Loss: 0.00001942 Testing Loss: 0.00002412\n",
      "Epoch 31492: reducing learning rate of group 0 to 7.0000e-04.\n",
      "Epoch [31500/200000] Training Loss: 0.00000203 Testing Loss: 0.00000299\n",
      "Epoch [31600/200000] Training Loss: 0.00000149 Testing Loss: 0.00000158\n",
      "Epoch [31700/200000] Training Loss: 0.00000148 Testing Loss: 0.00000156\n",
      "Epoch [31800/200000] Training Loss: 0.00000147 Testing Loss: 0.00000155\n",
      "Epoch [31900/200000] Training Loss: 0.00000146 Testing Loss: 0.00000154\n",
      "Epoch [32000/200000] Training Loss: 0.00000145 Testing Loss: 0.00000153\n",
      "Epoch [32100/200000] Training Loss: 0.00000144 Testing Loss: 0.00000153\n",
      "Epoch [32200/200000] Training Loss: 0.00000193 Testing Loss: 0.00000235\n",
      "Epoch [32300/200000] Training Loss: 0.00000146 Testing Loss: 0.00000154\n",
      "Epoch [32400/200000] Training Loss: 0.00000144 Testing Loss: 0.00000152\n",
      "Epoch [32500/200000] Training Loss: 0.00000154 Testing Loss: 0.00000161\n",
      "Epoch [32600/200000] Training Loss: 0.00000144 Testing Loss: 0.00000152\n",
      "Epoch [32700/200000] Training Loss: 0.00000148 Testing Loss: 0.00000160\n",
      "Epoch [32800/200000] Training Loss: 0.00000145 Testing Loss: 0.00000153\n",
      "Epoch [32900/200000] Training Loss: 0.00000142 Testing Loss: 0.00000150\n",
      "Epoch [33000/200000] Training Loss: 0.00000214 Testing Loss: 0.00000202\n",
      "Epoch [33100/200000] Training Loss: 0.00000142 Testing Loss: 0.00000150\n",
      "Epoch [33200/200000] Training Loss: 0.00000141 Testing Loss: 0.00000149\n",
      "Epoch [33300/200000] Training Loss: 0.00000158 Testing Loss: 0.00000166\n",
      "Epoch [33400/200000] Training Loss: 0.00000141 Testing Loss: 0.00000149\n",
      "Epoch [33500/200000] Training Loss: 0.00000140 Testing Loss: 0.00000148\n",
      "Epoch [33600/200000] Training Loss: 0.00000146 Testing Loss: 0.00000154\n",
      "Epoch [33700/200000] Training Loss: 0.00000140 Testing Loss: 0.00000148\n",
      "Epoch [33800/200000] Training Loss: 0.00000139 Testing Loss: 0.00000147\n",
      "Epoch [33900/200000] Training Loss: 0.00000144 Testing Loss: 0.00000153\n",
      "Epoch [34000/200000] Training Loss: 0.00000139 Testing Loss: 0.00000147\n",
      "Epoch [34100/200000] Training Loss: 0.00000629 Testing Loss: 0.00002228\n",
      "Epoch [34200/200000] Training Loss: 0.00000139 Testing Loss: 0.00000147\n",
      "Epoch [34300/200000] Training Loss: 0.00000138 Testing Loss: 0.00000145\n",
      "Epoch [34400/200000] Training Loss: 0.00001056 Testing Loss: 0.00000691\n",
      "Epoch [34500/200000] Training Loss: 0.00000138 Testing Loss: 0.00000146\n",
      "Epoch [34600/200000] Training Loss: 0.00000137 Testing Loss: 0.00000145\n",
      "Epoch [34700/200000] Training Loss: 0.00003898 Testing Loss: 0.00005559\n",
      "Epoch [34800/200000] Training Loss: 0.00000143 Testing Loss: 0.00000151\n",
      "Epoch [34900/200000] Training Loss: 0.00000138 Testing Loss: 0.00000145\n",
      "Epoch [35000/200000] Training Loss: 0.00000136 Testing Loss: 0.00000144\n",
      "Epoch [35100/200000] Training Loss: 0.00000825 Testing Loss: 0.00001317\n",
      "Epoch [35200/200000] Training Loss: 0.00000137 Testing Loss: 0.00000144\n",
      "Epoch [35300/200000] Training Loss: 0.00000135 Testing Loss: 0.00000143\n",
      "Epoch [35400/200000] Training Loss: 0.00000165 Testing Loss: 0.00000179\n",
      "Epoch [35500/200000] Training Loss: 0.00000135 Testing Loss: 0.00000143\n",
      "Epoch [35600/200000] Training Loss: 0.00001698 Testing Loss: 0.00002075\n",
      "Epoch [35700/200000] Training Loss: 0.00000135 Testing Loss: 0.00000143\n",
      "Epoch [35800/200000] Training Loss: 0.00000134 Testing Loss: 0.00000142\n",
      "Epoch [35900/200000] Training Loss: 0.00000494 Testing Loss: 0.00000365\n",
      "Epoch [36000/200000] Training Loss: 0.00000134 Testing Loss: 0.00000142\n",
      "Epoch [36100/200000] Training Loss: 0.00000133 Testing Loss: 0.00000141\n",
      "Epoch [36200/200000] Training Loss: 0.00001533 Testing Loss: 0.00002578\n",
      "Epoch [36300/200000] Training Loss: 0.00000135 Testing Loss: 0.00000143\n",
      "Epoch [36400/200000] Training Loss: 0.00000133 Testing Loss: 0.00000141\n",
      "Epoch [36500/200000] Training Loss: 0.00000147 Testing Loss: 0.00000165\n",
      "Epoch [36600/200000] Training Loss: 0.00000134 Testing Loss: 0.00000142\n",
      "Epoch [36700/200000] Training Loss: 0.00000132 Testing Loss: 0.00000140\n",
      "Epoch [36800/200000] Training Loss: 0.00002006 Testing Loss: 0.00002815\n",
      "Epoch [36900/200000] Training Loss: 0.00000132 Testing Loss: 0.00000140\n",
      "Epoch [37000/200000] Training Loss: 0.00000156 Testing Loss: 0.00000152\n",
      "Epoch [37100/200000] Training Loss: 0.00000132 Testing Loss: 0.00000139\n",
      "Epoch [37200/200000] Training Loss: 0.00000135 Testing Loss: 0.00000143\n",
      "Epoch [37300/200000] Training Loss: 0.00000131 Testing Loss: 0.00000139\n",
      "Epoch [37400/200000] Training Loss: 0.00000164 Testing Loss: 0.00000170\n",
      "Epoch [37500/200000] Training Loss: 0.00000134 Testing Loss: 0.00000141\n",
      "Epoch [37600/200000] Training Loss: 0.00000132 Testing Loss: 0.00000139\n",
      "Epoch [37700/200000] Training Loss: 0.00000131 Testing Loss: 0.00000138\n",
      "Epoch [37800/200000] Training Loss: 0.00002704 Testing Loss: 0.00003894\n",
      "Epoch [37900/200000] Training Loss: 0.00000131 Testing Loss: 0.00000138\n",
      "Epoch [38000/200000] Training Loss: 0.00000367 Testing Loss: 0.00000513\n",
      "Epoch [38100/200000] Training Loss: 0.00000131 Testing Loss: 0.00000138\n",
      "Epoch [38200/200000] Training Loss: 0.00000130 Testing Loss: 0.00000137\n",
      "Epoch [38300/200000] Training Loss: 0.00000134 Testing Loss: 0.00000141\n",
      "Epoch [38400/200000] Training Loss: 0.00000130 Testing Loss: 0.00000137\n",
      "Epoch [38500/200000] Training Loss: 0.00000129 Testing Loss: 0.00000136\n",
      "Epoch [38600/200000] Training Loss: 0.00000151 Testing Loss: 0.00000159\n",
      "Epoch [38700/200000] Training Loss: 0.00000129 Testing Loss: 0.00000137\n",
      "Epoch [38800/200000] Training Loss: 0.00000129 Testing Loss: 0.00000136\n",
      "Epoch [38900/200000] Training Loss: 0.00000366 Testing Loss: 0.00000375\n",
      "Epoch [39000/200000] Training Loss: 0.00000129 Testing Loss: 0.00000136\n",
      "Epoch [39100/200000] Training Loss: 0.00003318 Testing Loss: 0.00001712\n",
      "Epoch [39200/200000] Training Loss: 0.00000131 Testing Loss: 0.00000138\n",
      "Epoch [39300/200000] Training Loss: 0.00000129 Testing Loss: 0.00000136\n",
      "Epoch [39400/200000] Training Loss: 0.00000128 Testing Loss: 0.00000135\n",
      "Epoch [39500/200000] Training Loss: 0.00000155 Testing Loss: 0.00000177\n",
      "Epoch [39600/200000] Training Loss: 0.00000240 Testing Loss: 0.00000242\n",
      "Epoch [39700/200000] Training Loss: 0.00000481 Testing Loss: 0.00000329\n",
      "Epoch [39800/200000] Training Loss: 0.00000128 Testing Loss: 0.00000135\n",
      "Epoch [39900/200000] Training Loss: 0.00000251 Testing Loss: 0.00000346\n",
      "Epoch [40000/200000] Training Loss: 0.00000132 Testing Loss: 0.00000140\n",
      "Epoch [40100/200000] Training Loss: 0.00000128 Testing Loss: 0.00000135\n",
      "Epoch [40200/200000] Training Loss: 0.00000127 Testing Loss: 0.00000134\n",
      "Epoch [40300/200000] Training Loss: 0.00000128 Testing Loss: 0.00000136\n",
      "Epoch [40400/200000] Training Loss: 0.00000127 Testing Loss: 0.00000134\n",
      "Epoch [40500/200000] Training Loss: 0.00000155 Testing Loss: 0.00000160\n",
      "Epoch [40600/200000] Training Loss: 0.00000133 Testing Loss: 0.00000144\n",
      "Epoch [40700/200000] Training Loss: 0.00000127 Testing Loss: 0.00000134\n",
      "Epoch [40800/200000] Training Loss: 0.00000137 Testing Loss: 0.00000142\n",
      "Epoch [40900/200000] Training Loss: 0.00000247 Testing Loss: 0.00000203\n",
      "Epoch [41000/200000] Training Loss: 0.00000126 Testing Loss: 0.00000133\n",
      "Epoch [41100/200000] Training Loss: 0.00000157 Testing Loss: 0.00000165\n",
      "Epoch [41200/200000] Training Loss: 0.00000130 Testing Loss: 0.00000136\n",
      "Epoch [41300/200000] Training Loss: 0.00003193 Testing Loss: 0.00003120\n",
      "Epoch [41400/200000] Training Loss: 0.00000128 Testing Loss: 0.00000135\n",
      "Epoch [41500/200000] Training Loss: 0.00000126 Testing Loss: 0.00000133\n",
      "Epoch [41600/200000] Training Loss: 0.00000125 Testing Loss: 0.00000132\n",
      "Epoch [41700/200000] Training Loss: 0.00000125 Testing Loss: 0.00000132\n",
      "Epoch [41800/200000] Training Loss: 0.00000142 Testing Loss: 0.00000145\n",
      "Epoch [41900/200000] Training Loss: 0.00000130 Testing Loss: 0.00000136\n",
      "Epoch [42000/200000] Training Loss: 0.00000149 Testing Loss: 0.00000160\n",
      "Epoch [42100/200000] Training Loss: 0.00000125 Testing Loss: 0.00000132\n",
      "Epoch [42200/200000] Training Loss: 0.00000127 Testing Loss: 0.00000135\n",
      "Epoch [42300/200000] Training Loss: 0.00000125 Testing Loss: 0.00000133\n",
      "Epoch [42400/200000] Training Loss: 0.00000127 Testing Loss: 0.00000134\n",
      "Epoch [42500/200000] Training Loss: 0.00000125 Testing Loss: 0.00000132\n",
      "Epoch [42600/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [42700/200000] Training Loss: 0.00000126 Testing Loss: 0.00000134\n",
      "Epoch [42800/200000] Training Loss: 0.00000446 Testing Loss: 0.00000402\n",
      "Epoch [42900/200000] Training Loss: 0.00000125 Testing Loss: 0.00000133\n",
      "Epoch [43000/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [43100/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [43200/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [43300/200000] Training Loss: 0.00000390 Testing Loss: 0.00000333\n",
      "Epoch [43400/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [43500/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [43600/200000] Training Loss: 0.00000417 Testing Loss: 0.00000415\n",
      "Epoch [43700/200000] Training Loss: 0.00000137 Testing Loss: 0.00000149\n",
      "Epoch [43800/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [43900/200000] Training Loss: 0.00000256 Testing Loss: 0.00000222\n",
      "Epoch [44000/200000] Training Loss: 0.00006911 Testing Loss: 0.00002877\n",
      "Epoch [44100/200000] Training Loss: 0.00000125 Testing Loss: 0.00000132\n",
      "Epoch [44200/200000] Training Loss: 0.00000123 Testing Loss: 0.00000130\n",
      "Epoch [44300/200000] Training Loss: 0.00000123 Testing Loss: 0.00000130\n",
      "Epoch [44400/200000] Training Loss: 0.00000123 Testing Loss: 0.00000129\n",
      "Epoch [44500/200000] Training Loss: 0.00005495 Testing Loss: 0.00001740\n",
      "Epoch [44600/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [44700/200000] Training Loss: 0.00000123 Testing Loss: 0.00000130\n",
      "Epoch [44800/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [44900/200000] Training Loss: 0.00000126 Testing Loss: 0.00000133\n",
      "Epoch [45000/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [45100/200000] Training Loss: 0.00000885 Testing Loss: 0.00001298\n",
      "Epoch [45200/200000] Training Loss: 0.00000123 Testing Loss: 0.00000130\n",
      "Epoch [45300/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [45400/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [45500/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [45600/200000] Training Loss: 0.00000125 Testing Loss: 0.00000132\n",
      "Epoch [45700/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [45800/200000] Training Loss: 0.00000122 Testing Loss: 0.00000128\n",
      "Epoch [45900/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [46000/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [46100/200000] Training Loss: 0.00000148 Testing Loss: 0.00000160\n",
      "Epoch [46200/200000] Training Loss: 0.00000150 Testing Loss: 0.00000164\n",
      "Epoch [46300/200000] Training Loss: 0.00000802 Testing Loss: 0.00001184\n",
      "Epoch [46400/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [46500/200000] Training Loss: 0.00008223 Testing Loss: 0.00002206\n",
      "Epoch [46600/200000] Training Loss: 0.00000123 Testing Loss: 0.00000130\n",
      "Epoch [46700/200000] Training Loss: 0.00000122 Testing Loss: 0.00000128\n",
      "Epoch [46800/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [46900/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [47000/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [47100/200000] Training Loss: 0.00000123 Testing Loss: 0.00000130\n",
      "Epoch [47200/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [47300/200000] Training Loss: 0.00000156 Testing Loss: 0.00000159\n",
      "Epoch [47400/200000] Training Loss: 0.00000228 Testing Loss: 0.00000236\n",
      "Epoch [47500/200000] Training Loss: 0.00000168 Testing Loss: 0.00000153\n",
      "Epoch [47600/200000] Training Loss: 0.00000157 Testing Loss: 0.00000164\n",
      "Epoch [47700/200000] Training Loss: 0.00001159 Testing Loss: 0.00001691\n",
      "Epoch [47800/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [47900/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [48000/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [48100/200000] Training Loss: 0.00000261 Testing Loss: 0.00000358\n",
      "Epoch [48200/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [48300/200000] Training Loss: 0.00000322 Testing Loss: 0.00000347\n",
      "Epoch [48400/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [48500/200000] Training Loss: 0.00000147 Testing Loss: 0.00000173\n",
      "Epoch [48600/200000] Training Loss: 0.00000125 Testing Loss: 0.00000132\n",
      "Epoch [48700/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [48800/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [48900/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [49000/200000] Training Loss: 0.00000421 Testing Loss: 0.00000285\n",
      "Epoch [49100/200000] Training Loss: 0.00000188 Testing Loss: 0.00000226\n",
      "Epoch [49200/200000] Training Loss: 0.00000178 Testing Loss: 0.00000205\n",
      "Epoch [49300/200000] Training Loss: 0.00000147 Testing Loss: 0.00000193\n",
      "Epoch [49400/200000] Training Loss: 0.00000165 Testing Loss: 0.00000172\n",
      "Epoch [49500/200000] Training Loss: 0.00000134 Testing Loss: 0.00000141\n",
      "Epoch [49600/200000] Training Loss: 0.00000127 Testing Loss: 0.00000134\n",
      "Epoch [49700/200000] Training Loss: 0.00000124 Testing Loss: 0.00000131\n",
      "Epoch [49800/200000] Training Loss: 0.00000127 Testing Loss: 0.00000131\n",
      "Epoch [49900/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [50000/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [50100/200000] Training Loss: 0.00000205 Testing Loss: 0.00000250\n",
      "Epoch [50200/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [50300/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [50400/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [50500/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [50600/200000] Training Loss: 0.00000142 Testing Loss: 0.00000145\n",
      "Epoch [50700/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [50800/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [50900/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [51000/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [51100/200000] Training Loss: 0.00000157 Testing Loss: 0.00000159\n",
      "Epoch [51200/200000] Training Loss: 0.00000120 Testing Loss: 0.00000128\n",
      "Epoch [51300/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [51400/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [51500/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [51600/200000] Training Loss: 0.00000177 Testing Loss: 0.00000167\n",
      "Epoch [51700/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [51800/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [51900/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [52000/200000] Training Loss: 0.00000162 Testing Loss: 0.00000148\n",
      "Epoch [52100/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [52200/200000] Training Loss: 0.00006923 Testing Loss: 0.00010596\n",
      "Epoch [52300/200000] Training Loss: 0.00000123 Testing Loss: 0.00000130\n",
      "Epoch [52400/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [52500/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [52600/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [52700/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [52800/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [52900/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [53000/200000] Training Loss: 0.00001860 Testing Loss: 0.00000669\n",
      "Epoch [53100/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [53200/200000] Training Loss: 0.00000119 Testing Loss: 0.00000125\n",
      "Epoch [53300/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [53400/200000] Training Loss: 0.00000931 Testing Loss: 0.00000588\n",
      "Epoch [53500/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [53600/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [53700/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [53800/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [53900/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [54000/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [54100/200000] Training Loss: 0.00000187 Testing Loss: 0.00000185\n",
      "Epoch [54200/200000] Training Loss: 0.00000119 Testing Loss: 0.00000125\n",
      "Epoch [54300/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [54400/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [54500/200000] Training Loss: 0.00000122 Testing Loss: 0.00000131\n",
      "Epoch [54600/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [54700/200000] Training Loss: 0.00000150 Testing Loss: 0.00000152\n",
      "Epoch [54800/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [54900/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [55000/200000] Training Loss: 0.00000138 Testing Loss: 0.00000151\n",
      "Epoch [55100/200000] Training Loss: 0.00000127 Testing Loss: 0.00000137\n",
      "Epoch [55200/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [55300/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [55400/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [55500/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [55600/200000] Training Loss: 0.00000135 Testing Loss: 0.00000139\n",
      "Epoch [55700/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [55800/200000] Training Loss: 0.00000120 Testing Loss: 0.00000128\n",
      "Epoch [55900/200000] Training Loss: 0.00000608 Testing Loss: 0.00000673\n",
      "Epoch [56000/200000] Training Loss: 0.00000172 Testing Loss: 0.00000187\n",
      "Epoch [56100/200000] Training Loss: 0.00002858 Testing Loss: 0.00000625\n",
      "Epoch [56200/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [56300/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [56400/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [56500/200000] Training Loss: 0.00000147 Testing Loss: 0.00000163\n",
      "Epoch [56600/200000] Training Loss: 0.00000133 Testing Loss: 0.00000148\n",
      "Epoch [56700/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [56800/200000] Training Loss: 0.00000145 Testing Loss: 0.00000169\n",
      "Epoch [56900/200000] Training Loss: 0.00000589 Testing Loss: 0.00000536\n",
      "Epoch [57000/200000] Training Loss: 0.00000151 Testing Loss: 0.00000161\n",
      "Epoch [57100/200000] Training Loss: 0.00000156 Testing Loss: 0.00000160\n",
      "Epoch [57200/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [57300/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [57400/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [57500/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [57600/200000] Training Loss: 0.00000120 Testing Loss: 0.00000126\n",
      "Epoch [57700/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [57800/200000] Training Loss: 0.00000126 Testing Loss: 0.00000130\n",
      "Epoch [57900/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [58000/200000] Training Loss: 0.00000206 Testing Loss: 0.00000202\n",
      "Epoch [58100/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [58200/200000] Training Loss: 0.00000120 Testing Loss: 0.00000128\n",
      "Epoch [58300/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [58400/200000] Training Loss: 0.00000121 Testing Loss: 0.00000127\n",
      "Epoch [58500/200000] Training Loss: 0.00000350 Testing Loss: 0.00000332\n",
      "Epoch [58600/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [58700/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [58800/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [58900/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [59000/200000] Training Loss: 0.00000340 Testing Loss: 0.00000302\n",
      "Epoch [59100/200000] Training Loss: 0.00000201 Testing Loss: 0.00000231\n",
      "Epoch [59200/200000] Training Loss: 0.00000274 Testing Loss: 0.00000240\n",
      "Epoch [59300/200000] Training Loss: 0.00000205 Testing Loss: 0.00000210\n",
      "Epoch [59400/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [59500/200000] Training Loss: 0.00000118 Testing Loss: 0.00000126\n",
      "Epoch [59600/200000] Training Loss: 0.00000195 Testing Loss: 0.00000193\n",
      "Epoch [59700/200000] Training Loss: 0.00000330 Testing Loss: 0.00000329\n",
      "Epoch [59800/200000] Training Loss: 0.00000210 Testing Loss: 0.00000199\n",
      "Epoch [59900/200000] Training Loss: 0.00000237 Testing Loss: 0.00000229\n",
      "Epoch [60000/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [60100/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [60200/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [60300/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [60400/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [60500/200000] Training Loss: 0.00000123 Testing Loss: 0.00000128\n",
      "Epoch [60600/200000] Training Loss: 0.00001077 Testing Loss: 0.00001427\n",
      "Epoch [60700/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [60800/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [60900/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [61000/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [61100/200000] Training Loss: 0.00000189 Testing Loss: 0.00000233\n",
      "Epoch [61200/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [61300/200000] Training Loss: 0.00000121 Testing Loss: 0.00000126\n",
      "Epoch [61400/200000] Training Loss: 0.00000414 Testing Loss: 0.00000454\n",
      "Epoch [61500/200000] Training Loss: 0.00000158 Testing Loss: 0.00000182\n",
      "Epoch [61600/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [61700/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [61800/200000] Training Loss: 0.00000710 Testing Loss: 0.00000250\n",
      "Epoch [61900/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [62000/200000] Training Loss: 0.00000173 Testing Loss: 0.00000186\n",
      "Epoch [62100/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [62200/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [62300/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [62400/200000] Training Loss: 0.00007602 Testing Loss: 0.00004016\n",
      "Epoch [62500/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [62600/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [62700/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [62800/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [62900/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [63000/200000] Training Loss: 0.00000140 Testing Loss: 0.00000153\n",
      "Epoch [63100/200000] Training Loss: 0.00006226 Testing Loss: 0.00005812\n",
      "Epoch [63200/200000] Training Loss: 0.00000140 Testing Loss: 0.00000150\n",
      "Epoch [63300/200000] Training Loss: 0.00000122 Testing Loss: 0.00000129\n",
      "Epoch [63400/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [63500/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [63600/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [63700/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [63800/200000] Training Loss: 0.00000133 Testing Loss: 0.00000142\n",
      "Epoch [63900/200000] Training Loss: 0.00000118 Testing Loss: 0.00000126\n",
      "Epoch [64000/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [64100/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [64200/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [64300/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [64400/200000] Training Loss: 0.00000152 Testing Loss: 0.00000171\n",
      "Epoch [64500/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [64600/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [64700/200000] Training Loss: 0.00001204 Testing Loss: 0.00000810\n",
      "Epoch [64800/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [64900/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [65000/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [65100/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [65200/200000] Training Loss: 0.00000123 Testing Loss: 0.00000131\n",
      "Epoch [65300/200000] Training Loss: 0.00000129 Testing Loss: 0.00000143\n",
      "Epoch [65400/200000] Training Loss: 0.00000138 Testing Loss: 0.00000135\n",
      "Epoch [65500/200000] Training Loss: 0.00004683 Testing Loss: 0.00001429\n",
      "Epoch [65600/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [65700/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [65800/200000] Training Loss: 0.00000138 Testing Loss: 0.00000159\n",
      "Epoch [65900/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [66000/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [66100/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [66200/200000] Training Loss: 0.00000353 Testing Loss: 0.00000314\n",
      "Epoch [66300/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [66400/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [66500/200000] Training Loss: 0.00000219 Testing Loss: 0.00000243\n",
      "Epoch [66600/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [66700/200000] Training Loss: 0.00000336 Testing Loss: 0.00000266\n",
      "Epoch [66800/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [66900/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [67000/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [67100/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [67200/200000] Training Loss: 0.00000382 Testing Loss: 0.00000455\n",
      "Epoch [67300/200000] Training Loss: 0.00000361 Testing Loss: 0.00000250\n",
      "Epoch [67400/200000] Training Loss: 0.00000156 Testing Loss: 0.00000170\n",
      "Epoch [67500/200000] Training Loss: 0.00000121 Testing Loss: 0.00000129\n",
      "Epoch [67600/200000] Training Loss: 0.00000144 Testing Loss: 0.00000146\n",
      "Epoch [67700/200000] Training Loss: 0.00000131 Testing Loss: 0.00000137\n",
      "Epoch [67800/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [67900/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [68000/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [68100/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [68200/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [68300/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [68400/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [68500/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [68600/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [68700/200000] Training Loss: 0.00000140 Testing Loss: 0.00000143\n",
      "Epoch [68800/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [68900/200000] Training Loss: 0.00000116 Testing Loss: 0.00000124\n",
      "Epoch [69000/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [69100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [69200/200000] Training Loss: 0.00000138 Testing Loss: 0.00000144\n",
      "Epoch [69300/200000] Training Loss: 0.00000118 Testing Loss: 0.00000125\n",
      "Epoch [69400/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [69500/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [69600/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [69700/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [69800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [69900/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [70000/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [70100/200000] Training Loss: 0.00000124 Testing Loss: 0.00000135\n",
      "Epoch [70200/200000] Training Loss: 0.00000249 Testing Loss: 0.00000308\n",
      "Epoch [70300/200000] Training Loss: 0.00000271 Testing Loss: 0.00000335\n",
      "Epoch [70400/200000] Training Loss: 0.00000396 Testing Loss: 0.00000484\n",
      "Epoch [70500/200000] Training Loss: 0.00000123 Testing Loss: 0.00000133\n",
      "Epoch [70600/200000] Training Loss: 0.00000120 Testing Loss: 0.00000126\n",
      "Epoch [70700/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [70800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [70900/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [71000/200000] Training Loss: 0.00000144 Testing Loss: 0.00000150\n",
      "Epoch [71100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [71200/200000] Training Loss: 0.00000121 Testing Loss: 0.00000127\n",
      "Epoch [71300/200000] Training Loss: 0.00001630 Testing Loss: 0.00005762\n",
      "Epoch [71400/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [71500/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [71600/200000] Training Loss: 0.00000115 Testing Loss: 0.00000123\n",
      "Epoch [71700/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [71800/200000] Training Loss: 0.00000118 Testing Loss: 0.00000127\n",
      "Epoch [71900/200000] Training Loss: 0.00000388 Testing Loss: 0.00000489\n",
      "Epoch [72000/200000] Training Loss: 0.00000115 Testing Loss: 0.00000123\n",
      "Epoch [72100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [72200/200000] Training Loss: 0.00000182 Testing Loss: 0.00000185\n",
      "Epoch [72300/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [72400/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [72500/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [72600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [72700/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [72800/200000] Training Loss: 0.00000142 Testing Loss: 0.00000140\n",
      "Epoch [72900/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [73000/200000] Training Loss: 0.00000202 Testing Loss: 0.00000271\n",
      "Epoch [73100/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [73200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [73300/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [73400/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [73500/200000] Training Loss: 0.00000117 Testing Loss: 0.00000122\n",
      "Epoch [73600/200000] Training Loss: 0.00000139 Testing Loss: 0.00000144\n",
      "Epoch [73700/200000] Training Loss: 0.00000192 Testing Loss: 0.00000236\n",
      "Epoch [73800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [73900/200000] Training Loss: 0.00000149 Testing Loss: 0.00000173\n",
      "Epoch [74000/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [74100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [74200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [74300/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [74400/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [74500/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [74600/200000] Training Loss: 0.00000118 Testing Loss: 0.00000123\n",
      "Epoch [74700/200000] Training Loss: 0.00000278 Testing Loss: 0.00000259\n",
      "Epoch [74800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [74900/200000] Training Loss: 0.00000115 Testing Loss: 0.00000124\n",
      "Epoch [75000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [75100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [75200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [75300/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [75400/200000] Training Loss: 0.00000137 Testing Loss: 0.00000152\n",
      "Epoch [75500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [75600/200000] Training Loss: 0.00000128 Testing Loss: 0.00000134\n",
      "Epoch [75700/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [75800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [75900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [76000/200000] Training Loss: 0.00000162 Testing Loss: 0.00000171\n",
      "Epoch [76100/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [76200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [76300/200000] Training Loss: 0.00000186 Testing Loss: 0.00000171\n",
      "Epoch [76400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [76500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [76600/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [76700/200000] Training Loss: 0.00000640 Testing Loss: 0.00000590\n",
      "Epoch [76800/200000] Training Loss: 0.00000214 Testing Loss: 0.00000209\n",
      "Epoch [76900/200000] Training Loss: 0.00000207 Testing Loss: 0.00000220\n",
      "Epoch [77000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [77100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [77200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [77300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [77400/200000] Training Loss: 0.00000330 Testing Loss: 0.00000334\n",
      "Epoch [77500/200000] Training Loss: 0.00000122 Testing Loss: 0.00000126\n",
      "Epoch [77600/200000] Training Loss: 0.00000143 Testing Loss: 0.00000142\n",
      "Epoch [77700/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [77800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [77900/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [78000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [78100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [78200/200000] Training Loss: 0.00000320 Testing Loss: 0.00000673\n",
      "Epoch [78300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [78400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [78500/200000] Training Loss: 0.00000128 Testing Loss: 0.00000129\n",
      "Epoch [78600/200000] Training Loss: 0.00000131 Testing Loss: 0.00000145\n",
      "Epoch [78700/200000] Training Loss: 0.00000125 Testing Loss: 0.00000132\n",
      "Epoch [78800/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [78900/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [79000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [79100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [79200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [79300/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [79400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [79500/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [79600/200000] Training Loss: 0.00001279 Testing Loss: 0.00001293\n",
      "Epoch [79700/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [79800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [79900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [80000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [80100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [80200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [80300/200000] Training Loss: 0.00000160 Testing Loss: 0.00000146\n",
      "Epoch [80400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [80500/200000] Training Loss: 0.00000208 Testing Loss: 0.00000268\n",
      "Epoch [80600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [80700/200000] Training Loss: 0.00000370 Testing Loss: 0.00000481\n",
      "Epoch [80800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [80900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [81000/200000] Training Loss: 0.00000142 Testing Loss: 0.00000167\n",
      "Epoch [81100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [81200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [81300/200000] Training Loss: 0.00000199 Testing Loss: 0.00000199\n",
      "Epoch [81400/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [81500/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [81600/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [81700/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [81800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [81900/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [82000/200000] Training Loss: 0.00000812 Testing Loss: 0.00000526\n",
      "Epoch [82100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [82200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [82300/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [82400/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [82500/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [82600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [82700/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [82800/200000] Training Loss: 0.00000127 Testing Loss: 0.00000132\n",
      "Epoch [82900/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [83000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [83100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [83200/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [83300/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [83400/200000] Training Loss: 0.00000283 Testing Loss: 0.00000378\n",
      "Epoch [83500/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [83600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [83700/200000] Training Loss: 0.00000119 Testing Loss: 0.00000126\n",
      "Epoch [83800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [83900/200000] Training Loss: 0.00000127 Testing Loss: 0.00000133\n",
      "Epoch [84000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [84100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [84200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [84300/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [84400/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [84500/200000] Training Loss: 0.00000433 Testing Loss: 0.00000377\n",
      "Epoch [84600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [84700/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [84800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [84900/200000] Training Loss: 0.00000119 Testing Loss: 0.00000127\n",
      "Epoch [85000/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [85100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [85200/200000] Training Loss: 0.00000336 Testing Loss: 0.00000326\n",
      "Epoch [85300/200000] Training Loss: 0.00000120 Testing Loss: 0.00000127\n",
      "Epoch [85400/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [85500/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [85600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [85700/200000] Training Loss: 0.00000122 Testing Loss: 0.00000133\n",
      "Epoch 85711: reducing learning rate of group 0 to 4.9000e-04.\n",
      "Epoch [85800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [85900/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [86000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [86100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [86200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [86300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [86400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [86500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [86600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [86700/200000] Training Loss: 0.00000132 Testing Loss: 0.00000142\n",
      "Epoch [86800/200000] Training Loss: 0.00000324 Testing Loss: 0.00000331\n",
      "Epoch [86900/200000] Training Loss: 0.00000527 Testing Loss: 0.00000417\n",
      "Epoch [87000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [87100/200000] Training Loss: 0.00000123 Testing Loss: 0.00000134\n",
      "Epoch [87200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [87300/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [87400/200000] Training Loss: 0.00000147 Testing Loss: 0.00000143\n",
      "Epoch [87500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [87600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [87700/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [87800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [87900/200000] Training Loss: 0.00000215 Testing Loss: 0.00000144\n",
      "Epoch [88000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [88100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [88200/200000] Training Loss: 0.00000206 Testing Loss: 0.00000227\n",
      "Epoch [88300/200000] Training Loss: 0.00000135 Testing Loss: 0.00000151\n",
      "Epoch [88400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [88500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [88600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [88700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [88800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [88900/200000] Training Loss: 0.00001014 Testing Loss: 0.00000469\n",
      "Epoch [89000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [89100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [89200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [89300/200000] Training Loss: 0.00000142 Testing Loss: 0.00000142\n",
      "Epoch [89400/200000] Training Loss: 0.00000166 Testing Loss: 0.00000240\n",
      "Epoch [89500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [89600/200000] Training Loss: 0.00000121 Testing Loss: 0.00000128\n",
      "Epoch [89700/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [89800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [89900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [90000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [90100/200000] Training Loss: 0.00000135 Testing Loss: 0.00000128\n",
      "Epoch [90200/200000] Training Loss: 0.00001196 Testing Loss: 0.00000821\n",
      "Epoch 90212: reducing learning rate of group 0 to 3.4300e-04.\n",
      "Epoch [90300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [90400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [90500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [90600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [90700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [90800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [90900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [91000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [91100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [91200/200000] Training Loss: 0.00000144 Testing Loss: 0.00000139\n",
      "Epoch [91300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [91400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [91500/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [91600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [91700/200000] Training Loss: 0.00000124 Testing Loss: 0.00000133\n",
      "Epoch [91800/200000] Training Loss: 0.00000192 Testing Loss: 0.00000170\n",
      "Epoch [91900/200000] Training Loss: 0.00000340 Testing Loss: 0.00000308\n",
      "Epoch [92000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [92100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [92200/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [92300/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [92400/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [92500/200000] Training Loss: 0.00000127 Testing Loss: 0.00000137\n",
      "Epoch [92600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [92700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [92800/200000] Training Loss: 0.00000119 Testing Loss: 0.00000125\n",
      "Epoch [92900/200000] Training Loss: 0.00000117 Testing Loss: 0.00000125\n",
      "Epoch [93000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [93100/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [93200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [93300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [93400/200000] Training Loss: 0.00000119 Testing Loss: 0.00000127\n",
      "Epoch [93500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [93600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [93700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [93800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [93900/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [94000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [94100/200000] Training Loss: 0.00000162 Testing Loss: 0.00000186\n",
      "Epoch [94200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [94300/200000] Training Loss: 0.00000118 Testing Loss: 0.00000126\n",
      "Epoch [94400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [94500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [94600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [94700/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [94800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [94900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [95000/200000] Training Loss: 0.00000220 Testing Loss: 0.00000228\n",
      "Epoch [95100/200000] Training Loss: 0.00000155 Testing Loss: 0.00000127\n",
      "Epoch [95200/200000] Training Loss: 0.00000119 Testing Loss: 0.00000124\n",
      "Epoch [95300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [95400/200000] Training Loss: 0.00000115 Testing Loss: 0.00000123\n",
      "Epoch [95500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [95600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [95700/200000] Training Loss: 0.00000119 Testing Loss: 0.00000128\n",
      "Epoch [95800/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [95900/200000] Training Loss: 0.00000117 Testing Loss: 0.00000122\n",
      "Epoch [96000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [96100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [96200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [96300/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [96400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [96500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [96600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [96700/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [96800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [96900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [97000/200000] Training Loss: 0.00000118 Testing Loss: 0.00000123\n",
      "Epoch [97100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [97200/200000] Training Loss: 0.00000118 Testing Loss: 0.00000124\n",
      "Epoch [97300/200000] Training Loss: 0.00000131 Testing Loss: 0.00000128\n",
      "Epoch [97400/200000] Training Loss: 0.00000116 Testing Loss: 0.00000126\n",
      "Epoch [97500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [97600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [97700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [97800/200000] Training Loss: 0.00000207 Testing Loss: 0.00000185\n",
      "Epoch [97900/200000] Training Loss: 0.00000150 Testing Loss: 0.00000170\n",
      "Epoch [98000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [98100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [98200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [98300/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [98400/200000] Training Loss: 0.00000241 Testing Loss: 0.00000213\n",
      "Epoch [98500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [98600/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [98700/200000] Training Loss: 0.00000126 Testing Loss: 0.00000138\n",
      "Epoch [98800/200000] Training Loss: 0.00000118 Testing Loss: 0.00000123\n",
      "Epoch [98900/200000] Training Loss: 0.00000121 Testing Loss: 0.00000131\n",
      "Epoch [99000/200000] Training Loss: 0.00000122 Testing Loss: 0.00000146\n",
      "Epoch [99100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [99200/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [99300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [99400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [99500/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [99600/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [99700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [99800/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [99900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [100000/200000] Training Loss: 0.00000160 Testing Loss: 0.00000252\n",
      "Epoch [100100/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [100200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [100300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [100400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [100500/200000] Training Loss: 0.00000134 Testing Loss: 0.00000130\n",
      "Epoch [100600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [100700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [100800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [100900/200000] Training Loss: 0.00000116 Testing Loss: 0.00000122\n",
      "Epoch [101000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [101100/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [101200/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [101300/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [101400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [101500/200000] Training Loss: 0.00000207 Testing Loss: 0.00000221\n",
      "Epoch [101600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [101700/200000] Training Loss: 0.00000155 Testing Loss: 0.00000136\n",
      "Epoch [101800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [101900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [102000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [102100/200000] Training Loss: 0.00000116 Testing Loss: 0.00000124\n",
      "Epoch [102200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [102300/200000] Training Loss: 0.00000237 Testing Loss: 0.00000212\n",
      "Epoch [102400/200000] Training Loss: 0.00000118 Testing Loss: 0.00000126\n",
      "Epoch [102500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [102600/200000] Training Loss: 0.00000160 Testing Loss: 0.00000187\n",
      "Epoch [102700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [102800/200000] Training Loss: 0.00000115 Testing Loss: 0.00000121\n",
      "Epoch [102900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [103000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [103100/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [103200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [103300/200000] Training Loss: 0.00000172 Testing Loss: 0.00000136\n",
      "Epoch [103400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [103500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [103600/200000] Training Loss: 0.00000112 Testing Loss: 0.00000119\n",
      "Epoch [103700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [103800/200000] Training Loss: 0.00000126 Testing Loss: 0.00000140\n",
      "Epoch [103900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [104000/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [104100/200000] Training Loss: 0.00000117 Testing Loss: 0.00000123\n",
      "Epoch [104200/200000] Training Loss: 0.00000112 Testing Loss: 0.00000119\n",
      "Epoch [104300/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [104400/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [104500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [104600/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [104700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [104800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [104900/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [105000/200000] Training Loss: 0.00000114 Testing Loss: 0.00000120\n",
      "Epoch [105100/200000] Training Loss: 0.00000116 Testing Loss: 0.00000123\n",
      "Epoch [105200/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [105300/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n",
      "Epoch [105400/200000] Training Loss: 0.00000112 Testing Loss: 0.00000119\n",
      "Epoch [105500/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [105600/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [105700/200000] Training Loss: 0.00000113 Testing Loss: 0.00000120\n",
      "Epoch [105800/200000] Training Loss: 0.00000113 Testing Loss: 0.00000119\n",
      "Epoch [105900/200000] Training Loss: 0.00000125 Testing Loss: 0.00000134\n",
      "Epoch [106000/200000] Training Loss: 0.00000117 Testing Loss: 0.00000124\n",
      "Epoch [106100/200000] Training Loss: 0.00000112 Testing Loss: 0.00000119\n",
      "Epoch [106200/200000] Training Loss: 0.00000124 Testing Loss: 0.00000132\n",
      "Epoch [106300/200000] Training Loss: 0.00000286 Testing Loss: 0.00000268\n",
      "Epoch [106400/200000] Training Loss: 0.00000115 Testing Loss: 0.00000122\n",
      "Epoch [106500/200000] Training Loss: 0.00000114 Testing Loss: 0.00000121\n"
     ]
    }
   ],
   "source": [
    "net = GreenFun(2 * m2).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience = 4500, factor=0.7, verbose=True)\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 60000, verbose=False, delta=1e-8, path='net.pth')\n",
    "num_epochs = 200000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    net.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = net(train_lambda, train_f)\n",
    "    \n",
    "    loss = criterion(output, train_u)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_test = net(test_lambda, test_f)\n",
    "        loss_test = criterion(output_test, test_u)\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {loss.item():.8f} Testing Loss: {loss_test.item():.8f}\")\n",
    "\n",
    "    # 调整学习率\n",
    "    scheduler.step(loss_test)\n",
    "\n",
    "    early_stopping(loss_test, net)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net.pth\", map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, N)\n",
    "y = np.linspace(0, 1, N)\n",
    "X, Y = np.meshgrid(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeErrors(u_exact, u_pre, printOrNot):\n",
    "    error = u_exact - u_pre\n",
    "    l2_norm_abs = np.linalg.norm(error, ord=2) / np.sqrt(error.size)\n",
    "    max_norm_abs = np.max(np.abs(error))\n",
    "    l2_norm_rel = np.linalg.norm(error, ord=2) / np.linalg.norm(u_exact, ord=2)\n",
    "    max_norm_rel = np.max(np.abs(error)) / np.max(np.abs(u_exact))  \n",
    "    \n",
    "    l2_norm_rel_percent = l2_norm_rel * 100\n",
    "    max_norm_rel_percent = max_norm_rel * 100\n",
    "    \n",
    "    if printOrNot == True:\n",
    "        print(f\"Absolute L2 Norm Error: {l2_norm_abs:.8f}\")\n",
    "        print(f\"Absolute Max Norm Error: {max_norm_abs:.8f}\")\n",
    "        print(f\"Relative L2 Norm Error: {l2_norm_rel_percent:.6f}%\")\n",
    "        print(f\"Relative Max Norm Error: {max_norm_rel_percent:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(lambda_, net):\n",
    "    u1_exact = np.sin(np.pi * X) * Y * (1 - Y)\n",
    "    u2_exact = X * (1 - X) * np.sin(np.pi * Y)\n",
    "    laplace_u1 = -2 * np.sin(np.pi * X) - np.pi ** 2 * u1_exact\n",
    "    laplace_u2 = -2 * np.sin(np.pi * Y) - np.pi ** 2 * u2_exact\n",
    "    f1 = u1_exact - lambda_ * laplace_u2\n",
    "    f2 = lambda_ * laplace_u1 + u2_exact\n",
    "\n",
    "    f = np.concatenate([f1[1:-1, 1:-1].flatten(), f2[1:-1, 1:-1].flatten()])\n",
    "\n",
    "    lambda_torch = (lambda_ + torch.zeros(1, 1)).to(device)\n",
    "    f_torch = torch.tensor(f, dtype=torch.float32).view(1, -1).to(device)\n",
    "\n",
    "    u_numerical_torch = net(lambda_torch, f_torch)\n",
    "    u_numerical = u_numerical_torch.cpu().detach().numpy().flatten()\n",
    "\n",
    "    u1_numerical = np.zeros_like(u1_exact)\n",
    "    u2_numerical = np.zeros_like(u2_exact)\n",
    "    u1_numerical[1:-1, 1:-1] = u_numerical[0: m2].reshape((m, m))\n",
    "    u2_numerical[1:-1, 1:-1] = u_numerical[m2: 2 * m2].reshape((m, m))\n",
    "\n",
    "    print(\"numerical result for u_1:\")\n",
    "    computeErrors(u1_exact, u1_numerical, True)\n",
    "\n",
    "    print(\"numerical result for u_2:\")\n",
    "    computeErrors(u2_exact, u2_numerical, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical result for u_1:\n",
      "Absolute L2 Norm Error: 0.00032290\n",
      "Absolute Max Norm Error: 0.00162486\n",
      "Relative L2 Norm Error: 0.256367%\n",
      "Relative Max Norm Error: 0.649944%\n",
      "numerical result for u_2:\n",
      "Absolute L2 Norm Error: 0.00026581\n",
      "Absolute Max Norm Error: 0.00156649\n",
      "Relative L2 Norm Error: 0.211047%\n",
      "Relative Max Norm Error: 0.626598%\n"
     ]
    }
   ],
   "source": [
    "validation(0.05, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical result for u_1:\n",
      "Absolute L2 Norm Error: 0.00018411\n",
      "Absolute Max Norm Error: 0.00105639\n",
      "Relative L2 Norm Error: 0.146174%\n",
      "Relative Max Norm Error: 0.422554%\n",
      "numerical result for u_2:\n",
      "Absolute L2 Norm Error: 0.00034188\n",
      "Absolute Max Norm Error: 0.00125038\n",
      "Relative L2 Norm Error: 0.271442%\n",
      "Relative Max Norm Error: 0.500152%\n"
     ]
    }
   ],
   "source": [
    "validation(1/30, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical result for u_1:\n",
      "Absolute L2 Norm Error: 0.00023538\n",
      "Absolute Max Norm Error: 0.00153296\n",
      "Relative L2 Norm Error: 0.186883%\n",
      "Relative Max Norm Error: 0.613186%\n",
      "numerical result for u_2:\n",
      "Absolute L2 Norm Error: 0.00024665\n",
      "Absolute Max Norm Error: 0.00155449\n",
      "Relative L2 Norm Error: 0.195832%\n",
      "Relative Max Norm Error: 0.621797%\n"
     ]
    }
   ],
   "source": [
    "validation(0.025, net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
